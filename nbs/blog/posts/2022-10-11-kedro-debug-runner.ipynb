{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "badges: true\n",
    "categories:\n",
    "- europython\n",
    "date: '2022-11-01'\n",
    "description: Quick implementation of a debug runner\n",
    "hide: false\n",
    "output-file: kedro-debug-runner.html\n",
    "title: Quick implementation of Kedro DebugRunner \n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from typing import Any, Dict, Iterable, List, Set\n",
    "\n",
    "from kedro.framework.hooks.manager import _NullPluginManager\n",
    "from kedro.io import AbstractDataSet, DataCatalog, MemoryDataSet\n",
    "from kedro.pipeline import Pipeline\n",
    "from kedro.pipeline.node import Node\n",
    "from kedro.runner import SequentialRunner\n",
    "from kedro.runner.runner import AbstractRunner, run_node\n",
    "from pluggy import PluginManager\n",
    "\n",
    "\n",
    "class DebugRunner(SequentialRunner):\n",
    "    def run(\n",
    "        self,\n",
    "        pipeline: Pipeline,\n",
    "        catalog: DataCatalog,\n",
    "        dataset_names: List[str] = None,\n",
    "        hook_manager: PluginManager = None,\n",
    "        session_id: str = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run the ``Pipeline`` using the datasets provided by ``catalog``\n",
    "        and save results back to the same objects.\n",
    "\n",
    "        Args:\n",
    "            pipeline: The ``Pipeline`` to run.\n",
    "            catalog: The ``DataCatalog`` from which to fetch data.\n",
    "            hook_manager: The ``PluginManager`` to activate hooks.\n",
    "            session_id: The id of the session.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Raised when ``Pipeline`` inputs cannot be satisfied.\n",
    "\n",
    "        Returns:\n",
    "            Any node outputs that cannot be processed by the ``DataCatalog``.\n",
    "            These are returned in a dictionary, where the keys are defined\n",
    "            by the node outputs.\n",
    "\n",
    "        \"\"\"\n",
    "        if not dataset_names:\n",
    "            dataset_names = []\n",
    "        hook_manager = hook_manager or _NullPluginManager()\n",
    "        catalog = catalog.shallow_copy()\n",
    "\n",
    "        unsatisfied = pipeline.inputs() - set(catalog.list())\n",
    "        if unsatisfied:\n",
    "            raise ValueError(\n",
    "                f\"Pipeline input(s) {unsatisfied} not found in the DataCatalog\"\n",
    "            )\n",
    "\n",
    "        free_outputs = (\n",
    "            pipeline.outputs()\n",
    "        )  # Return everything regardless if it it's in catalog\n",
    "        unregistered_ds = pipeline.data_sets() - set(catalog.list())\n",
    "        for ds_name in unregistered_ds:\n",
    "            catalog.add(ds_name, self.create_default_data_set(ds_name))\n",
    "\n",
    "        if self._is_async:\n",
    "            self._logger.info(\n",
    "                \"Asynchronous mode is enabled for loading and saving data\"\n",
    "            )\n",
    "        self._run(pipeline, catalog, dataset_names, hook_manager, session_id)\n",
    "\n",
    "        self._logger.info(\"Pipeline execution completed successfully.\")\n",
    "        \n",
    "        free_outputs = free_outputs | set(dataset_names)  # Union\n",
    "\n",
    "        return {ds_name: catalog.load(ds_name) for ds_name in free_outputs}\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        pipeline: Pipeline,\n",
    "        catalog: DataCatalog,\n",
    "        dataset_names: List[str],\n",
    "        hook_manager: PluginManager,\n",
    "        session_id: str = None,\n",
    "    ) -> None:\n",
    "        \"\"\"The method implementing sequential pipeline running.\n",
    "\n",
    "        Args:\n",
    "            pipeline: The ``Pipeline`` to run.\n",
    "            catalog: The ``DataCatalog`` from which to fetch data.\n",
    "            hook_manager: The ``PluginManager`` to activate hooks.\n",
    "            session_id: The id of the session.\n",
    "\n",
    "        Raises:\n",
    "            Exception: in case of any downstream node failure.\n",
    "        \"\"\"\n",
    "        nodes = pipeline.nodes\n",
    "        done_nodes = set()\n",
    "\n",
    "        load_counts = Counter(chain.from_iterable(n.inputs for n in nodes))\n",
    "\n",
    "        for exec_index, node in enumerate(nodes):\n",
    "            try:\n",
    "                run_node(node, catalog, hook_manager, self._is_async, session_id)\n",
    "                done_nodes.add(node)\n",
    "            except Exception:\n",
    "                self._suggest_resume_scenario(pipeline, done_nodes, catalog)\n",
    "                raise\n",
    "\n",
    "            # decrement load counts and release any data sets we've finished with\n",
    "            for data_set in node.inputs:\n",
    "                load_counts[data_set] -= 1\n",
    "                if load_counts[data_set] < 1 and data_set not in pipeline.inputs():\n",
    "                    if data_set not in dataset_names:\n",
    "                        catalog.release(data_set)\n",
    "            for data_set in node.outputs:\n",
    "                if load_counts[data_set] < 1 and data_set not in pipeline.outputs():\n",
    "                    if data_set not in dataset_names:\n",
    "                        catalog.release(data_set)\n",
    "\n",
    "            self._logger.info(\n",
    "                \"Completed %d out of %d tasks\", exec_index + 1, len(nodes)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kedro.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext kedro.ipython\n",
      "[10/06/22 14:45:20] INFO     Updated path to Kedro project:       __init__.py:54\n",
      "                             /Users/Nok_Lam_Chan/dev/kedro_galler               \n",
      "                             y/kedro-debug-runner-demo                          \n",
      "[10/06/22 14:45:22] INFO     Kedro project                        __init__.py:77\n",
      "                             kedro_debug_runner_demo                            \n",
      "                    INFO     Defined global variable 'context',   __init__.py:78\n",
      "                             'session', 'catalog' and 'pipelines'               \n"
     ]
    }
   ],
   "source": [
    "# `DebugRunner` has to be used in a different way since `session.run` don't support additional argument, so we are going to use a lower level approach and construct `Runner` and `Pipeline` and `DataCatalog` ourselves.\n",
    "\n",
    "# Testing Kedro Project: https://github.com/noklam/kedro_gallery/tree/master/kedro-debug-runner-demo\n",
    "%load_ext kedro.ipython\n",
    "%reload_kedro ~/dev/kedro_gallery/kedro-debug-runner-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    INFO     Updated path to Kedro project:       __init__.py:54\n",
      "                             /Users/Nok_Lam_Chan/dev/kedro_galler               \n",
      "                             y/kedro-debug-runner-demo                          \n",
      "[10/06/22 14:45:24] INFO     Kedro project                        __init__.py:77\n",
      "                             kedro_debug_runner_demo                            \n",
      "                    INFO     Defined global variable 'context',   __init__.py:78\n",
      "                             'session', 'catalog' and 'pipelines'               \n",
      "                    INFO     Loading data from               data_catalog.py:343\n",
      "                             'example_iris_data'                                \n",
      "                             (CSVDataSet)...                                    \n",
      "                    INFO     Loading data from 'parameters'  data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Running node: split:                    node.py:327\n",
      "                             split_data([example_iris_data,parameter            \n",
      "                             s]) -> [X_train,X_test,y_train,y_test]             \n",
      "                    INFO     Saving data to 'X_train'        data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Saving data to 'X_test'         data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Saving data to 'y_train'        data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Saving data to 'y_test'         data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'X_train'     data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'X_test'      data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'y_train'     data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Running node: make_predictions:         node.py:327\n",
      "                             make_predictions([X_train,X_test,y_trai            \n",
      "                             n]) -> [y_pred]                                    \n",
      "                    INFO     Saving data to 'y_pred'         data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'y_pred'      data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'y_test'      data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Running node: report_accuracy:          node.py:327\n",
      "                             report_accuracy([y_pred,y_test]) ->                \n",
      "                             None                                               \n",
      "                    INFO     Model has accuracy of 0.933 on test     nodes.py:74\n",
      "                             data.                                              \n"
     ]
    }
   ],
   "source": [
    "%reload_kedro ~/dev/kedro_gallery/kedro-debug-runner-demo\n",
    "runner = DebugRunner()\n",
    "default_pipeline = pipelines[\"__default__\"]\n",
    "run_1 = runner.run(default_pipeline, catalog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/06/22 14:45:27] INFO     Loading data from               data_catalog.py:343\n",
      "                             'example_iris_data'                                \n",
      "                             (CSVDataSet)...                                    \n",
      "                    INFO     Loading data from 'parameters'  data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Running node: split:                    node.py:327\n",
      "                             split_data([example_iris_data,parameter            \n",
      "                             s]) -> [X_train,X_test,y_train,y_test]             \n",
      "                    INFO     Saving data to 'X_train'        data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Saving data to 'X_test'         data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Saving data to 'y_train'        data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Saving data to 'y_test'         data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'X_train'     data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'X_test'      data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'y_train'     data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Running node: make_predictions:         node.py:327\n",
      "                             make_predictions([X_train,X_test,y_trai            \n",
      "                             n]) -> [y_pred]                                    \n",
      "                    INFO     Saving data to 'y_pred'         data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'y_pred'      data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'y_test'      data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Running node: report_accuracy:          node.py:327\n",
      "                             report_accuracy([y_pred,y_test]) ->                \n",
      "                             None                                               \n",
      "                    INFO     Model has accuracy of 0.933 on test     nodes.py:74\n",
      "                             data.                                              \n",
      "                    INFO     Loading data from               data_catalog.py:343\n",
      "                             'example_iris_data'                                \n",
      "                             (CSVDataSet)...                                    \n"
     ]
    }
   ],
   "source": [
    "runner = DebugRunner()\n",
    "default_pipeline = pipelines[\"__default__\"]\n",
    "run_2 = runner.run(default_pipeline, catalog, dataset_names=[\"example_iris_data\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/06/22 14:46:01] INFO     Loading data from               data_catalog.py:343\n",
      "                             'example_iris_data'                                \n",
      "                             (CSVDataSet)...                                    \n",
      "                    INFO     Loading data from 'parameters'  data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Running node: split:                    node.py:327\n",
      "                             split_data([example_iris_data,parameter            \n",
      "                             s]) -> [X_train,X_test,y_train,y_test]             \n",
      "                    INFO     Saving data to 'X_train'        data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Saving data to 'X_test'         data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Saving data to 'y_train'        data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Saving data to 'y_test'         data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'X_train'     data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'X_test'      data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'y_train'     data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Running node: make_predictions:         node.py:327\n",
      "                             make_predictions([X_train,X_test,y_trai            \n",
      "                             n]) -> [y_pred]                                    \n",
      "                    INFO     Saving data to 'y_pred'         data_catalog.py:382\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'y_pred'      data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Loading data from 'y_test'      data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n",
      "                    INFO     Running node: report_accuracy:          node.py:327\n",
      "                             report_accuracy([y_pred,y_test]) ->                \n",
      "                             None                                               \n",
      "                    INFO     Model has accuracy of 0.933 on test     nodes.py:74\n",
      "                             data.                                              \n",
      "                    INFO     Loading data from 'X_train'     data_catalog.py:343\n",
      "                             (MemoryDataSet)...                                 \n"
     ]
    }
   ],
   "source": [
    "runner = DebugRunner()\n",
    "default_pipeline = pipelines[\"__default__\"]\n",
    "run_3 = runner.run(default_pipeline, catalog, dataset_names=[\"X_train\"]) # Input datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_iris_data':      sepal_length  sepal_width  petal_length  petal_width    species\n",
       " 0             5.1          3.5           1.4          0.2     setosa\n",
       " 1             4.9          3.0           1.4          0.2     setosa\n",
       " 2             4.7          3.2           1.3          0.2     setosa\n",
       " 3             4.6          3.1           1.5          0.2     setosa\n",
       " 4             5.0          3.6           1.4          0.2     setosa\n",
       " ..            ...          ...           ...          ...        ...\n",
       " 145           6.7          3.0           5.2          2.3  virginica\n",
       " 146           6.3          2.5           5.0          1.9  virginica\n",
       " 147           6.5          3.0           5.2          2.0  virginica\n",
       " 148           6.2          3.4           5.4          2.3  virginica\n",
       " 149           5.9          3.0           5.1          1.8  virginica\n",
       " \n",
       " [150 rows x 5 columns]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_train':      sepal_length  sepal_width  petal_length  petal_width\n",
       " 47            4.6          3.2           1.4          0.2\n",
       " 3             4.6          3.1           1.5          0.2\n",
       " 31            5.4          3.4           1.5          0.4\n",
       " 25            5.0          3.0           1.6          0.2\n",
       " 15            5.7          4.4           1.5          0.4\n",
       " ..            ...          ...           ...          ...\n",
       " 28            5.2          3.4           1.4          0.2\n",
       " 78            6.0          2.9           4.5          1.5\n",
       " 146           6.3          2.5           5.0          1.9\n",
       " 49            5.0          3.3           1.4          0.2\n",
       " 94            5.6          2.7           4.2          1.3\n",
       " \n",
       " [120 rows x 4 columns]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySequentialRunner(SequentialRunner):\n",
    "    def run(\n",
    "        self,\n",
    "        pipeline: Pipeline,\n",
    "        catalog: DataCatalog,\n",
    "        hook_manager: PluginManager = None,\n",
    "        session_id: str = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run the ``Pipeline`` using the datasets provided by ``catalog``\n",
    "        and save results back to the same objects.\n",
    "\n",
    "        Args:\n",
    "            pipeline: The ``Pipeline`` to run.\n",
    "            catalog: The ``DataCatalog`` from which to fetch data.\n",
    "            hook_manager: The ``PluginManager`` to activate hooks.\n",
    "            session_id: The id of the session.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Raised when ``Pipeline`` inputs cannot be satisfied.\n",
    "\n",
    "        Returns:\n",
    "            Any node outputs that cannot be processed by the ``DataCatalog``.\n",
    "            These are returned in a dictionary, where the keys are defined\n",
    "            by the node outputs.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        hook_manager = hook_manager or _NullPluginManager()\n",
    "        catalog = catalog.shallow_copy()\n",
    "\n",
    "        unsatisfied = pipeline.inputs() - set(catalog.list())\n",
    "        if unsatisfied:\n",
    "            raise ValueError(\n",
    "                f\"Pipeline input(s) {unsatisfied} not found in the DataCatalog\"\n",
    "            )\n",
    "\n",
    "        free_outputs = pipeline.outputs() # Return everything regardless if it it's in catalog\n",
    "        unregistered_ds = pipeline.data_sets() - set(catalog.list())\n",
    "        for ds_name in unregistered_ds:\n",
    "            catalog.add(ds_name, self.create_default_data_set(ds_name))\n",
    "\n",
    "        if self._is_async:\n",
    "            self._logger.info(\n",
    "                \"Asynchronous mode is enabled for loading and saving data\"\n",
    "            )\n",
    "        self._run(pipeline, catalog, hook_manager, session_id)\n",
    "\n",
    "        self._logger.info(\"Pipeline execution completed successfully.\")\n",
    "\n",
    "        return {ds_name: catalog.load(ds_name) for ds_name in free_outputs}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
