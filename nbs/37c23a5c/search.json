[
  {
    "objectID": "00_core.html",
    "href": "00_core.html",
    "title": "noklam",
    "section": "",
    "text": "core\n\nFill in a module description here\n\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nfrom nbdev.showdoc import *\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef foo(): pass\n:::\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nimport nbdev; nbdev.nbdev_export()\n:::"
  },
  {
    "objectID": "archive/posts/2020-02-22-python-dynamic-dispatch.html",
    "href": "archive/posts/2020-02-22-python-dynamic-dispatch.html",
    "title": "The missing piece in Python tutorial - What is dispatch why you should care",
    "section": "",
    "text": "In python, we often think of it as a dynamic language, and type is barely noticed in Python as you can change the type of a variable whenever you want.\nSince Python 3.4(PEP443)[https://www.python.org/dev/peps/pep-0443/], generic function is added to Python. This add a new feature that I found much of the exsiting tutorial does not cover it. Such feature is common in other language and is very useful to keep your code concise and clean.\nIn python, you cannot overload a normal function twice for different behavior base on the arguments. For example:\n\ndef foo(number:int ):\n    print('it is a integer')\n    \ndef foo(number: float):\n    print('it is a float')\n\n\nfoo(1)\n\nit is a float\n\n\nThe definition simply get replaced by the second definition. However, with singledispatch, you can define the function behavior base on the type of the argument.\n\nfrom functools import singledispatch\n@singledispatch\ndef foo(number ):\n    print(f'{type(number)}, {number}')\n\n\nfoo(1)\n\n<class 'int'>, 1\n\n\nWe can now register the function for different argument type.\n\n@foo.register(int)\ndef _(data):\n    print('It is a integer!')\n    \n@foo.register(float)\ndef _(data):\n    print('It is a float!')\n\n@foo.register(dict)\ndef _(data):\n    print('It is a dict!')\n\n\nfoo(1.0)\nfoo(1)\nfoo({'1':1})\n\nIt is a float!\nIt is a integer!\nIt is a dict!\n\n\nHow is this possible? Basically there are multiple version of a generic function, singlepatch will pick the correct one base on the type of the first argument.\nIt will fallback to the most generic function if the type of argument is not registered.\n\nfoo([1,2,3])\n\n<class 'list'>, [1, 2, 3]\n\n\nI hope you can see how this is going to be useful. singledispatch limited the usage to the first argument of a function. But we can actually do more than that.\nIn next post I will cover the patch method from fastai will leverage singledispatch more to do multi-dispatch. In python, everything is just an object, even a function itself. So there is no reason why you can only dispatch to a function object. In fact, you could dispatch method to a class too.\n\nFastai @typedispatch\nSingle Dispatch is great, but what if we can do multi dispatch for more than 1 argument?\n\nfrom fastcore.dispatch import  typedispatch, TypeDispatch\n\nLet us first try if this work as expected\n\n@typedispatch\ndef add(x:int, y:int):\n    return x+y\n@typedispatch\ndef add(x:int, y:str):\n    return x + int(y)\n\n\nprint(add(1,2))\nprint(add(1,'2'))\nprint(add('a','a'))\n\n3\n3\na\n\n\n\nadd(1,2)\n\n3\n\n\n\nadd(1,'2')\n\n3\n\n\nBut what if we added something does not define?\n\nadd('2',1)\n\n'2'\n\n\n‘2’? where does it come from? Let’s have a look at the definition of typedispatch and understand how it works.\n\n??typedispatch\nclass DispatchReg:\n    \"A global registry for `TypeDispatch` objects keyed by function name\"\n    def __init__(self): self.d = defaultdict(TypeDispatch)\n    def __call__(self, f):\n        nm = f'{f.__qualname__}'\n        self.d[nm].add(f)\n        return self.d[nm]\n\nIn fact, typedispatch is not even a function, it’s an instance! In python, everything is an object. With the __call__ method, we can use an instance just liek a function. And the typedispatch is just an instance of DispatchReg\n\ntype(typedispatch)\n\nfastcore.dispatch.DispatchReg\n\n\ntypedispatch store a dictionary inside, when you first register your function, it actually store inside a dict. As shown previously, you cannot define the same function twice. But you actually can, because function is nothing but just an object! Let me show you.\n\ndef foo(): return 'foo'\na = foo\ndef foo(): return 'not foo'\nb = foo\n\n\nfoo()\n\n'not foo'\n\n\nfoo() is replaced by the latest definition indeed, but we store a copy of the original function as a variable.\n\na()\n\n'foo'\n\n\n\nb()\n\n'not foo'\n\n\n\nhex(id(a)), hex(id(b))\n\n('0x2b9d28bb5e8', '0x2b9d2ebe048')\n\n\nThe two function is nothing other than two Python object. typedispatch make use of these, when you register a new function, you create an new object and stored inside typedispatch dictionary. It then checks your type annotation and find the corresponding type until it match the issubclass condition.\n\ntypedispatch.d\n\ndefaultdict(fastcore.dispatch.TypeDispatch,\n            {'cast': (object,object) -> cast, 'add': (int,str) -> add\n             (int,int) -> add})\n\n\nSo back to our question, why does add(‘a’,1) return ‘a’? The following explain the reasons. When you call your method, you are really calling the __call__ method inside TypeDispatch, and when the signature is not find, it will simply return the first argument.\ndef __call__(self, *args, **kwargs):\n       ts = L(args).map(type)[:2]\n       f = self[tuple(ts)]\n       if not f: return args[0]\n       if self.inst is not None: f = MethodType(f, self.inst)\n       return f(*args, **kwargs)"
  },
  {
    "objectID": "archive/posts/2021-06-20-logging-config-dict-issue-kedro.html",
    "href": "archive/posts/2021-06-20-logging-config-dict-issue-kedro.html",
    "title": "A logging.config.dictConfig() issue in python",
    "section": "",
    "text": "import logging\nfrom clearml import Task\nconf_logging = {\"version\":1, \n                \"formatters\":{\n                      \"simple\":{\n                             \"format\":\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"}\n                      }\n                  }\nt = Task.init(project_name=\"test\")\nlogging.config.dictConfig(conf_logging)\nlogging.info(\"INFO!\")\nlogging.debug(\"DEBUG!\")\nlogging.warning(\"WARN!\")\nprint(\"PRINT!\")\nWith this code block, you will find no print() or logging is sent to ClearML logging Console. Turns out kedro use logging.config.dictConfig(conf_logging) as the default and causing this issue.\nA quick fix is to add \"incremental\": True in the config dict. In the standard documentation, the default is False, which means the configuration will replace existing one, thus removing the clearml handlers, and causing the issue I had.\nconf_logging = {\"version\":1, \n                \"incremental\": True\n                \"formatters\":{\n                      \"simple\":{\n                             \"format\":\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"}\n                      }\n                  }"
  },
  {
    "objectID": "archive/posts/2021-03-17-pytest-data-test-truncated-error.html",
    "href": "archive/posts/2021-03-17-pytest-data-test-truncated-error.html",
    "title": "Data Test as CI",
    "section": "",
    "text": "I am running test with great_expectations that validate data in UAT and production server with CI, so it would be nice if the log can capture this.\nI created a custom error class that would do the job, however, pytest truncated my AssertionError since it is quite long.\nI am using pytest magic from https://github.com/akaihola/ipython_pytest which allow me to run pytest in a Jupyter notebook cell.\nIt is quite simple with a few tens of lines.\n\n%%writefile ipython_pytest.py\nimport os\nimport shlex\nimport sys\nfrom pathlib import Path\n\nimport tempfile\nfrom IPython.core import magic\nfrom pytest import main as pytest_main\n\n\nTEST_MODULE_NAME = '_ipytesttmp'\n\ndef pytest(line, cell):\n    with tempfile.TemporaryDirectory() as root:\n        oldcwd = os.getcwd()\n        os.chdir(root)\n        tests_module_path = '{}.py'.format(TEST_MODULE_NAME)\n        try:\n            Path(tests_module_path).write_text(cell)\n            args = shlex.split(line)\n            os.environ['COLUMNS'] = '80'\n            pytest_main(args + [tests_module_path])\n            if TEST_MODULE_NAME in sys.modules:\n                del sys.modules[TEST_MODULE_NAME]\n        finally:\n            os.chdir(oldcwd)\n\ndef load_ipython_extension(ipython):\n    magic.register_cell_magic(pytest)\n\nWriting ipython_pytest.py\n\n\n\n# !pip install pytest\n%load_ext ipython_pytest\n\nThe ipython_pytest extension is already loaded. To reload it, use:\n  %reload_ext ipython_pytest\n\n\n\n%%pytest\n\ndef test_long_assertion_error():\n    x = \"placeholder\"\n    expect = \"abcdefg\\n\"*20 # Long string\n    assert x == expect\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.3, pytest-6.2.2, py-1.10.0, pluggy-0.13.1\nrootdir: C:\\Users\\channo\\AppData\\Local\\Temp\\tmpohw9e_9w\ncollected 1 item\n\n_ipytesttmp.py F                                                         [100%]\n\n================================== FAILURES ===================================\n__________________________ test_long_assertion_error __________________________\n\n    def test_long_assertion_error():\n        x = \"placeholder\"\n        expect = \"abcdefg\\n\"*20 # Long string\n>       assert x == expect\nE       AssertionError: assert 'placeholder' == 'abcdefg\\nabc...fg\\nabcdefg\\n'\nE         + placeholder\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg...\nE         \nE         ...Full output truncated (15 lines hidden), use '-vv' to show\n\n_ipytesttmp.py:5: AssertionError\n=========================== short test summary info ===========================\nFAILED _ipytesttmp.py::test_long_assertion_error - AssertionError: assert 'pl...\n============================== 1 failed in 0.06s ==============================\n\n\nYou can see that pytest truncated my error with ... Here is how I solve ths issue\n\n%%pytest -vv\n\ndef test_long_assertion_error():\n    x = \"placeholder\"\n    expect = \"abcdefg\\n\"*20 # Long string\n    assert x == expect\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.3, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- c:\\programdata\\miniconda3\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\channo\\AppData\\Local\\Temp\\tmpyic4vcra\ncollecting ... collected 1 item\n\n_ipytesttmp.py::test_long_assertion_error FAILED                         [100%]\n\n================================== FAILURES ===================================\n__________________________ test_long_assertion_error __________________________\n\n    def test_long_assertion_error():\n        x = \"placeholder\"\n        expect = \"abcdefg\\n\"*20 # Long string\n>       assert x == expect\nE       AssertionError: assert 'placeholder' == ('abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n')\nE         + placeholder\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\n\n_ipytesttmp.py:5: AssertionError\n=========================== short test summary info ===========================\nFAILED _ipytesttmp.py::test_long_assertion_error - AssertionError: assert 'pl...\n============================== 1 failed in 0.06s =============================="
  },
  {
    "objectID": "archive/posts/2021-05-29-python-internal-series-python-gil-and-memory.html",
    "href": "archive/posts/2021-05-29-python-internal-series-python-gil-and-memory.html",
    "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
    "section": "",
    "text": "Most people first learn about GIL because of how it slows down Python program and prevent multi-threading running efficiently, however, the GIL is one of the reason why Python survive 30 years and still growing healthyly.\nGIL is nothing like the stereotype people think, legacy, slow. There are multiple benefits GIL provide:\n\nIt speed ups single thread program.\nIt is compatible with many C Program thanks to the C API of CPysthon."
  },
  {
    "objectID": "archive/posts/2021-05-29-python-internal-series-python-gil-and-memory.html#global-interpreter-lock-a.k.a-mutex-lock",
    "href": "archive/posts/2021-05-29-python-internal-series-python-gil-and-memory.html#global-interpreter-lock-a.k.a-mutex-lock",
    "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
    "section": "Global Interpreter Lock a.k.a Mutex Lock",
    "text": "Global Interpreter Lock a.k.a Mutex Lock\nTo start with, GIL is a mutex lock."
  },
  {
    "objectID": "archive/posts/2021-05-29-python-internal-series-python-gil-and-memory.html#why-gil-is-needed-in-the-first-place",
    "href": "archive/posts/2021-05-29-python-internal-series-python-gil-and-memory.html#why-gil-is-needed-in-the-first-place",
    "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
    "section": "Why GIL is needed in the first place?",
    "text": "Why GIL is needed in the first place?\nMemory management. Python use something called “reference counting”, which make it different from many modern programming lanaguage. It is what allow Python programmer to lay back and let Python take care when to release memory. Precisely, it is actually the C program controlling the memory life cycle for Python (Cpython). Cpython is known as the default Python interpreter. It first compiles Python to intermediate bytecode (.pyc files). These bytecode then being interpreted by a virtual machine ane executed. It is worth to mention that other variants of Python exist, i.e. IronPython(C#), Jython(Java), Pypy(Python) and they have different memory management mechanisms.\n\nPython Memory Management - Reference Count & Garbage Collection (gc)\n\nimport sys\n\n\nsys.getrefcount(a)\n\n3\n\n\nReference counting is a simple idea. The intuition is that if a particular object is not referenced by anything, it can be recycled since it will not be used anymore.\nFor example, the list [1] is now referenced by the variable a, so the reference count is incremented by 1.\n\nimport sys\na = [1]\nsys.getrefcount(a)\n\n2\n\n\nNote that the reference count is 2 instead of 1. 1. The first reference is a = [1] 2. When the variable a is passed to sys.getrefcount(a) as an argument, it also increases the reference count.\n\ndel a\n\nWhen del a is called, the list [1] have 0 reference count, and it is collected by Python automatically behind the scene.\n\n\nLock & Deadlock\n\n\nMemory Management"
  },
  {
    "objectID": "archive/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html",
    "href": "archive/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "",
    "text": "Note\n\n\n\nThese series are written as a quick introduction to software design for data scientists, something that is lightweight than the Design Pattern Bible - Clean Code I wish exists when I first started to learn. Design patterns refer to reusable solutions to some common problems, and some happen to be useful for data science. There is a good chance that someone else has solved your problem before. When used wisely, it helps to reduce the complexity of your code."
  },
  {
    "objectID": "archive/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#so-what-is-callback-after-all",
    "href": "archive/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#so-what-is-callback-after-all",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "So, What is Callback after all?",
    "text": "So, What is Callback after all?\nCallback function, or call after, simply means a function will be called after another function. It is a piece of executable code (function) that passed as an argument to another function. [1]\n\ndef foo(x, callback=None):\n    print('foo!')\n    if callback:\n        callback(x)\n\n\nfoo('123')\n\nfoo!\n\n\n\nfoo('123', print)\n\nfoo!\n123\n\n\nHere I pass the function print as a callback, hence the string 123 get printed after foo!."
  },
  {
    "objectID": "archive/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#why-do-i-need-to-use-callback",
    "href": "archive/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#why-do-i-need-to-use-callback",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "Why do I need to use Callback?",
    "text": "Why do I need to use Callback?\nCallback is very common in high-level deep learning libraries, most likely you will find them in the training loop. * fastai - fastai provide high-level API for PyTorch * Keras - the high-level API for Tensorflow * ignite - they use event & handler, which provides more flexibility in their opinion\n\nimport numpy as np\n\n# A boring training Loop\ndef train(x):\n    n_epochs = 3\n    n_batches = 2\n    loss = 20\n\n    for epoch in range(n_epochs):\n        for batch in range(n_batches):\n            loss = loss - 1  # Pretend we are training the model\n\n\nx = np.ones(10)\ntrain(x);\n\nSo, let’s say you now want to print the loss at the end of an epoch. You can just add 1 lines of code.\n\nThe simple approach\n\ndef train_with_print(x):\n    n_epochs = 3\n    n_batches = 2\n    loss = 20\n\n    for epoch in range(n_epochs):\n        for batch in range(n_batches):\n            loss = loss - 1 # Pretend we are training the model\n        print(f'End of Epoch. Epoch: {epoch}, Loss: {loss}')\n    return loss\n\n\ntrain_with_print(x);\n\nEnd of Epoch. Epoch: 0, Loss: 18\nEnd of Epoch. Epoch: 1, Loss: 16\nEnd of Epoch. Epoch: 2, Loss: 14\n\n\n\n\nCallback approach\nOr you call add a PrintCallback, which does the same thing but with a bit more code.\n\nclass Callback:\n    def on_epoch_start(self, x):\n        pass\n\n    def on_epoch_end(self, x):\n        pass\n\n    def on_batch_start(self, x):\n        pass\n\n    def on_batch_end(self, x):\n        pass\n\n\nclass PrintCallback(Callback):\n    def on_epoch_end(self, x):\n        print(f'End of Epoch. Loss: {x}')\n\n\ndef train_with_callback(x, callback=None):\n    n_epochs = 3\n    n_batches = 2\n    loss = 20\n\n    for epoch in range(n_epochs):\n\n        callback.on_epoch_start(loss)\n\n        for batch in range(n_batches):\n            callback.on_batch_start(loss)\n            loss = loss - 1  # Pretend we are training the model\n            callback.on_batch_end(loss)\n\n        callback.on_epoch_end(loss)\n\n\ntrain_with_callback(x, callback=PrintCallback());\n\nEnd of Epoch. Loss: 18\nEnd of Epoch. Loss: 16\nEnd of Epoch. Loss: 14\n\n\nUsually, a callback defines a few particular events on_xxx_xxx, which indicate that the function will be executed according to the corresponding condition. So all callbacks will inherit the base class Callback, and override the desired function, here we only implemented the on_epoch_end method because we only want to show the loss at the end.\nIt may seem awkward to write so many more code to do one simple thing, but there are good reasons. Consider now you need to add more features, how would you do it?\n\nModelCheckpoint\nEarly Stopping\nLearningRateScheduler\n\nYou can just add code in the loop, but it will start growing into a really giant function. It is impossible to test this function because it does 10 things at the same time. In addition, the extra code may not even be related to the training logic, they are just there to save the model or plot a chart. So, it is best to separate the logic. A function should only do 1 thing according to the Single Responsibility Principle. It helps you to reduce the complexity as it provides a nice abstraction, you are only modifying code within the specific callback you are interested.\n\n\nAdd some more sauce!\nWhen using the Callback Pattern, I can just implement a few more classes and the training loop is barely touched. Here we introduce a new class Callbacks because we need to execute more than 1 callback, it is used for holding all callbacks and executed them sequentially.\n\nclass Callbacks:\n    \"\"\"\n    It is the container for callback\n    \"\"\"\n\n    def __init__(self, callbacks):\n        self.callbacks = callbacks\n\n    def on_epoch_start(self, x):\n        for callback in self.callbacks:\n            callback.on_epoch_start(x)\n\n    def on_epoch_end(self, x):\n        for callback in self.callbacks:\n            callback.on_epoch_end(x)\n\n    def on_batch_start(self, x):\n        for callback in self.callbacks:\n            callback.on_batch_start(x)\n\n    def on_batch_end(self, x):\n        for callback in self.callbacks:\n            callback.on_batch_end(x)\n\nThen we implement the new Callback one by one, here we only have the pseudocode, but you should get the gist. For example, we only need to save the model at the end of an epoch, thus we implement the method on_epoch_end with a ModelCheckPoint callback.\n\nclass PrintCallback(Callback):\n    def on_epoch_end(self, x):\n        print(f'[{type(self).__name__}]: End of Epoch. Loss: {x}')\n\n\nclass ModelCheckPoint(Callback):\n    def on_epoch_end(self, x):\n        print(f'[{type(self).__name__}]: Save Model')\n\n\nclass EarlyStoppingCallback(Callback):\n    def on_epoch_end(self, x):\n        if x < 16:\n            print(f'[{type(self).__name__}]: Early Stopped')\n\n\nclass LearningRateScheduler(Callback):\n    def on_batch_end(self, x):\n        print(f'    [{type(self).__name__}]: Reduce learning rate')\n\nAnd we also modify the training loop a bit, the argument now takes a Callbacks which contain zero to many callbacks.\n\ndef train_with_callbacks(x, callbacks=None):\n    n_epochs = 2\n    n_batches = 3\n    loss = 20\n\n    for epoch in range(n_epochs):\n\n        callbacks.on_epoch_start(loss)                             # on_epoch_start\n        for batch in range(n_batches):\n            callbacks.on_batch_start(loss)                         # on_batch_start\n            loss = loss - 1  # Pretend we are training the model\n            callbacks.on_batch_end(loss)                           # on_batch_end\n        callbacks.on_epoch_end(loss)                               # on_epoch_end\n\n\ncallbacks = Callbacks([PrintCallback(), ModelCheckPoint(),\n                      EarlyStoppingCallback(), LearningRateScheduler()])\ntrain_with_callbacks(x, callbacks=callbacks)\n\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n[PrintCallback]: End of Epoch. Loss: 17\n[ModelCheckPoint]: Save Model\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n[PrintCallback]: End of Epoch. Loss: 14\n[ModelCheckPoint]: Save Model\n[EarlyStoppingCallback]: Early Stopped\n\n\nHopefully, it convinces you Callback makes the code cleaner and easier to maintain. If you just use plain if-else statements, you may end up with a big chunk of if-else clauses.\n\nfastai - fastai provide high-level API for PyTorch\nKeras - the high-level API for Tensorflow\nignite - they use event & handler, which provides more flexibility in their opinion"
  },
  {
    "objectID": "archive/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#reference",
    "href": "archive/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#reference",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "Reference",
    "text": "Reference\n\nhttps://stackoverflow.com/questions/824234/what-is-a-callback-function"
  },
  {
    "objectID": "archive/posts/2020-12-10-typer-create-command-line-and-use-it-anywhere.html",
    "href": "archive/posts/2020-12-10-typer-create-command-line-and-use-it-anywhere.html",
    "title": "Create python command line in few lines, and use it anywhere as a standalone tool!",
    "section": "",
    "text": "The Typer documentation has great example explaining how to use it. This is the example copied from their GitHub homepage. https://github.com/tiangolo/typer.\n\n%%writefile main1.py\nimport typer\n\n\ndef main(name: str):\n    typer.echo(f\"Hello {name}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n\nOverwriting main1.py\n\n\n\n!python main1.py world\n\nHello world\n\n\n\n!python main1.py --help \n\nUsage: main1.py [OPTIONS] NAME\n\nArguments:\n  NAME  [required]\n\nOptions:\n  --install-completion [bash|zsh|fish|powershell|pwsh]\n                                  Install completion for the specified shell.\n  --show-completion [bash|zsh|fish|powershell|pwsh]\n                                  Show completion for the specified shell, to\n                                  copy it or customize the installation.\n\n  --help                          Show this message and exit.\n\n\nHere I write a new file to main1.py and execute it as a command line with just 5 lines of code. It always comes with a help message for free. Let’s see another example.\n\n%%writefile main2.py \nimport typer\n\n\ndef main(name: str, age: int = 20, height_meters: float = 1.89, female: bool = True):\n    typer.echo(f\"NAME is {name}, of type: {type(name)}\")\n    typer.echo(f\"--age is {age}, of type: {type(age)}\")\n    typer.echo(f\"--height-meters is {height_meters}, of type: {type(height_meters)}\")\n    typer.echo(f\"--female is {female}, of type: {type(female)}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n\nWriting main2.py\n\n\n\n!python main2.py --help \n\nUsage: main2.py [OPTIONS] NAME\n\nArguments:\n  NAME  [required]\n\nOptions:\n  --age INTEGER                   [default: 20]\n  --height-meters FLOAT           [default: 1.89]\n  --female / --no-female          [default: True]\n  --install-completion [bash|zsh|fish|powershell|pwsh]\n                                  Install completion for the specified shell.\n  --show-completion [bash|zsh|fish|powershell|pwsh]\n                                  Show completion for the specified shell, to\n                                  copy it or customize the installation.\n\n  --help                          Show this message and exit.\n\n\nThis time, we can see that the help message even supplement the expected datatype. Typer will validate the type and conevrt it when possible.\n\n!python main2.py Nok --age=3\n\nNAME is Nok, of type: <class 'str'>\n--age is 3, of type: <class 'int'>\n--height-meters is 1.89, of type: <class 'float'>\n--female is True, of type: <class 'bool'>\n\n\nThe command line works file, but it only works in the same directory, and you always have to type the keyword python. With python setuptools, we can actually installed a command line and run it anywhere. It is pretty easy with just 1 trick, let’s go back to the simple Hello command.\n\n%%writefile main3.py\nimport typer\n\ndef hello(name:str):\n    typer.echo(f\"Hello {name}\")\n    \ndef main():\n    typer.run(hello)\n    \n\nOverwriting main3.py\n\n\nHere we made a few changes. 1. The logic is move to a new function named hello 2. We removed the __main__ part, as we will not call this python file directly anymore. 3. typer.run(main) is changed to typer.run(hello) and moved inside the main function.\n\nConsole Script\nWe will use setuptool to build console script, which may call the function main. The magic is using console script to install a command line interface (It creates a .exe file) that can be run anywhere. We can name our command line instead of using the filename with a pattern of command_name=file:func_name. Here our function main is inside a file main3.py, so we use hello=main3:main.\n\n%%writefile setup.py\nfrom setuptools import setup, find_packages\n\n\nsetup(\n    name=\"my_library\",\n    version=\"1.0\",\n    packages=find_packages(),\n        entry_points = {\n        'console_scripts': ['hello=main3:main']}\n)\n\nOverwriting setup.py\n\n\nThen we install the console script .\n\n!python setup.py develop\n\n\n!hello world \n\nHello world\n\n\nWe can now call hello anywhere, as it is installed as a executable.\n\n!where hello\n\nC:\\ProgramData\\Miniconda3\\Scripts\\hello.exe\n\n\nIt’s time for you to build your own commands. This can be easily extended to support multiple commands. https://github.com/tiangolo/typer"
  },
  {
    "objectID": "archive/posts/2022-07-11-europython2022-summary.html",
    "href": "archive/posts/2022-07-11-europython2022-summary.html",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "",
    "text": "EuroPython2022\nSchedule: https://ep2022.europython.eu/schedule/\nSession that I attended: #europython"
  },
  {
    "objectID": "archive/posts/2022-07-11-europython2022-summary.html#bulletproof-python-property-based-testing-with-hypothesis",
    "href": "archive/posts/2022-07-11-europython2022-summary.html#bulletproof-python-property-based-testing-with-hypothesis",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "Bulletproof Python – Property-Based Testing with Hypothesis",
    "text": "Bulletproof Python – Property-Based Testing with Hypothesis\nThe term property based testing isn’t too important. In a nutshell hypothesis is a python library that help you to write (better) tests by modifying your workflow.\n\nPrepare mock data Provide a specification of data, let hypothesis do the work\nPerform some operation\nAssert the result with expected value\n\nThe rationale behind this is\n\n\n\n\n\n\nNote\n\n\n\n** People write code don’t come up with good test. **\n\n\nFor example, you can generate integers with hypotesis.strategies.integers, it does something smart under the hood so it’s not just random number but more meaningful test. For example, you usually want to test for zero, negative number, positive number, large number. hypoethsis try to maximize the variety of tests and you just need to give it a specification.\nYou can also generate more sophisticated data, for example, a tuple of two integers, where the second integer has to be larger than the first one.\n@st.composite\n\ndef list_and_index(draw, elements=st.integers()):\n    first = draw(elements)\n    second = draw(st.integers(min_value=first + 1))\n    return (first, second)\nThink of it as your virtual QA buddy."
  },
  {
    "objectID": "archive/posts/2022-07-11-europython2022-summary.html#tdd-development-with-pytest",
    "href": "archive/posts/2022-07-11-europython2022-summary.html#tdd-development-with-pytest",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "TDD Development with pytest",
    "text": "TDD Development with pytest\nWorkflow for TDD 1. Pick one bug/missing feature 2. Write a test that fails 3. Minimal amount of code that pass - (even hard coded!) 4. Refactor\nThere are good questions asked * In case of you don’t know what’s the expected answer, how do you write test that fails meaningfully?\nI jump out of the session because of a call, so not too many comments about this session. In general I like the idea of TDD but struggle to apply the textbook version of TDD as examples are often much simpler than the real application.\nFew key points * Tests as specification about your program (What it does and what not) * Understand why you test fail and pass. * Tests are also good source of documentation.\nThinking about test first also force you to think more about the design, you almost start from pseudocode (you function doesn’t even exist!)."
  },
  {
    "objectID": "archive/posts/2022-07-11-europython2022-summary.html#introduction-to-apache-tvm",
    "href": "archive/posts/2022-07-11-europython2022-summary.html#introduction-to-apache-tvm",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "Introduction to Apache TVM",
    "text": "Introduction to Apache TVM\n\n\n\nApache TVM\n\n\n\nApache TVM is a framework that try to squeeze extra performance from specialized hardware.\n\nIn practice, the workflow roughly go like this 1. Trained a model with your favored libraries (PyTorch/Tensorflow etc) 2. Use TVM to compile and tune -> After this you get a compiled module as output 3. Use TVM python API for inference\nThe performance gains are mainly from hardware architecture that can give better performance, TVM did some architecture search and try to find the optimal one.\n\n\n\n\n\n\nNote\n\n\n\nMaybe one side benefit of this is it does not need the deep learning pipeline dependecenies since you just need the TVM Python API and the model file for inference."
  },
  {
    "objectID": "archive/posts/2022-07-11-europython2022-summary.html#how-many-modules-imported-in-python-by-default",
    "href": "archive/posts/2022-07-11-europython2022-summary.html#how-many-modules-imported-in-python-by-default",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "How many modules imported in Python by default?",
    "text": "How many modules imported in Python by default?\n\nPython Shell - 79\nIPython - 646!\nJupyter - 1020!!!\n\nIt’s quite surprising how many libraries are imported by default, and this explains why it takes some time whenever you do ipython on a shell, as the Python Interpreter is busy reading all the files and evalute it.\nSome other interesting notes: * Python use a Finder and Loader to import modules * sys.path is the order that Python Interpreter search for modules, and the first match wins (This is important if you have duplicate namespace or if you do namespace package) * Don’t do sys.path.append although you will find this very common if you do a Stackoverflow search, use environment variable PYTHONPATH=some_path instead"
  },
  {
    "objectID": "archive/posts/2022-11-08-kedro-debugging.html",
    "href": "archive/posts/2022-11-08-kedro-debugging.html",
    "title": "Demo of debugging Kedro pipeline with noetebook",
    "section": "",
    "text": "Steps to debug Kedro pipeline in a notebook\n\nRead from stack trace - find out the line of code that produce the error\nFind which node this function belongs to\nTrying to rerun the pipeline just before this node\nIf it’s not a persisted dataset, you need to change it in catalog.yml, and re-run the pipeline, error is thrown again\nsession has already been used once, so if you call session again it will throw error. (so he had a wrapper function that recreate session and do something similar to session.run\nCreate a new session or %reload_kedro?\nNow catalog.load that persisted dataset, i.e. func(catalog.load(\"some_data\"))\nCopy the source code of func to notebook, it would work if the function itself is the node function, but if it is some function buried deep down, that’s a lot more copy-pasting and change of import maybe.\nChange the source code and make it work in the notebook\nRerun the pipeline to ensure everything works\n\n\n\nRunning Session as Usual\n\n%reload_kedro\n\n[11/08/22 16:44:22] INFO     Resolved project path as:                                              __init__.py:132\n                             /Users/Nok_Lam_Chan/dev/kedro_gallery/jupyter-debug-demo.                             \n                             To set a different path, run '%reload_kedro <project_root>'                           \n\n\n\n[11/08/22 16:44:24] INFO     Kedro project jupyter_debug_demo                                       __init__.py:101\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:102\n                             'pipelines'                                                                           \n\n\n\n                    INFO     Registered line magic 'run_viz'                                        __init__.py:108\n\n\n\n\nsession\n\n<kedro.framework.session.session.KedroSession object at 0x7fc47a1a0be0>\n\n\n\n\npipelines\n\n{'__default__': Pipeline([\nNode(split_data, ['example_iris_data', 'parameters'], ['X_train', 'X_test', 'y_train', 'y_test'], 'split'),\nNode(make_predictions, ['X_train', 'X_test', 'y_train'], 'y_pred', 'make_predictions'),\nNode(report_accuracy, ['y_pred', 'y_test'], None, 'report_accuracy')\n])}\n\n\n\n\nsession.run()\n\n                    INFO     Kedro project jupyter-debug-demo                                        session.py:340\n\n\n\n[11/08/22 16:44:25] INFO     Loading data from 'example_iris_data' (CSVDataSet)...              data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'parameters' (MemoryDataSet)...                  data_catalog.py:343\n\n\n\n                    INFO     Running node: split: split_data([example_iris_data,parameters]) ->         node.py:327\n                             [X_train,X_test,y_train,y_test]                                                       \n\n\n\n                    INFO     Saving data to 'X_train' (MemoryDataSet)...                        data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'X_test' (MemoryDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'y_train' (MemoryDataSet)...                        data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'y_test' (PickleDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Completed 1 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Loading data from 'X_train' (MemoryDataSet)...                     data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'X_test' (MemoryDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_train' (MemoryDataSet)...                     data_catalog.py:343\n\n\n\n                    INFO     Running node: make_predictions: make_predictions([X_train,X_test,y_train]) node.py:327\n                             -> [y_pred]                                                                           \n\n\n\n1\n\n\n                    INFO     Saving data to 'y_pred' (PickleDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Completed 2 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Loading data from 'y_pred' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_test' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Running node: report_accuracy: report_accuracy([y_pred,y_test]) -> None    node.py:327\n\n\n\n                    ERROR    Node 'report_accuracy: report_accuracy([y_pred,y_test]) -> None' failed    node.py:352\n                             with error:                                                                           \n                             Simulate some bug here                                                                \n\n\n\n                    WARNING  There are 1 nodes that have not run.                                     runner.py:202\n                             You can resume the pipeline run from the nearest nodes with persisted                 \n                             inputs by adding the following argument to your previous command:                     \n                               --from-nodes \"report_accuracy\"                                                      \n\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/833844929.py:1 in <cell line: 1> │\n│                                                                                                  │\n│ [Errno 2] No such file or directory:                                                             │\n│ '/var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/833844929.py'                   │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/session.py:404 in run                   │\n│                                                                                                  │\n│   401 │   │   )                                                                                  │\n│   402 │   │                                                                                      │\n│   403 │   │   try:                                                                               │\n│ ❱ 404 │   │   │   run_result = runner.run(                                                       │\n│   405 │   │   │   │   filtered_pipeline, catalog, hook_manager, session_id                       │\n│   406 │   │   │   )                                                                              │\n│   407 │   │   │   self._run_called = True                                                        │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:88 in run                                │\n│                                                                                                  │\n│    85 │   │   │   self._logger.info(                                                             │\n│    86 │   │   │   │   \"Asynchronous mode is enabled for loading and saving data\"                 │\n│    87 │   │   │   )                                                                              │\n│ ❱  88 │   │   self._run(pipeline, catalog, hook_manager, session_id)                             │\n│    89 │   │                                                                                      │\n│    90 │   │   self._logger.info(\"Pipeline execution completed successfully.\")                    │\n│    91                                                                                            │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/sequential_runner.py:70 in _run                    │\n│                                                                                                  │\n│   67 │   │                                                                                       │\n│   68 │   │   for exec_index, node in enumerate(nodes):                                           │\n│   69 │   │   │   try:                                                                            │\n│ ❱ 70 │   │   │   │   run_node(node, catalog, hook_manager, self._is_async, session_id)           │\n│   71 │   │   │   │   done_nodes.add(node)                                                        │\n│   72 │   │   │   except Exception:                                                               │\n│   73 │   │   │   │   self._suggest_resume_scenario(pipeline, done_nodes, catalog)                │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:304 in run_node                          │\n│                                                                                                  │\n│   301 │   if is_async:                                                                           │\n│   302 │   │   node = _run_node_async(node, catalog, hook_manager, session_id)                    │\n│   303 │   else:                                                                                  │\n│ ❱ 304 │   │   node = _run_node_sequential(node, catalog, hook_manager, session_id)               │\n│   305 │                                                                                          │\n│   306 │   for name in node.confirms:                                                             │\n│   307 │   │   catalog.confirm(name)                                                              │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:398 in _run_node_sequential              │\n│                                                                                                  │\n│   395 │   )                                                                                      │\n│   396 │   inputs.update(additional_inputs)                                                       │\n│   397 │                                                                                          │\n│ ❱ 398 │   outputs = _call_node_run(                                                              │\n│   399 │   │   node, catalog, inputs, is_async, hook_manager, session_id=session_id               │\n│   400 │   )                                                                                      │\n│   401                                                                                            │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:366 in _call_node_run                    │\n│                                                                                                  │\n│   363 │   │   │   is_async=is_async,                                                             │\n│   364 │   │   │   session_id=session_id,                                                         │\n│   365 │   │   )                                                                                  │\n│ ❱ 366 │   │   raise exc                                                                          │\n│   367 │   hook_manager.hook.after_node_run(                                                      │\n│   368 │   │   node=node,                                                                         │\n│   369 │   │   catalog=catalog,                                                                   │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:356 in _call_node_run                    │\n│                                                                                                  │\n│   353 ) -> Dict[str, Any]:                                                                       │\n│   354 │   # pylint: disable=too-many-arguments                                                   │\n│   355 │   try:                                                                                   │\n│ ❱ 356 │   │   outputs = node.run(inputs)                                                         │\n│   357 │   except Exception as exc:                                                               │\n│   358 │   │   hook_manager.hook.on_node_error(                                                   │\n│   359 │   │   │   error=exc,                                                                     │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py:353 in run                               │\n│                                                                                                  │\n│   350 │   │   # purposely catch all exceptions                                                   │\n│   351 │   │   except Exception as exc:                                                           │\n│   352 │   │   │   self._logger.error(\"Node '%s' failed with error: \\n%s\", str(self), str(exc))   │\n│ ❱ 353 │   │   │   raise exc                                                                      │\n│   354 │                                                                                          │\n│   355 │   def _run_with_no_inputs(self, inputs: Dict[str, Any]):                                 │\n│   356 │   │   if inputs:                                                                         │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py:344 in run                               │\n│                                                                                                  │\n│   341 │   │   │   elif isinstance(self._inputs, str):                                            │\n│   342 │   │   │   │   outputs = self._run_with_one_input(inputs, self._inputs)                   │\n│   343 │   │   │   elif isinstance(self._inputs, list):                                           │\n│ ❱ 344 │   │   │   │   outputs = self._run_with_list(inputs, self._inputs)                        │\n│   345 │   │   │   elif isinstance(self._inputs, dict):                                           │\n│   346 │   │   │   │   outputs = self._run_with_dict(inputs, self._inputs)                        │\n│   347                                                                                            │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py:384 in _run_with_list                    │\n│                                                                                                  │\n│   381 │   │   │   │   f\"{sorted(inputs.keys())}.\"                                                │\n│   382 │   │   │   )                                                                              │\n│   383 │   │   # Ensure the function gets the inputs in the correct order                         │\n│ ❱ 384 │   │   return self._func(*(inputs[item] for item in node_inputs))                         │\n│   385 │                                                                                          │\n│   386 │   def _run_with_dict(self, inputs: Dict[str, Any], node_inputs: Dict[str, str]):         │\n│   387 │   │   # Node inputs and provided run inputs should completely overlap                    │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/dev/kedro_gallery/jupyter-debug-demo/src/jupyter_debug_demo/nodes.py:74 in   │\n│ report_accuracy                                                                                  │\n│                                                                                                  │\n│   71 │   │   y_pred: Predicted target.                                                           │\n│   72 │   │   y_test: True target.                                                                │\n│   73 │   \"\"\"                                                                                     │\n│ ❱ 74 │   raise ValueError(\"Simulate some bug here\")                                              │\n│   75 │   accuracy = (y_pred == y_test).sum() / len(y_test)                                       │\n│   76 │   logger = logging.getLogger(__name__)                                                    │\n│   77 │   logger.info(\"Model has accuracy of %.3f on test data.\", accuracy)                       │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nValueError: Simulate some bug here\n\n\n\n\nRead from stack trace - find out the line of code that produce the error\nFind which node this function belongs to\nTrying to rerun the pipeline just before this node\nIf it’s not a persisted dataset, you need to change it in catalog.yml, and re-run the pipeline, error is thrown again\nsession has already been used once, so if you call session again it will throw error. (so he had a wrapper function that recreate session and do something similar to session.run\nCreate a new session or %reload_kedro and re-run?\n\nThis is not efficient because in interactive workflow, these intermdiate variables is likely store in the catalog already.\n\n%reload_kedro\n\n[11/08/22 16:46:49] INFO     Resolved project path as:                                              __init__.py:132\n                             /Users/Nok_Lam_Chan/dev/kedro_gallery/jupyter-debug-demo.                             \n                             To set a different path, run '%reload_kedro <project_root>'                           \n\n\n\n[11/08/22 16:46:50] INFO     Kedro project jupyter_debug_demo                                       __init__.py:101\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:102\n                             'pipelines'                                                                           \n\n\n\n                    INFO     Registered line magic 'run_viz'                                        __init__.py:108\n\n\n\n\nsession.run()\n\n[11/08/22 16:46:53] INFO     Kedro project jupyter-debug-demo                                        session.py:340\n\n\n\n[11/08/22 16:46:54] INFO     Loading data from 'example_iris_data' (CSVDataSet)...              data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'parameters' (MemoryDataSet)...                  data_catalog.py:343\n\n\n\n                    INFO     Running node: split: split_data([example_iris_data,parameters]) ->         node.py:327\n                             [X_train,X_test,y_train,y_test]                                                       \n\n\n\n                    INFO     Saving data to 'X_train' (MemoryDataSet)...                        data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'X_test' (MemoryDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'y_train' (MemoryDataSet)...                        data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'y_test' (PickleDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Completed 1 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Loading data from 'X_train' (MemoryDataSet)...                     data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'X_test' (MemoryDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_train' (MemoryDataSet)...                     data_catalog.py:343\n\n\n\n                    INFO     Running node: make_predictions: make_predictions([X_train,X_test,y_train]) node.py:327\n                             -> [y_pred]                                                                           \n\n\n\n1\n\n\n                    INFO     Saving data to 'y_pred' (PickleDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Completed 2 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Loading data from 'y_pred' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_test' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Running node: report_accuracy: report_accuracy([y_pred,y_test]) -> None    node.py:327\n\n\n\n                    ERROR    Node 'report_accuracy: report_accuracy([y_pred,y_test]) -> None' failed    node.py:352\n                             with error:                                                                           \n                             Simulate some bug here                                                                \n\n\n\n                    WARNING  There are 1 nodes that have not run.                                     runner.py:202\n                             You can resume the pipeline run from the nearest nodes with persisted                 \n                             inputs by adding the following argument to your previous command:                     \n                               --from-nodes \"report_accuracy\"                                                      \n\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/833844929.py:1 in <cell line: 1> │\n│                                                                                                  │\n│ [Errno 2] No such file or directory:                                                             │\n│ '/var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/833844929.py'                   │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/session.py:404 in run                   │\n│                                                                                                  │\n│   401 │   │   )                                                                                  │\n│   402 │   │                                                                                      │\n│   403 │   │   try:                                                                               │\n│ ❱ 404 │   │   │   run_result = runner.run(                                                       │\n│   405 │   │   │   │   filtered_pipeline, catalog, hook_manager, session_id                       │\n│   406 │   │   │   )                                                                              │\n│   407 │   │   │   self._run_called = True                                                        │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:88 in run                                │\n│                                                                                                  │\n│    85 │   │   │   self._logger.info(                                                             │\n│    86 │   │   │   │   \"Asynchronous mode is enabled for loading and saving data\"                 │\n│    87 │   │   │   )                                                                              │\n│ ❱  88 │   │   self._run(pipeline, catalog, hook_manager, session_id)                             │\n│    89 │   │                                                                                      │\n│    90 │   │   self._logger.info(\"Pipeline execution completed successfully.\")                    │\n│    91                                                                                            │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/sequential_runner.py:70 in _run                    │\n│                                                                                                  │\n│   67 │   │                                                                                       │\n│   68 │   │   for exec_index, node in enumerate(nodes):                                           │\n│   69 │   │   │   try:                                                                            │\n│ ❱ 70 │   │   │   │   run_node(node, catalog, hook_manager, self._is_async, session_id)           │\n│   71 │   │   │   │   done_nodes.add(node)                                                        │\n│   72 │   │   │   except Exception:                                                               │\n│   73 │   │   │   │   self._suggest_resume_scenario(pipeline, done_nodes, catalog)                │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:304 in run_node                          │\n│                                                                                                  │\n│   301 │   if is_async:                                                                           │\n│   302 │   │   node = _run_node_async(node, catalog, hook_manager, session_id)                    │\n│   303 │   else:                                                                                  │\n│ ❱ 304 │   │   node = _run_node_sequential(node, catalog, hook_manager, session_id)               │\n│   305 │                                                                                          │\n│   306 │   for name in node.confirms:                                                             │\n│   307 │   │   catalog.confirm(name)                                                              │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:398 in _run_node_sequential              │\n│                                                                                                  │\n│   395 │   )                                                                                      │\n│   396 │   inputs.update(additional_inputs)                                                       │\n│   397 │                                                                                          │\n│ ❱ 398 │   outputs = _call_node_run(                                                              │\n│   399 │   │   node, catalog, inputs, is_async, hook_manager, session_id=session_id               │\n│   400 │   )                                                                                      │\n│   401                                                                                            │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:366 in _call_node_run                    │\n│                                                                                                  │\n│   363 │   │   │   is_async=is_async,                                                             │\n│   364 │   │   │   session_id=session_id,                                                         │\n│   365 │   │   )                                                                                  │\n│ ❱ 366 │   │   raise exc                                                                          │\n│   367 │   hook_manager.hook.after_node_run(                                                      │\n│   368 │   │   node=node,                                                                         │\n│   369 │   │   catalog=catalog,                                                                   │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:356 in _call_node_run                    │\n│                                                                                                  │\n│   353 ) -> Dict[str, Any]:                                                                       │\n│   354 │   # pylint: disable=too-many-arguments                                                   │\n│   355 │   try:                                                                                   │\n│ ❱ 356 │   │   outputs = node.run(inputs)                                                         │\n│   357 │   except Exception as exc:                                                               │\n│   358 │   │   hook_manager.hook.on_node_error(                                                   │\n│   359 │   │   │   error=exc,                                                                     │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py:353 in run                               │\n│                                                                                                  │\n│   350 │   │   # purposely catch all exceptions                                                   │\n│   351 │   │   except Exception as exc:                                                           │\n│   352 │   │   │   self._logger.error(\"Node '%s' failed with error: \\n%s\", str(self), str(exc))   │\n│ ❱ 353 │   │   │   raise exc                                                                      │\n│   354 │                                                                                          │\n│   355 │   def _run_with_no_inputs(self, inputs: Dict[str, Any]):                                 │\n│   356 │   │   if inputs:                                                                         │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py:344 in run                               │\n│                                                                                                  │\n│   341 │   │   │   elif isinstance(self._inputs, str):                                            │\n│   342 │   │   │   │   outputs = self._run_with_one_input(inputs, self._inputs)                   │\n│   343 │   │   │   elif isinstance(self._inputs, list):                                           │\n│ ❱ 344 │   │   │   │   outputs = self._run_with_list(inputs, self._inputs)                        │\n│   345 │   │   │   elif isinstance(self._inputs, dict):                                           │\n│   346 │   │   │   │   outputs = self._run_with_dict(inputs, self._inputs)                        │\n│   347                                                                                            │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py:384 in _run_with_list                    │\n│                                                                                                  │\n│   381 │   │   │   │   f\"{sorted(inputs.keys())}.\"                                                │\n│   382 │   │   │   )                                                                              │\n│   383 │   │   # Ensure the function gets the inputs in the correct order                         │\n│ ❱ 384 │   │   return self._func(*(inputs[item] for item in node_inputs))                         │\n│   385 │                                                                                          │\n│   386 │   def _run_with_dict(self, inputs: Dict[str, Any], node_inputs: Dict[str, str]):         │\n│   387 │   │   # Node inputs and provided run inputs should completely overlap                    │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/dev/kedro_gallery/jupyter-debug-demo/src/jupyter_debug_demo/nodes.py:74 in   │\n│ report_accuracy                                                                                  │\n│                                                                                                  │\n│   71 │   │   y_pred: Predicted target.                                                           │\n│   72 │   │   y_test: True target.                                                                │\n│   73 │   \"\"\"                                                                                     │\n│ ❱ 74 │   raise ValueError(\"Simulate some bug here\")                                              │\n│   75 │   accuracy = (y_pred == y_test).sum() / len(y_test)                                       │\n│   76 │   logger = logging.getLogger(__name__)                                                    │\n│   77 │   logger.info(\"Model has accuracy of %.3f on test data.\", accuracy)                       │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nValueError: Simulate some bug here\n\n\n\n\nNow catalog.load that persisted dataset, i.e. func(catalog.load(\"some_data\"))\n\n\ny_pred = catalog.load(\"y_pred\")\ny_test = catalog.load(\"y_test\")\n\n[11/08/22 16:47:19] INFO     Loading data from 'y_pred' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_test' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n\ncatalog.datasets.y_pred.load().head()  # This is the alternative way to use auto-discovery which can be improved\n\n0     setosa\n2     setosa\n7     setosa\n20    setosa\n21    setosa\nName: species, dtype: object\n\n\n\n\nCopy the source code of func to notebook, it would work if the function itself is the node function, but if it is some function buried deep down, that’s a lot more copy-pasting and change of import maybe.\n\n\ndef report_accuracy(y_pred: pd.Series, y_test: pd.Series):\n    \"\"\"Calculates and logs the accuracy.\n\n    Args:\n        y_pred: Predicted target.\n        y_test: True target.\n    \"\"\"\n    raise ValueError(\"Simulate some bug here\")\n    accuracy = (y_pred == y_test).sum() / len(y_test)\n    logger = logging.getLogger(__name__)\n    logger.info(\"Model has accuracy of %.3f on test data.\", accuracy)\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/1415042900.py:1 in <cell line:   │\n│ 1>                                                                                               │\n│                                                                                                  │\n│ [Errno 2] No such file or directory:                                                             │\n│ '/var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/1415042900.py'                  │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'pd' is not defined\n\n\n\nThis won’t work immediately work, a couple of copy&paste is needed\n\nmanual copy the imports\nRemove the function now - copy the source code as a cell instead\n\n\nimport pandas as pd\nimport logging\n\n\nraise ValueError(\"Simulate some bug here\")\naccuracy = (y_pred == y_test).sum() / len(y_test)\nlogger = logging.getLogger(__name__)\nlogger.info(\"Model has accuracy of %.3f on test data.\", accuracy)\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/2816569123.py:1 in <cell line:   │\n│ 1>                                                                                               │\n│                                                                                                  │\n│ [Errno 2] No such file or directory:                                                             │\n│ '/var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/2816569123.py'                  │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nValueError: Simulate some bug here\n\n\n\nAssume we know that the first line is buggy, let’s remove it\n\n# raise ValueError(\"Simulate some bug here\")\naccuracy = (y_pred == y_test).sum() / len(y_test)\nlogger = logging.getLogger(__name__)\nlogger.info(\"Model has accuracy of %.3f on test data.\", accuracy)\n# It now works - lets copy this block back into the function and rerun\n\n\nChange the source code and make it work in the notebook\nRerun the pipeline to ensure everything works\n\n\n%reload_kedro\nsession.run()\n\n[11/08/22 16:50:48] INFO     Resolved project path as:                                              __init__.py:132\n                             /Users/Nok_Lam_Chan/dev/kedro_gallery/jupyter-debug-demo.                             \n                             To set a different path, run '%reload_kedro <project_root>'                           \n\n\n\n[11/08/22 16:50:49] INFO     Kedro project jupyter_debug_demo                                       __init__.py:101\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:102\n                             'pipelines'                                                                           \n\n\n\n                    INFO     Registered line magic 'run_viz'                                        __init__.py:108\n\n\n\n                    INFO     Kedro project jupyter-debug-demo                                        session.py:340\n\n\n\n[11/08/22 16:50:50] INFO     Loading data from 'example_iris_data' (CSVDataSet)...              data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'parameters' (MemoryDataSet)...                  data_catalog.py:343\n\n\n\n                    INFO     Running node: split: split_data([example_iris_data,parameters]) ->         node.py:327\n                             [X_train,X_test,y_train,y_test]                                                       \n\n\n\n                    INFO     Saving data to 'X_train' (MemoryDataSet)...                        data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'X_test' (MemoryDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'y_train' (MemoryDataSet)...                        data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'y_test' (PickleDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Completed 1 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Loading data from 'X_train' (MemoryDataSet)...                     data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'X_test' (MemoryDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_train' (MemoryDataSet)...                     data_catalog.py:343\n\n\n\n                    INFO     Running node: make_predictions: make_predictions([X_train,X_test,y_train]) node.py:327\n                             -> [y_pred]                                                                           \n\n\n\n1\n\n\n                    INFO     Saving data to 'y_pred' (PickleDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Completed 2 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Loading data from 'y_pred' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_test' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Running node: report_accuracy: report_accuracy([y_pred,y_test]) -> None    node.py:327\n\n\n\n                    INFO     Model has accuracy of 0.933 on test data.                                  nodes.py:77\n\n\n\n                    INFO     Completed 3 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Pipeline execution completed successfully.                                runner.py:90\n\n\n\n{}\n\n\n\nIt works now!\nDebugging with interactive session is not uncommon - compare to IDE/breakpoint. * You can make plots and see the data * You can intercept the variable and continue with the program - espeically useful when it is computation intensive.\nSee more comments from Antony\nMore to optimize 1st PoC * %load_node - populate all neccessary data where the node throws error * When pipeline fail - raise something like %load_node debug=True - the traceback should have information about which node the error is coming from. * Is there anything we can use viz? Sometimes I get question from people can kedro-viz help with debugging too.\nMore to optimize: * What if the error is not in the node function but somewhere deeper in the call stack? * Handle case when the inputs are not in catalog - how to recompute the necessary inputs? Potentially we can use the backtracking to do it in a more efficient way."
  },
  {
    "objectID": "archive/posts/2019-10-19-Deskto-Notification.html",
    "href": "archive/posts/2019-10-19-Deskto-Notification.html",
    "title": "plyer - Desktop Notification with Python",
    "section": "",
    "text": "from plyer import notification\nimport random\n\nclass DesktopNotification:\n    @staticmethod\n    def notify(title='Hey~', message='Done!', timeout=10):\n        ls = ['👍','✔','✌','👌','👍','😎']\n        notification.notify(\n            title = title ,\n            message = random.choice(ls) * 3 + ' ' + message,\n            timeout = timeout # seconds\n        )\n\n\nif __name__ == '__main__':\n    DesktopNotification.notify()\nYou could add this simple code block to notify you when the program is done! A desktop notification will be prompt on the bottom right corner in Window."
  },
  {
    "objectID": "archive/posts/2021-03-21-full-stack-deep-learning-lecture-01.html",
    "href": "archive/posts/2021-03-21-full-stack-deep-learning-lecture-01.html",
    "title": "Full Stack Deep Learning Notes - Lecture 01",
    "section": "",
    "text": "Models become hardware agnostic\nCode is clear to read because engineering code is abstracted away\nEasier to reproduce\nMake fewer mistakes because lightning handles the tricky engineering\nKeeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate\nLightning has dozens of integrations with popular machine learning tools.\nTested rigorously with every new PR. We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs.\nMinimal running speed overhead (about 300 ms per epoch compared with pure PyTorch)."
  },
  {
    "objectID": "archive/posts/2021-04-16-full-stack-deep-learning-lecture-03.html",
    "href": "archive/posts/2021-04-16-full-stack-deep-learning-lecture-03.html",
    "title": "Full Stack Deep Learning Notes - Lecture 03 - Recurrent Neural Network",
    "section": "",
    "text": "LSTM\nReference: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\nThe diagrams are from Chris Colah’s blog.\n\n\n\nRNN\nLSTM\n\n\n\n\n\n\n\n\n\n Forget Gate - Control the magnitude of cell state should be kept. Sigmoid range from (0 to 1). If 0, it means we should throw away the state cell, if 1 we keep everything.  * Input Gate - Control what relevant information can be added from the current step. It takes hidden step from last step and the current input into consideration.  * Output Gate - finalize the next hidden state\n\n\n# Google Neurl Machine Translation (GNMT)\nIt more or less follow the attention mechanism described here.\nhttps://blog.floydhub.com/attention-mechanism/#luong-att-step6\n\n\n\nattention_gnmt\n\n\n1.If you take the dot product of 1 encoder vector (at t_i) and decoder, you get a scalar. (Alignment Score) (1,h) * (h,1) -> (1,1) 2. If encoder have 5 time_step, repeat the above steps -> You get a vector with length of 5 (A vector of Alignment Scores) (5,h) (h,1) -> (5,1) 3. Take softmax of the alignments scores -> (attention weights which sum to 1) (5,1) 4. Take dot product of encoders state with attention weights (h, 5)  (5, 1) -> (h, 1), where h stands for dimension of hidden state. The result is a “Context Vector”"
  },
  {
    "objectID": "archive/posts/2021-11-18-what-can-we-learn-from-shipping-crisis-as-a-data-scientist.html",
    "href": "archive/posts/2021-11-18-what-can-we-learn-from-shipping-crisis-as-a-data-scientist.html",
    "title": "What can we learn from Shipping Crisis as a Data Scientist?",
    "section": "",
    "text": "Even if you are not working in shipping industry, you probably heard about shipping cost is skyrocking for the last year. COVID is clearly the initial disruption, but the story does not end there. Recently, Long Beach’s port congestion is at a historcial scale, there are now more than 70+ ships waiting outside the port, the typical number is 1 or 2.\nYou may think the terminal must be busy as hell, so did I, but it is actualy far from the truth. In fact, the port is actually paralyzed. The reason surprised me a lot, it is not because of lacking of driver or empty containers, but yard space. Container are being unloaded from ships, then they are being put at the container yard before they go into depot or being stuffed again.\nOn a high level, it is caused by a negative feedback loop which COVID probably contributed a lot, as it caused a lot of disruption to the supply chain.\n\nPort Congestion -> Containers pilled up at container yard since it is waiting to be loaded on ship\nContainer yard space is taken up by cotnainers, less space is available\nA container need to be put on a chassis before it is loaded, but as the container yard is full, empty containers stuck on the chassis and they need to be unloaded before you put a stuffed container.\nLess Chassis is available to load stuff, so it further slow down the process\nThe loop complete and it starts from 1 again\n\n\n\n\nPort Congestion Feedback Loop\n\n\nThis is a simplified story, you can find more details from this twitter thread from flexport’s CEO Ryan. There are more constraints that making this load/unload process inefficient, so the whole process is jammed. Think about a restaurant with limited amount of trays, you need to get a tray if you want to get food. But because there are too many customers, it jammed the door . So there are many customers holding an empty tray while many food are waiting to be served.\nRyan point out a very important lesson here, that is, you need to choose your bottleneck, and it should really be the capital intensive assets. Going back to our restaurant’s analogy, chef and space is probably the most expensive assets, so we should try to keep the utilization high. A simple solution is to buy more trays, so that it won’t be jammed. Ofcourse, you can also find a larger space, build a bigger door, but that will cost you more money too.\nFor shipping, the terminal’s crane should be the most capital intensive, so we should try our best to keep it working 24/7 to digest the terminal queue.\nThis is a simple idea yet it is powerful and it strikes me hard. As a data scientist, I work on optimization problem. To maximize the output of a system, we can use linear programming. When we are solving this problem, we are asking question like this.\n\nGiven x1 Terminals, x2 drivers, x3 containers, x4 ships, what is the maximize output of this system and how do you arrange them to achieve so?\n\nHowever, if you are a product/business analyst, a better question may be > What is the output of this system if I add more container yard space?\nBy changing the input of the system, you may achieve much better result. But as a data scientist, we often stuck in a mode that how do we optimize x metrics with these features. So we may end up spending months and try to schedule ships and driver perfectly to load 10% more container, but you can actually increase loading efficiency by 50% simply by adding more yard space. It feels like cheating as a scientific question, since this is not we asked originally, but this happened a lot in a business context.\nWe are not trying to find the best algorithm to solve a problem, the algorithm is just one way of doing it. We may get surprising result by just tweaking the asked question a little bit.\nI am curious about what is the limiting factor in our current supply chain system, and how sensitive it is to the environment. Is forecasting & optimization the right way to do it? Do we actually need a precise forecast or we can have a bit of redundancy (like in this case, having extra yard space which could be a waste but improve the system robustness)? This is questions that we need to ask ourselves constantly, as the true question is often not asked, but explored after lots of iterations. We need to, and we have to ask the right question, and that is an art more elegant than an algorithm in my opinion.\nI do not know if Ryan’s word are 100% true, but it reminds me an important lesson. The right solution (question) may be simple, but it may not be obvious. Have we exploited all the simple solution before we went nuts with fancy algorithms?\np.s. Apologised as I don’t have time to proofread but simply try to write down the snapshot of my current mind [2021-11-18]\n\nReference\n{% twitter https://twitter.com/typesfast/status/1451543776992845834?s=20 %} https://twitter.com/typesfast/status/1451543776992845834?s=20 https://www.facebook.com/669645890/posts/10159859049175891/ unroll version: https://threadreaderapp.com/thread/1451543776992845834.html"
  },
  {
    "objectID": "archive/posts/index.html",
    "href": "archive/posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "archive/posts/2022-05-30-mocking-with-pytest-patch.html#mock",
    "href": "archive/posts/2022-05-30-mocking-with-pytest-patch.html#mock",
    "title": "Testing with Mocking",
    "section": "Mock",
    "text": "Mock\n\nmock = Mock()\n\nWith the Mock object, you can treat it like a magic object that have any attributes or methods.\n\nmock.super_method(), mock.attribute_that_does_not_exist_at_all\n\n(<Mock name='mock.super_method()' id='1587554283232'>,\n <Mock name='mock.attribute_that_does_not_exist_at_all' id='1587554282512'>)\n\n\n\nstr(mock)\n\n\"<Mock id='1587554282848'>\""
  },
  {
    "objectID": "archive/posts/2022-05-30-mocking-with-pytest-patch.html#magicmock",
    "href": "archive/posts/2022-05-30-mocking-with-pytest-patch.html#magicmock",
    "title": "Testing with Mocking",
    "section": "MagicMock",
    "text": "MagicMock\nThe “magic” comes from the magic methods of python object, for example, when you add two object together, it is calling the __add__ magic method under the hook.\n\nmock + mock\n\nTypeError: unsupported operand type(s) for +: 'Mock' and 'Mock'\n\n\n\nmagic_mock = MagicMock()\n\n\nmagic_mock + magic_mock\n\n<MagicMock name='mock.__add__()' id='1587563722784'>\n\n\nWith MagicMock, you get these magic methods for free, this is why adding two mock will not throw an error but adding two Mock will result in a TypeError\nLet say we want to mock the pandas.read_csv function, because we don’t actually want it to read a data, but just return some mock data whenever it is called. It’s easier to explain with an example."
  },
  {
    "objectID": "archive/posts/2022-05-30-mocking-with-pytest-patch.html#mocker.patch-with-createtrue",
    "href": "archive/posts/2022-05-30-mocking-with-pytest-patch.html#mocker.patch-with-createtrue",
    "title": "Testing with Mocking",
    "section": "mocker.patch with create=True",
    "text": "mocker.patch with create=True\n\n%%pytest\nimport pandas as pd\n\ndef test_read_csv(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n    mocker.patch(\"pandas.read_special_csv\", return_value = \"fake_data\", create=False)\n    assert pd.read_special_csv(\"some_data\") == \"fake_data\"\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\nrootdir: C:\\Users\\lrcno\\AppData\\Local\\Temp\\tmpzbddlxxg\nplugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0\ncollected 1 item\n\n_ipytesttmp.py F                                                         [100%]\n\n================================== FAILURES ===================================\n________________________________ test_read_csv ________________________________\n\nmocker = <pytest_mock.plugin.MockFixture object at 0x00000171B28B1820>\n\n    def test_read_csv(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n>       mocker.patch(\"pandas.read_special_csv\", return_value = \"fake_data\", create=False)\n\n_ipytesttmp.py:4: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\..\\..\\miniconda3\\lib\\site-packages\\pytest_mock\\plugin.py:193: in __call__\n    return self._start_patch(self.mock_module.patch, *args, **kwargs)\n..\\..\\..\\..\\miniconda3\\lib\\site-packages\\pytest_mock\\plugin.py:157: in _start_patch\n    mocked = p.start()\n..\\..\\..\\..\\miniconda3\\lib\\unittest\\mock.py:1529: in start\n    result = self.__enter__()\n..\\..\\..\\..\\miniconda3\\lib\\unittest\\mock.py:1393: in __enter__\n    original, local = self.get_original()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <unittest.mock._patch object at 0x00000171B28B10D0>\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n>           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: <module 'pandas' from 'c:\\\\users\\\\lrcno\\\\miniconda3\\\\lib\\\\site-packages\\\\pandas\\\\__init__.py'> does not have the attribute 'read_special_csv'\n\n..\\..\\..\\..\\miniconda3\\lib\\unittest\\mock.py:1366: AttributeError\n=========================== short test summary info ===========================\nFAILED _ipytesttmp.py::test_read_csv - AttributeError: <module 'pandas' from ...\n============================== 1 failed in 0.43s ==============================\n\n\nNow we fail the test because pandas.read_special_csv does not exist. However, with create=True you can make the test pass again. Normally you won’t want to do this, but it is an option that available.\n\n%%pytest\nimport pandas as pd\n\ndef test_read_csv(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n    mocker.patch(\"pandas.read_special_csv\", return_value = \"fake_data\", create=True)\n    assert pd.read_special_csv(\"some_data\") == \"fake_data\"\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\nrootdir: C:\\Users\\lrcno\\AppData\\Local\\Temp\\tmphqbckliw\nplugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0\ncollected 1 item\n\n_ipytesttmp.py .                                                         [100%]\n\n============================== 1 passed in 0.10s ==============================\n\n\nMore often, you would want your mock resemble your real object, which means it has the same attributes and method, but it should fails when the method being called isn’t valid. You may specify the return_value with the mock type\n\n%%pytest -vvv\nimport pandas as pd\nfrom unittest.mock import Mock\nimport pytest\n\ndef test_read_csv_valid_method(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n    mocker.patch(\"pandas.read_csv\", return_value = Mock(pd.DataFrame))\n    df =  pd.read_csv(\"some_data\")\n    df.mean()  # A DataFrame method\n\ndef test_read_csv_invalid_method(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n    mocker.patch(\"pandas.read_csv\", return_value = Mock(pd.DataFrame))\n    df =  pd.read_csv(\"some_data\")\n    with pytest.raises(Exception):\n        df.not_a_dataframe_method()\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- c:\\users\\lrcno\\miniconda3\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\lrcno\\AppData\\Local\\Temp\\tmpyfiqtkoy\nplugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0\ncollecting ... collected 2 items\n\n_ipytesttmp.py::test_read_csv_valid_method PASSED                        [ 50%]\n_ipytesttmp.py::test_read_csv_invalid_method PASSED                      [100%]\n\n============================== 2 passed in 0.16s =============================="
  },
  {
    "objectID": "archive/posts/2021-03-27-microsoft-azure-dp100.html",
    "href": "archive/posts/2021-03-27-microsoft-azure-dp100.html",
    "title": "Microsoft Azure - DP100",
    "section": "",
    "text": "Last Updated: 2021-04-22"
  },
  {
    "objectID": "archive/posts/2021-03-27-microsoft-azure-dp100.html#official-suggested-materials",
    "href": "archive/posts/2021-03-27-microsoft-azure-dp100.html#official-suggested-materials",
    "title": "Microsoft Azure - DP100",
    "section": "Official Suggested Materials",
    "text": "Official Suggested Materials\n\n❗ https://docs.microsoft.com/en-us/learn/paths/build-ai-solutions-with-azure-ml-service/ - This should be your main focus, try to finish the labs and read the tutorials. You need to understand the use case of different products and familiar yourself with the syntax of Azure ML SDK etc.\nhttps://docs.microsoft.com/en-us/learn/paths/create-no-code-predictive-models-azure-machine-learning/ - You should at least finish 1 of the lab to get some sense of the UI. It would be included in the exam for sure (2-3 questions maybe)\nhttps://docs.microsoft.com/en-us/learn/paths/create-machine-learn-models/ - I didn’t spend much time on it as most of them are baisc data science concepts. You would need to how to apply different types of models (Regression/Classification/Time Series) & AutoML for given scenario."
  },
  {
    "objectID": "archive/posts/2021-03-27-microsoft-azure-dp100.html#compute-target",
    "href": "archive/posts/2021-03-27-microsoft-azure-dp100.html#compute-target",
    "title": "Microsoft Azure - DP100",
    "section": "Compute Target",
    "text": "Compute Target\nMachine Learning Studio - single/multi - Development/Experiment - Local Machine/Cloud VM. - Scale up to larger data/distributed - training compute target - Azure Machine Learning compute cluster - Azure Machine Learning compute instance - Deploy Compute Target (You need to able to judge the most appropiate option based on the context.) - Local web service - Azure Kubernetes Service (AKS) - Azure Container Instances - Azure Machine Learning compute clusters (Batch Inference)"
  },
  {
    "objectID": "archive/posts/2021-03-27-microsoft-azure-dp100.html#datastore",
    "href": "archive/posts/2021-03-27-microsoft-azure-dp100.html#datastore",
    "title": "Microsoft Azure - DP100",
    "section": "DataStore",
    "text": "DataStore\n\nAzure Storage (blob and file containers)\nAzure Data Lake stores\nAzure SQL Database\nAzure Databricks file system (DBFS)"
  },
  {
    "objectID": "archive/posts/2022-02-10-journey-of-understanding-python-and-programming-langauge.html",
    "href": "archive/posts/2022-02-10-journey-of-understanding-python-and-programming-langauge.html",
    "title": "Journey of understanding Python and programming language",
    "section": "",
    "text": "To be written… # What is Python Interpreter?\n\nWhat is Bytecode?\n\n\nPython Virtual Machine\n\n\nCompiler\n\n\nEBNF Grammar\n\n\nLLVM"
  },
  {
    "objectID": "archive/posts/2020-11-10-pandas memory optimization.html",
    "href": "archive/posts/2020-11-10-pandas memory optimization.html",
    "title": "Optimizing pandas - Reducing 90% memory footprint - updated version",
    "section": "",
    "text": "Todo\n\nTWO options to automatically optimize pandas\n\nWe can check some basic info about the data with pandas .info() function\n\ndf_gamelogs.info(memory_usage='deep')\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 171907 entries, 0 to 171906\nColumns: 161 entries, date to acquisition_info\ndtypes: float64(77), int64(6), object(78)\nmemory usage: 860.5 MB\n\n\nWe can see the data has 171907 rows and 161 columns and 859.4 MB memory. Let’s see how much we can optimize dtype_diet.\n\nproposed_df = report_on_dataframe(df_gamelogs, unit=\"MB\")\nproposed_df\n\n\n\n\n\n  \n    \n      \n      Current dtype\n      Proposed dtype\n      Current Memory (MB)\n      Proposed Memory (MB)\n      Ram Usage Improvement (MB)\n      Ram Usage Improvement (%)\n    \n    \n      Column\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      date\n      int64\n      int32\n      671.574219\n      335.818359\n      335.755859\n      49.995347\n    \n    \n      number_of_game\n      int64\n      int8\n      671.574219\n      84.001465\n      587.572754\n      87.491857\n    \n    \n      day_of_week\n      object\n      category\n      5036.400391\n      84.362793\n      4952.037598\n      98.324939\n    \n    \n      v_name\n      object\n      category\n      5036.400391\n      174.776367\n      4861.624023\n      96.529736\n    \n    \n      v_league\n      object\n      category\n      4952.461426\n      84.359375\n      4868.102051\n      98.296617\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      h_player_9_id\n      object\n      category\n      4955.471680\n      412.757324\n      4542.714355\n      91.670675\n    \n    \n      h_player_9_name\n      object\n      category\n      5225.463379\n      421.197266\n      4804.266113\n      91.939523\n    \n    \n      h_player_9_def_pos\n      float64\n      float16\n      671.574219\n      167.940430\n      503.633789\n      74.993020\n    \n    \n      additional_info\n      object\n      category\n      2714.671875\n      190.601074\n      2524.070801\n      92.978854\n    \n    \n      acquisition_info\n      object\n      category\n      4749.209961\n      84.070801\n      4665.139160\n      98.229794\n    \n  \n\n161 rows × 6 columns\n\n\n\n\nnew_df = optimize_dtypes(df_gamelogs, proposed_df) # Avoid Type conversion error from int64 to int 8 with NA\n\n\nprint(f'Original df memory: {df_gamelogs.memory_usage(deep=True).sum()/1024/1024} MB')\nprint(f'Propsed df memory: {new_df.memory_usage(deep=True).sum()/1024/1024} MB')\n\nOriginal df memory: 860.500262260437 MB\nPropsed df memory: 79.04368686676025 MB\n\n\n\n# sell_prices.csv.zip Source data: https://www.kaggle.com/c/m5-forecasting-uncertainty/\ndf = pd.read_csv('../data/sell_prices.csv.zip')\n\n\nproposed_df = report_on_dataframe(df, unit=\"MB\")\nnew_df = optimize_dtypes(df, proposed_df) # Avoid Type conversion error from int64 to int 8 with NA\n\n\nprint(f'Original df memory: {df_gamelogs.memory_usage(deep=True).sum()/1024/1024} MB')\nprint(f'Propsed df memory: {new_df.memory_usage(deep=True).sum()/1024/1024} MB')\n\nOriginal df memory: 860.500262260437 MB\nPropsed df memory: 85.09655094146729 MB\n\n\n\n## hide\n## collapse-hide"
  },
  {
    "objectID": "archive/posts/2021-08-18-python-file-not-found-long-file-path-window.html",
    "href": "archive/posts/2021-08-18-python-file-not-found-long-file-path-window.html",
    "title": "Python FileNotFoundError or You have a really long file path?",
    "section": "",
    "text": "FileNotFoundError? Not so quick\n\n\n\n\n\nscreenshot\n\n\nTo illustrate the issue, I perpared some fake file. The script is simple, it just read a file with plain text, except that the filename is really long.\n\n\n\n\n\nerror\n\n\nUnforuntately, even though the file exists, Python gives me a FileNotFoundError, how come? However long debugging, I found out that this is related to the filename that exist only on Windows.\nThis StackOverflow thread explain this issue.\n\nMaximum Path Length Limitation\nIn the Windows API (with some exceptions discussed in the following paragraphs), the maximum length for a path is MAX_PATH, which is defined as 260 characters. A local path is structured in the following order: drive letter, colon, backslash, name components separated by backslashes, and a terminating null character. For example, the maximum path on drive D is “D:*some 256-character path string*” where “” represents the invisible terminating null character for the current system codepage. (The characters < > are used here for visual clarity and cannot be part of a valid path string.)\n\n\n\nSolution - Registry\nUpdating your Registry can solve this problem.\n\n\n\n\n\nHello World\n\n\nAfter applying the config, I can finally read the file. :)\n\n\nSummary (TLDR version)\n\nWindow filesystem only allow 256 characters, beyond that you will have trouble to open the file.\nPython will not be able to see this file and throw FileNotFoundError (I have no idea, anyone know why is that?)\nYou can update registry to enable long file path in Window to fix this issue.\n\n(Bonus: Window actually has weird behavior for long filepath, you can try to break it with different ways.)"
  },
  {
    "objectID": "archive/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html",
    "href": "archive/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html",
    "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
    "section": "",
    "text": "I have teamed up with a friend to participate in the Bengali Image Classification Competition. We struggled to get a high rank in the Public leaderboard throughout the competition. In the end, the result is a big surprise to everyone as the leaderboard shook a lot.\nThe final private score was much lower than the public score. It suggests that most participants are over-fitting Public leaderboard."
  },
  {
    "objectID": "archive/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#evaluation-metrics",
    "href": "archive/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#evaluation-metrics",
    "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
    "section": "Evaluation Metrics",
    "text": "Evaluation Metrics\nThe competition use macro-recall as the evaluation metric. In general, people get >96% recall in training, the tops are even getting >99% recall.\n\n\nCode\npython\nimport numpy as np\nimport sklearn.metrics\n\nscores = []\nfor component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n    y_true_subset = solution[solution[component] == component]['target'].values\n    y_pred_subset = submission[submission[component] == component]['target'].values\n    scores.append(sklearn.metrics.recall_score(\n        y_true_subset, y_pred_subset, average='macro'))\nfinal_score = np.average(scores, weights=[2,1,1])"
  },
  {
    "objectID": "archive/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#model-bigger-still-better",
    "href": "archive/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#model-bigger-still-better",
    "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
    "section": "Model (Bigger still better)",
    "text": "Model (Bigger still better)\nWe start with xresnet50, which is a relatively small model. As we have the assumption that this classification task is a very standard task, therefore the difference of model will not be the most important one. Thus we pick xresnet50 as it has a good performance in terms of accuracy and train relatively fast.\nNear the end of the competition, we switch to a larger model se-resnext101. It requires triple training time plus we have to scale down the batch size as it does not fit into the GPU memory. Surprisingly (maybe not surprising to everyone), the bigger model did boost the performance more than I expected with ~0.3-0.5% recall. It is a big improvement as the recall is very high (~0.97), in other words, it reduces ~10% error solely by just using a better model, not bad!"
  },
  {
    "objectID": "archive/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#augmentation",
    "href": "archive/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#augmentation",
    "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
    "section": "Augmentation",
    "text": "Augmentation\nThere are never “enough” data for deep learning, so we always try our best to collect more data. Since we cannot collect more data, we need data augmentation. We start with rotation + scale. We also find MixUp and CutMix is very effective to boost the performance. It also gives us roughly 10% boost initially from 0.96 -> 0.964 recall.\n\nCutMix & MixUp\n\n\n\n\n\nExample of Augmentation\n\n\nMixup is simple, if you know about photography, it is similar to have double exposure of your photos. It overlays two images (cat+dog in this case) by sampling weights. So instead of prediction P(dog) = 1, the new target could become P(dog) = 0.8 and P(cat) = 0.2.\nCutMix shares a similar idea, instead of overlay 2 images, it crops out a certain ratio of the image and replaces it with another one.\nIt always surprises me that these augmented data does not make much sense to a human, but it is very effective to improve model accuracy and reduce overfitting empirically."
  },
  {
    "objectID": "archive/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#hydra",
    "href": "archive/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#hydra",
    "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
    "section": "Hydra",
    "text": "Hydra\n\n\n\nHydra for configuration composition\n\n\nIt is often a good idea to make your experiment configurable. We use Hydra for this purpose and it is useful to compose different configuration group. By making your hyper-paramters configurable, you can define an experiment by configuration files and run multiple experiments. By logging the configuration with the training statistics, it is easy to do cross-models comparison and find out which configuration is useful for your model.\nI have written an short example for how to use Hydra."
  },
  {
    "objectID": "archive/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#wandb",
    "href": "archive/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#wandb",
    "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
    "section": "Wandb",
    "text": "Wandb\nwandb (Weight & Biases) does a few things. It provides built-in functions that automatically log all your model statistics, you can also log your custom metrics with simple functions.\n\nCompare the configuration of different experiments to find out the model with the best performance.\nBuilt-in function for logging model weights and gradient for debugging purpose.\nLog any metrics that you want\n\nAll of these combined to make collaboration experience better. It is really important to sync the progress frequently and getting everyone results in a single platform makes these conversations easier.\n\n\n\n\n\nimage.png"
  },
  {
    "objectID": "archive/index.html",
    "href": "archive/index.html",
    "title": "Archive",
    "section": "",
    "text": "Blog\n\n\nAnything blog about Python\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nDemo of debugging Kedro pipeline with noetebook\n\n\n\n\n\n\n\npython\n\n\nkedro\n\n\n\n\nDemo of debugging Kedro pipeline with noetebook\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\n  \n\n\n\n\nEuroPython 2022 - Conference Notes & Summary\n\n\n\n\n\n\n\neuropython\n\n\n\n\nNotes for EuroPython2022 (Update daily)\n\n\n\n\n\n\nJul 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting with Mocking\n\n\n\n\n\n\n\npython\n\n\n\n\nI have been working with on some unit tests recently with mocking. There are some traps that I falled into and I want to document it here.\n\n\n\n\n\n\nMay 30, 2022\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJourney of understanding Python and programming language\n\n\n\n\n\n\n\npython\n\n\n\n\nSome unfinished notes about Python lower level details.\n\n\n\n\n\n\nFeb 10, 2022\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nWhat can we learn from Shipping Crisis as a Data Scientist?\n\n\n\n\n\n\n\nproduct\n\n\n\n\nThe Long Beach port congestion could teach us a lot about data science in real world.\n\n\n\n\n\n\nNov 18, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nPython FileNotFoundError or You have a really long file path?\n\n\n\n\n\n\n\npython\n\n\n\n\nToday I encountered an interesting bug that I think it is worth to write it down for my future self.\n\n\n\n\n\n\nAug 18, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 Minutes Data Science Design Patterns I - Callback\n\n\n\n\n\n\n\npython\n\n\ndesign pattern\n\n\nsoftware\n\n\n\n\nA mini collections of design pattern for Data Science - Starting with callbacks.\n\n\n\n\n\n\nJul 10, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA logging.config.dictConfig() issue in python\n\n\n\n\n\n\n\npython\n\n\n\n\nlogging.config.dictConfig()\n\n\n\n\n\n\nJun 20, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Internal Series - Global Interpreter Lock (GIL) and Memory Management\n\n\n\n\n\n\n\npython-internal\n\n\n\n\nDespite the bad reputation of GIL, it was arguably designed on purpose. The GIL actually comes with a lot of benefit.\n\n\n\n\n\n\nMay 29, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nFull Stack Deep Learning Notes - Lecture 03 - Recurrent Neural Network\n\n\n\n\n\n\n\nfsdl\n\n\n\n\nLecture & Lab notes - This lecture is about Recurrent Neural Network. Key concetps included input gate, forget gate, cell state, and output gate. It also explains how attention mechanism works for a encoder-decoder based architecture.\n\n\n\n\n\n\nApr 16, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft Azure - DP100\n\n\n\n\n\n\n\nazure\n\n\n\n\nThis note helps you to prepare the Azure Assoicate Data Scientist DP-100 exam. I took DP100 in Mar 2021 and includes some important notes for study. Particularly, syntax types questions are very common. You need to study the lab and make sure you understand and remember some syntax to pass this exam.\n\n\n\n\n\n\nMar 27, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFull Stack Deep Learning Notes - Lecture 01\n\n\n\n\n\n\n\nfsdl\n\n\n\n\nLecture & Lab notes, explain DataModules, Trainer LightningModule.\n\n\n\n\n\n\nMar 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Test as CI\n\n\n\n\n\n\n\npython\n\n\n\n\nUnit Test with Pytest. I want to display a full information rich string into my CI Log, but it is trimmed by Python. Here is a simple fix to make it display full string in the log file.\n\n\n\n\n\n\nMar 17, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate python command line in few lines, and use it anywhere as a standalone tool!\n\n\n\n\n\n\n\npython\n\n\ncli\n\n\ntyper\n\n\n\n\nCreating python command line could be useful for a lot of tools. Traditionally, argparse has been used heavily, a new library called typer leverage python type hint, that makes creating command line interface in Python much pleasant.\n\n\n\n\n\n\nDec 10, 2020\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing pandas - Reducing 90% memory footprint - updated version\n\n\n\n\n\n\n\npython\n\n\n\n\nHave you ever deal with data files that does not fit into your memory? Here is a function that just trim memory footprint for you. This post is base on https://www.dataquest.io/blog/pandas-big-data/ and updated with a new automated functions from https://github.com/ianozsvald/dtype_diet/blob/master/dtype_diet.py\n\n\n\n\n\n\nNov 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\nLesson learnt from Kaggle - Bengali Image Classification Competition\n\n\n\n\n\n\n\npython\n\n\nkaggle\n\n\n\n\nMy first silver medal on Kaggles Competition - Bengali Image Classification Competition. 10 lessons learn!\n\n\n\n\n\n\nMar 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe missing piece in Python tutorial - What is dispatch why you should care\n\n\n\n\n\n\n\npython\n\n\nfastai\n\n\n\n\nDispatch is an amazing useful features which is underused in Python. In this article, I will show you how you can use it to make Python more powerful.\n\n\n\n\n\n\nFeb 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplyer - Desktop Notification with Python\n\n\n\n\n\n\n\npython\n\n\n\n\nDesktop notifiaction with Python\n\n\n\n\n\n\nOct 19, 2019\n\n\nnoklam\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2023-04-12-Jupyter-Kernel/upyter-kernel-explained-chatgpt.html",
    "href": "blog/posts/2023-04-12-Jupyter-Kernel/upyter-kernel-explained-chatgpt.html",
    "title": "What is a Jupyter Kernel? By ChatGPT",
    "section": "",
    "text": "{% twitter https://twitter.com/edublancas/status/1643259630942838785 %}\nI read this on Twitter and it gets me thinking, do I really understand what is a “Kernel”? Software Engineering jargon can be confusing, particularly when they are overloaded. So I have a little chat with my best friend - ChatGPT.\nHere is the conversation (I edited it a bit to reduce noise)"
  },
  {
    "objectID": "blog/posts/2023-04-12-Jupyter-Kernel/upyter-kernel-explained-chatgpt.html#what-is-a-kernel-for-computer",
    "href": "blog/posts/2023-04-12-Jupyter-Kernel/upyter-kernel-explained-chatgpt.html#what-is-a-kernel-for-computer",
    "title": "What is a Jupyter Kernel? By ChatGPT",
    "section": "What is a Kernel for computer?",
    "text": "What is a Kernel for computer?\n\nIn computer science, a kernel is a fundamental part of an operating system that acts as a bridge between software and hardware. It is a low-level program that controls and manages the resources of a computer, including memory, input/output operations, and system calls.\n\n\nThe kernel is responsible for handling all requests from applications or other software programs to access the computer’s hardware resources, and it provides a layer of abstraction that shields the higher-level software from the complexities of the underlying hardware. In essence, the kernel is the core of the operating system, and it is responsible for managing all of the system’s resources and providing a stable and secure platform for running software applications.\n\n\nThere are different types of kernels, including monolithic kernels, microkernels, hybrid kernels, and exokernels. Each type has its own advantages and disadvantages, depending on the specific requirements of the operating system and the hardware it runs on."
  },
  {
    "objectID": "blog/posts/2023-04-12-Jupyter-Kernel/upyter-kernel-explained-chatgpt.html#what-does-kernel-means-in-jupyter",
    "href": "blog/posts/2023-04-12-Jupyter-Kernel/upyter-kernel-explained-chatgpt.html#what-does-kernel-means-in-jupyter",
    "title": "What is a Jupyter Kernel? By ChatGPT",
    "section": "What does kernel means in Jupyter",
    "text": "What does kernel means in Jupyter\n\nIn the context of Jupyter, a kernel refers to a computational engine that executes the code in a notebook. Each notebook is associated with a kernel that runs the code in the notebook cells and communicates the results back to the notebook interface.\n\n\nThe kernel in Jupyter is responsible for executing the code in the notebook cells, managing the state of the kernel session, and communicating with the notebook interface. The kernel provides the computing power for the notebook and enables the user to interact with the notebook in real-time.\n\n\nThere are many different kernels available for use with Jupyter, including kernels for Python, R, Julia, and many other programming languages. Each kernel is specific to the language it supports and provides the necessary tools and libraries to execute code in that language."
  },
  {
    "objectID": "blog/posts/2023-04-12-Jupyter-Kernel/upyter-kernel-explained-chatgpt.html#whats-the-difference",
    "href": "blog/posts/2023-04-12-Jupyter-Kernel/upyter-kernel-explained-chatgpt.html#whats-the-difference",
    "title": "What is a Jupyter Kernel? By ChatGPT",
    "section": "What’s the difference?",
    "text": "What’s the difference?\n\nThe main difference between the kernel in Jupyter and the kernel in an operating system is that the Jupyter kernel is a software component that runs within the Jupyter environment, whereas the operating system kernel is a low-level program that manages the computer’s resources. While both kernels have different functions and operate at different levels of the computing stack, they are both critical components of their respective systems."
  },
  {
    "objectID": "blog/posts/2023-04-12-Jupyter-Kernel/upyter-kernel-explained-chatgpt.html#hmm-okay-so-whats-does-a-kernel-includes---particular-the-python-kernel",
    "href": "blog/posts/2023-04-12-Jupyter-Kernel/upyter-kernel-explained-chatgpt.html#hmm-okay-so-whats-does-a-kernel-includes---particular-the-python-kernel",
    "title": "What is a Jupyter Kernel? By ChatGPT",
    "section": "Hmm… Okay, so what’s does a “Kernel” includes - particular the Python Kernel?",
    "text": "Hmm… Okay, so what’s does a “Kernel” includes - particular the Python Kernel?\n\nIn the case of Python, the kernel provides a few key features that a standard Python interpreter does not. These include: Code execution within an isolated process: When a user runs code in a Jupyter notebook, the kernel executes that code within an isolated process, separate from the notebook interface. This allows the kernel to manage the state of the Python session independently of the notebook, providing greater stability and reliability. Input and output history: The Jupyter kernel maintains a history of inputs and outputs from previous notebook sessions, allowing users to recall and reuse previous code snippets and results.\n\n\nRich media display: The kernel provides support for displaying rich media content within the notebook, including images, videos, and interactive visualizations. This allows users to create more dynamic and interactive notebooks that incorporate multimedia elements. Support for multiple languages: The Jupyter kernel supports many different programming languages, allowing users to write and execute code in the language of their choice within the same notebook interface.\n\n\nWhile it is possible to run a Python interpreter outside of Jupyter, using the Jupyter kernel provides many additional features and benefits that are not available in a standard Python interpreter. These features make Jupyter a powerful tool for data analysis, scientific computing, and interactive programming."
  },
  {
    "objectID": "blog/posts/2023-04-12-Jupyter-Kernel/upyter-kernel-explained-chatgpt.html#okay-so-what-happens-when-i-make-a-plot-in-jupyter",
    "href": "blog/posts/2023-04-12-Jupyter-Kernel/upyter-kernel-explained-chatgpt.html#okay-so-what-happens-when-i-make-a-plot-in-jupyter",
    "title": "What is a Jupyter Kernel? By ChatGPT",
    "section": "Okay, so what happens when I make a plot in Jupyter?",
    "text": "Okay, so what happens when I make a plot in Jupyter?\n\nWhen a user runs a cell in a Jupyter notebook that contains a call to one of these display functions, the kernel executes the Python code and generates the requested media object. The kernel then sends the media object to the notebook interface using the display protocol, which displays the object in the notebook. The communication between Jupyter and Python is supported by a messaging protocol called the “Jupyter protocol”. The Jupyter protocol is a messaging protocol that allows for bidirectional communication between the Jupyter notebook interface and the kernel that is running the Python process. When a user interacts with a Jupyter notebook, the notebook interface sends messages to the kernel, requesting that code be executed and displaying the results of that code. The kernel receives these messages, executes the requested code, and sends the results back to the notebook interface. The notebook interface then displays the results in the appropriate format, such as text output, plots, or interactive visualizations.\n\n\nIn the Jupyter ecosystem, ZeroMQ is used to establish a “wire protocol” between the notebook interface and the kernel. The wire protocol specifies the format of the messages that are sent between the notebook and the kernel, and how those messages are encoded and decoded.\n\n\nThe wire protocol is implemented using ZeroMQ sockets, which provide a reliable, low-latency communication channel between the notebook interface and the kernel. The sockets are used to send and receive messages in real-time, allowing the user to interact with the notebook interface and see the results of code execution immediately."
  },
  {
    "objectID": "blog/posts/2020-02-09-MixUp-and-Beta-Distribution.html",
    "href": "blog/posts/2020-02-09-MixUp-and-Beta-Distribution.html",
    "title": "data augmentation - Understand MixUp and Beta Distribution",
    "section": "",
    "text": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/mixup-beta"
  },
  {
    "objectID": "blog/posts/2020-02-09-MixUp-and-Beta-Distribution.html#beta-distribution",
    "href": "blog/posts/2020-02-09-MixUp-and-Beta-Distribution.html#beta-distribution",
    "title": "data augmentation - Understand MixUp and Beta Distribution",
    "section": "Beta Distribution",
    "text": "Beta Distribution\nBeta distribution is control by two parameters, α and β with interval [0, 1], which make it useful for Mixup. Mixup is basically a superposition of two image with a parameter t. Instead of using a dog image, with Mixup, you may end up have a image which is 0.7 dog + 0.3 cat\nTo get some sense of what a beta distribution is, let plot beta distribution with different alpha and beta to see its effect\nimport math\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch import tensor\n# PyTorch has a log-gamma but not a gamma, so we'll create one\nΓ = lambda x: x.lgamma().exp()\nfacts = [math.factorial(i) for i in range(7)]\n\nplt.plot(range(7), facts, 'ro')\nplt.plot(torch.linspace(0,6), Γ(torch.linspace(0,6)+1))\nplt.legend(['factorial','Γ']);\n\n\n\npng"
  },
  {
    "objectID": "blog/posts/2022-11-10-python-expert/python-expert-decorator-generator-contextmanager.html",
    "href": "blog/posts/2022-11-10-python-expert/python-expert-decorator-generator-contextmanager.html",
    "title": "Being Python Expert",
    "section": "",
    "text": "This presentation introduce three concepts, decorator, generator and contextmanager. The presenter explains that Python is a Protocol oriented langauge brilliantly and put these three concepts together to illustrate a story. Ultimately, being a Python expert doesn’t mean that you write advance syntax, but using these pattern wisely with simple code. These three concepts both serves its own puprose and are orthogonal to each other, yet working nicely when they are composed together."
  },
  {
    "objectID": "blog/posts/2022-11-10-python-expert/python-expert-decorator-generator-contextmanager.html#meet-the-python-data-model",
    "href": "blog/posts/2022-11-10-python-expert/python-expert-decorator-generator-contextmanager.html#meet-the-python-data-model",
    "title": "Being Python Expert",
    "section": "Meet the Python Data Model",
    "text": "Meet the Python Data Model\nWith python, there are almost always a lower level __ method corresponds to a higher-level function/syntax. It’s useful to change these behavior and understand how the Python Data Model work. You can find all the dunder __ method here. https://docs.python.org/3/reference/datamodel.html\n x + y   --> __add__\n repr(s) --> __repr__\n x()     --> __call__\n\n## Metaclasses\n`metaclass` exists to allow library code works nicely with user code. For example, how can a library author ensure that its user will follow its protocol and not using it wrongly? Again you can dive into `__new__` for ensuring that. In practice, the code is already written for `abc` and people use `@abstractmethod` `abc.ABCMeta`\n\n\n# Meet the `__builtins__` library\nimport builtins\n__build_class__\n\nclass Nok:\n    pass\n\nNok() # This is possible because class is a Python keyword\n\n# You can actually construct a class with a function.\nbuiltins.__build_class__(\"Nok\")\n\nTypeError: __build_class__: not enough arguments"
  },
  {
    "objectID": "blog/posts/2022-11-10-python-expert/python-expert-decorator-generator-contextmanager.html#generator",
    "href": "blog/posts/2022-11-10-python-expert/python-expert-decorator-generator-contextmanager.html#generator",
    "title": "Being Python Expert",
    "section": "Generator",
    "text": "Generator\n\nEager vs Lazy\nProcess when data comes - memory efficient and no wait.\nyield control - interleaving. Idea of executing some code, then passing the output back to user, do something and continue.\n\n\ndef temptable(cur):\n    print(\"Create Table\")\n    yield\n    print(\"Drop Table\")\n\n\nclass T:\n    def __enter__(self):\n        self.gen = temptable(\"123\")\n        next(self.gen)\n        return self\n\n    def __exit__(self, *args):\n        return next(self.gen, None)\n\nwith T():\n    print(\"Finish\")\n\n\nCreate Table\nFinish\nDrop Table"
  },
  {
    "objectID": "blog/posts/2021-03-05-pyodbc-linux.html",
    "href": "blog/posts/2021-03-05-pyodbc-linux.html",
    "title": "Setting up pyodbc for Impala connection, works on both Linux and Window",
    "section": "",
    "text": "Introduction\nLong story short, connect with Impala is a big headache in Windows. pyhive, impyla are both buggy. At the end, I stick with pyodbc as it works on both Linux and Windows, and seems to have better performance. There are not many steps, but it would be tricky if you try to Google as there are not much guide that just work out of the box\n\n\nSetup\nFirst, you need to download the ODBC driver from Cloudera.\nThen you need to instsall the driver properly.\ndpkg -i docker/clouderaimpalaodbc_2.6.10.1010-2_amd64.deb\nAdd this file to the directory /etc/odbcinst.ini, if you already have add, append this to the file.\n# /etc/odbcinst.ini\n[ODBC Drivers]\nCloudera Impala ODBC Driver 32-bit=Installed\nCloudera Impala ODBC Driver 64-bit=Installed\n[Cloudera Impala ODBC Driver 32-bit]\nDescription=Cloudera Impala ODBC Driver (32-bit)\nDriver=/opt/cloudera/impalaodbc/lib/32/libclouderaimpalaodbc32.so\n[Cloudera Impala ODBC Driver 64-bit]\nDescription=Cloudera Impala ODBC Driver (64-bit)\nDriver=/opt/cloudera/impalaodbc/lib/64/libclouderaimpalaodbc64.so\nThen install some additional package.\napt-get update && apt-get -y install gnupg apt-transport-https\napt-get update && apt-get -y install libssl1.0.0 unixodbc unixodbc-dev \\\n&& ACCEPT_EULA=Y apt-get -y install msodbcsql17\napt-get install unixodbc-dev -y\nLast, pip install pyodbc and have fun.\nTo read a database table, you can simply do this.\nimport pyodbc\nimport pandas as pd\n\nconn = pyodbc.connect(f\"\"\"\nDriver=Cloudera ODBC Driver for Impala 64-bit;\nPWD=password;\nUID=username;\nDatabase=database\n\"\"\")\nThere are multiple way to connect, but I found using a connection string is the most straight forward solution that does not require any additional enviornment variable setup."
  },
  {
    "objectID": "blog/posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html",
    "href": "blog/posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html",
    "title": "deepcopy, LGBM and pickle",
    "section": "",
    "text": "To start with, let’s look at some code to get some context."
  },
  {
    "objectID": "blog/posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#deepcopy-or-no-copy",
    "href": "blog/posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#deepcopy-or-no-copy",
    "title": "deepcopy, LGBM and pickle",
    "section": "deepcopy or no copy?",
    "text": "deepcopy or no copy?\n\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom copy import deepcopy\n\nparams = {\n'objective': 'regression',\n'verbose': -1,\n'num_leaves': 3\n}\n\nX = np.random.rand(100,2)\nY = np.ravel(np.random.rand(100,1))\nlgbm = lgb.train(params, lgb.Dataset(X,label=Y),num_boost_round=1)\nprint(\"Parameters of the model: \", lgbm.params)\n\nParameters of the model:  {'objective': 'regression', 'verbose': -1, 'num_leaves': 3, 'num_iterations': 1, 'early_stopping_round': None}\n\n\n\n## Deep copy will missing params\nnew_model = deepcopy(lgbm)\n\nFinished loading model, total used 1 iterations\n\n\nYou would expect new_model.parameters return the same dict right? Not quite.\n\nprint(\"Parameters of the copied model: \", new_model.params)\n\nParameters of the copied model:  {}\n\n\nSurprise, surprise. It’s an empty dict, where did the parameters go? To dive deep into the issue, let’s have a look at the source code of deepcopy to understand how does it work.\nreference: https://github.com/python/cpython/blob/e8e341993e3f80a3c456fb8e0219530c93c13151/Lib/copy.py#L128\ndef deepcopy(x, memo=None, _nil=[]):\n    \"\"\"Deep copy operation on arbitrary Python objects.\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    ... # skip some irrelevant code  \n\n    cls = type(x)\n\n    copier = _deepcopy_dispatch.get(cls)\n    if copier is not None:\n        y = copier(x, memo)\n    else:\n        if issubclass(cls, type):\n            y = _deepcopy_atomic(x, memo)\n        else:\n            copier = getattr(x, \"__deepcopy__\", None)\n            if copier is not None:\n                y = copier(memo)\n            else:\n                ... # skip irrelevant code\n\n    # If is its own copy, don't memoize.\n    if y is not x:\n        memo[d] = y\n        _keep_alive(x, memo) # Make sure x lives at least as long as d\n    return y\nIn particular, line 17 is what we care.\ncopier = getattr(x, \"__deepcopy__\", None)\nIf a particular class has implement the __deepcopy__ method, deepcopy will try to invoke that instead of the standard copy. The following dummy class should illustrate this clearly.\n\nclass DummyClass():\n    def __deepcopy__(self, _):\n        print('Just hanging around and not copying.')\n\n\no = DummyClass()\ndeepcopy(o)\n\nJust hanging around and not copying.\n\n\na lightgbm model is actually a Booster object and implement its own __deepcopy__. It only copy the model string but nothing else, this explains why deepcopy(lgbm).paramters is an empty dictionary.\n def __deepcopy__(self, _): \n     model_str = self.model_to_string(num_iteration=-1) \n     booster = Booster(model_str=model_str) \n     return booster \nReference: https://github.com/microsoft/LightGBM/blob/d6ebd063fff7ff9ed557c3f2bcacc8f9456583e6/python-package/lightgbm/basic.py#L2279-L2282\nOkay, so why lightgbm need to have an custom implementation? I thought this is a bug, but turns out there are some deeper reason behind this. I created an issue on GitHub.\nhttps://github.com/microsoft/LightGBM/issues/4085 Their response is > Custom deepcopy is needed to make Booster class picklable."
  },
  {
    "objectID": "blog/posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#italian-bmt-lettuce-tomato-and-some-pickles-please",
    "href": "blog/posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#italian-bmt-lettuce-tomato-and-some-pickles-please",
    "title": "deepcopy, LGBM and pickle",
    "section": "🥖Italian BMT, 🥬Lettuce 🍅 tomato and some 🥒pickles please",
    "text": "🥖Italian BMT, 🥬Lettuce 🍅 tomato and some 🥒pickles please\nWhat does pickle really is? and what makes an object pickable?\n\nPython Pickle is used to serialize and deserialize a python object structure. Any object on python can be pickled so that it can be saved on disk.\n\nSerialization roughly means translating the data in memory into a format that can be stored on disk or sent over network. It’s like ordering a chair from Ikea, they will send you a box, but not a chair.\nThe process of decomposing the chair and put it into a box is serialization, while putting it together is deserialization. With pickle terms, we called it Pickling and Unpickling.\n\n\n\n\n\ndeserialize and serialize\n\n\n\nWhat is Pickle\nPickle is a protocol for Python, you and either pickling a Python object to memory or to file.\n\nimport pickle\n\n\nd = {'a': 1}\npickle_d = pickle.dumps(d)\npickle_d\n\nb'\\x80\\x04\\x95\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94\\x8c\\x01a\\x94K\\x01s.'\n\n\nThe python dict is now transfrom into a series of binary str, this string can be only understand by Python. We can also deserialize a binary string back to a python dict.\n\nbinary_str = b'\\x80\\x04\\x95\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94\\x8c\\x01a\\x94K\\x01s.'\npickle.loads(binary_str)\n\n{'a': 1}\n\n\nReference: https://www.python.org/dev/peps/pep-0574/#:~:text=The%20pickle%20protocol%20was%20originally%20designed%20in%201995,copying%20temporary%20data%20before%20writing%20it%20to%20disk.\n\n\nWhat makes something picklable\nFinally, we come back to our initial questions. > What makes something picklable? Why lightgbm need to have deepcopy to make the Booster class picklable?\n\nWhat can be pickled and unpickled? The following types can be pickled:\n* None, True, and False\n* integers, floating point numbers, complex numbers\n* strings, bytes, bytearrays\n* tuples, lists, sets, and dictionaries containing only picklable objects\n* functions defined at the top level of a module (using def, not lambda)\n* built-in functions defined at the top level of a module\n* classes that are defined at the top level of a module\n\nSo pretty much common datatype, functions and classes are picklable. Let’s see without __deepcopy__, the Booster class is not serializable as it claims.\n\nimport lightgbm\nfrom lightgbm import Booster\ndel Booster.__deepcopy__\n\nparams = {\n'objective': 'regression',\n'verbose': -1,\n'num_leaves': 3\n}\n\nX = np.random.rand(100,2)\nY = np.ravel(np.random.rand(100,1))\nlgbm = lgb.train(params, lgb.Dataset(X,label=Y),num_boost_round=1)\n\n\ndeepcopy_lgbm = deepcopy(lgbm)\nlgbm.params, deepcopy_lgbm.params\n\n({'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None},\n {'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None})\n\n\n\npickle.dumps(deepcopy_lgbm) == pickle.dumps(lgbm)\n\nTrue\n\n\n\nunpickle_model = pickle.loads(pickle.dumps(deepcopy_lgbm))\nunpickle_deepcopy_model = pickle.loads(pickle.dumps(lgbm))\n\n\nunpickle_model.params, unpickle_deepcopy_model.params\n\n({'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None},\n {'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None})\n\n\n\nunpickle_model.model_to_string() == unpickle_deepcopy_model.model_to_string()\n\nTrue\n\n\n\nunpickle_deepcopy_model.predict(X)\n\narray([0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803,\n       0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.48439803,\n       0.49029787, 0.49029787, 0.48439803, 0.48439803, 0.48439803,\n       0.49029787, 0.48439803, 0.50141491, 0.50141491, 0.50141491,\n       0.48439803, 0.50141491, 0.48439803, 0.49029787, 0.50141491,\n       0.50141491, 0.48439803, 0.49029787, 0.49029787, 0.49029787,\n       0.49029787, 0.50141491, 0.48439803, 0.50141491, 0.48439803,\n       0.49029787, 0.50141491, 0.48439803, 0.48439803, 0.48439803,\n       0.48439803, 0.50141491, 0.50141491, 0.48439803, 0.49029787,\n       0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803,\n       0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.50141491,\n       0.49029787, 0.48439803, 0.50141491, 0.49029787, 0.49029787,\n       0.50141491, 0.50141491, 0.48439803, 0.50141491, 0.48439803,\n       0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.48439803,\n       0.48439803, 0.50141491, 0.50141491, 0.49029787, 0.50141491,\n       0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.50141491,\n       0.50141491, 0.48439803, 0.49029787, 0.48439803, 0.48439803,\n       0.50141491, 0.49029787, 0.50141491, 0.50141491, 0.49029787,\n       0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.48439803,\n       0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.49029787])\n\n\n\n\nLast Word\nWell…. It seems actually picklable? I may need to investigate the issue a bit more. For now, the __deepcopy__ does not seems to be necessary.\nI tried to dig into lightgbm source code and find this potential related issue. https://github.com/microsoft/LightGBM/blame/dc1bc23adf1137ef78722176e2da69f8411b1feb/python-package/lightgbm/basic.py#L2298"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "",
    "text": "Missing Subtitle (plotnine)\nMissing Style"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-example",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-example",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "Plotnine Example",
    "text": "Plotnine Example\n\n(ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)'))\n + geom_point()\n + stat_smooth(method='lm')\n + facet_wrap('~gear'))\n\n\n\n\n<ggplot: (-9223371941312347920)>"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot\nline_df <- gapminder %>%\n  filter(country == \"Malawi\") \n\n#Make plot\nline <- ggplot(line_df, aes(x = year, y = lifeExp)) +\n  geom_line(colour = \"#1380A1\", size = 1) +\n  geom_hline(yintercept = 0, size = 1, colour=\"#333333\") +\n  bbc_style() +\n  labs(title=\"Living longer\",\n       subtitle = \"Life expectancy in Malawi 1952-2007\")"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\n\n(ggplot(line_df, aes(x='year', y='lifeExp')) +\n geom_line(colour='#1380A1', size=1) +\n geom_hline(yintercept = 0, size = 1, colour='#333333') +\n labs(title='Living longer', \n     subtitle = 'Life expectancy in Malawi 1952-2007')\n )\n\n\n\n\n<ggplot: (-9223371941310406772)>\n\n\n\n## altair\nline = (alt.Chart(line_df).mark_line().encode(\nx='year',\ny='lifeExp')\n.properties(title={'text': 'Living Longer',\n                   'subtitle': 'Life expectancy in Malawi 1952-2007'})\n) \n\n# hline\noverlay = overlay = pd.DataFrame({'y': [0]})\nhline = alt.Chart(overlay).mark_rule(color='#333333', strokeWidth=3).encode(y='y:Q')\n\nline + hline"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair\n\n## altair\nline = (alt.Chart(line_df).mark_line().encode(\nx='year',\ny='lifeExp')\n.properties(title={'text': 'Living Longer',\n                   'subtitle': 'Life expectancy in China 1952-2007'})\n) \n\n# hline\noverlay = overlay = pd.DataFrame({'lifeExp': [0]})\nhline = alt.Chart(overlay).mark_rule(color='#333333', strokeWidth=3).encode(y='lifeExp:Q')\n\nline + hline"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-1",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-1",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot\n#Prepare data\nmultiple_line_df <- gapminder %>%\n  filter(country == \"China\" | country == \"United States\") \n\n#Make plot\nmultiple_line <- ggplot(multiple_line_df, aes(x = year, y = lifeExp, colour = country)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0, size = 1, colour=\"#333333\") +\n  scale_colour_manual(values = c(\"#FAAB18\", \"#1380A1\")) +\n  bbc_style() +\n  labs(title=\"Living longer\",\n       subtitle = \"Life expectancy in China and the US\")"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-1",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-1",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\n\n# Make plot\nmultiline = (\n    ggplot(multiline_df, aes(x='year', y='lifeExp', colour='country')) +\n    geom_line(colour=\"#1380A1\", size=1) +\n    geom_hline(yintercept=0, size=1, color=\"#333333\") +\n    scale_colour_manual(values=[\"#FAAB18\", \"#1380A1\"]) +\n      bbc_style() +\n    labs(title=\"Living longer\",\n         subtitle=\"Life expectancy in China 1952-2007\"))\nmultiline\n\nfindfont: Font family ['Helvetica'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Helvetica'] not found. Falling back to DejaVu Sans.\n\n\n\n\n\n<ggplot: (-9223371941310014864)>"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-1",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-1",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair\n\nmultiline_altair = (alt.Chart(multiline_df).mark_line().encode(\nx='year',\ny='lifeExp',\ncolor='country')\n.properties(title={'text': 'Living Longer',\n                   'subtitle': 'Life expectancy in China 1952-2007'})\n) \n\n# hline\noverlay = overlay = pd.DataFrame({'lifeExp': [0]})\nhline = alt.Chart(overlay).mark_rule(color='#333333', strokeWidth=3).encode(y='lifeExp:Q')\n\nmultiline_altair + hline"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-2",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-2",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\n\nbars_ggplot = (ggplot(bar_df, aes(x='country', y='lifeExp')) +\n  geom_bar(stat=\"identity\",\n           position=\"identity\",\n           fill=\"#1380A1\") +\n  geom_hline(yintercept=0, size=1, colour=\"#333333\") +\n#   bbc_style() +\n  labs(title=\"Reunion is highest\",\n       subtitle=\"Highest African life expectancy, 2007\"))\n\nbars_ggplot\n\n\n\n\n<ggplot: (-9223371941310355340)>"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-2",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-2",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair\n\nbars_altair = (alt.Chart(bar_df).mark_bar().encode(\nx='country',\ny='lifeExp',\n# color='country'\n)\n.properties(title={'text': 'Reunion is highest',\n                   'subtitle': 'Highest African life expectancy, 2007'})\n) \n\nbars_altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#data-preprocessing",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#data-preprocessing",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "Data preprocessing",
    "text": "Data preprocessing\n\n## collapse-hide\nstacked_bar_df = (\n    gapminder.query(' year == 2007')\n    .assign(\n        lifeExpGrouped=lambda x: pd.cut(\n            x['lifeExp'],\n            bins=[0, 50, 65, 80, 90],\n            labels=[\"under 50\", \"50-65\", \"65-80\", \"80+\"]))\n    .groupby(\n        ['continent', 'lifeExpGrouped'], as_index=True)\n    .agg({'pop': 'sum'})\n    .rename(columns={'pop': 'continentPop'})\n    .reset_index()\n)\nstacked_bar_df['lifeExpGrouped'] = pd.Categorical(stacked_bar_df['lifeExpGrouped'], ordered=True)\n\nstacked_bar_df.head(6)\n\n\n\n\n\n  \n    \n      \n      continent\n      lifeExpGrouped\n      continentPop\n    \n  \n  \n    \n      0\n      Africa\n      under 50\n      376100713.0\n    \n    \n      1\n      Africa\n      50-65\n      386811458.0\n    \n    \n      2\n      Africa\n      65-80\n      166627521.0\n    \n    \n      3\n      Africa\n      80+\n      NaN\n    \n    \n      4\n      Americas\n      under 50\n      NaN\n    \n    \n      5\n      Americas\n      50-65\n      8502814.0"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-3",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-3",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot\n#prepare data\nstacked_df <- gapminder %>% \n  filter(year == 2007) %>%\n  mutate(lifeExpGrouped = cut(lifeExp, \n                    breaks = c(0, 50, 65, 80, 90),\n                    labels = c(\"Under 50\", \"50-65\", \"65-80\", \"80+\"))) %>%\n  group_by(continent, lifeExpGrouped) %>%\n  summarise(continentPop = sum(as.numeric(pop)))\n\n#set order of stacks by changing factor levels\nstacked_df$lifeExpGrouped = factor(stacked_df$lifeExpGrouped, levels = rev(levels(stacked_df$lifeExpGrouped)))\n\n#create plot\nstacked_bars <- ggplot(data = stacked_df, \n                       aes(x = continent,\n                           y = continentPop,\n                           fill = lifeExpGrouped)) +\n  geom_bar(stat = \"identity\", \n           position = \"fill\") +\n  bbc_style() +\n  scale_y_continuous(labels = scales::percent) +\n  scale_fill_viridis_d(direction = -1) +\n  geom_hline(yintercept = 0, size = 1, colour = \"#333333\") +\n  labs(title = \"How life expectancy varies\",\n       subtitle = \"% of population by life expectancy band, 2007\") +\n  theme(legend.position = \"top\", \n        legend.justification = \"left\") +\n  guides(fill = guide_legend(reverse = TRUE))"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-3",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-3",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\n\n# create plot\nstacked_bar_ggplot = (\n    ggplot(stacked_bar_df,\n           aes(x='continent',\n               y='continentPop',\n               fill='lifeExpGrouped')\n           ) +\n    geom_bar(stat=\"identity\",\n             position=\"fill\") +\n    #   bbc_style() +\n    scale_y_continuous(labels=lambda l: [\"%d%%\" % (v * 100) for v in l]) +\n    scale_fill_cmap_d(direction=-1) +  # scale_fill_viridis_d\n    geom_hline(yintercept=0, size=1, colour=\"#333333\") +\n    labs(title=\"How life expectancy varies\",\n         subtitle=\"% of population by life expectancy band, 2007\") +\n\n    guides(fill=guide_legend(reverse=True)))\n\nstacked_bar_ggplot\n\nC:\\Users\\CHANNO.OOCLDM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:91: PlotnineWarning: scale_fill_cmap_d could not recognise parameter `direction`\n  warn(msg.format(self.__class__.__name__, k), PlotnineWarning)\nC:\\Users\\CHANNO.OOCLDM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\plotnine\\layer.py:433: PlotnineWarning: position_stack : Removed 7 rows containing missing values.\n  data = self.position.setup_data(self.data, params)\n\n\n\n\n\n<ggplot: (-9223371941310320660)>\n\n\n\n# create plot\nstacked_bar_ggplot = (\n    ggplot(stacked_bar_df,\n           aes(x='continent',\n               y='continentPop',\n               fill='lifeExpGrouped')\n           ) +\n    geom_bar(stat=\"identity\",\n             position=\"fill\") +\n    #   bbc_style() +\n    scale_y_continuous(labels=lambda l: [\"%d%%\" % (v * 100) for v in l]) +\n    scale_fill_cmap_d(direction=-1) +  # scale_fill_viridis_d\n    geom_hline(yintercept=0, size=1, colour=\"#333333\") +\n    labs(title=\"How life expectancy varies\",\n         subtitle=\"% of population by life expectancy band, 2007\") +\n\n    guides(fill=guide_legend(reverse=True)))\n\nstacked_bar_ggplot\n\nC:\\Users\\CHANNO.OOCLDM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:91: PlotnineWarning: scale_fill_cmap_d could not recognise parameter `direction`\n  warn(msg.format(self.__class__.__name__, k), PlotnineWarning)\nC:\\Users\\CHANNO.OOCLDM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\plotnine\\layer.py:433: PlotnineWarning: position_stack : Removed 7 rows containing missing values.\n  data = self.position.setup_data(self.data, params)\n\n\n\n\n\n<ggplot: (-9223371941310406808)>"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-3",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-3",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair\n\nstacked_bar_altair = (\n    alt.Chart(stacked_bar_df)\n    .mark_bar()\n    .encode(x='continent',\n            y=alt.Y('continentPop', stack='normalize',\n                    axis=alt.Axis(format='%')),\n            fill=alt.Fill('lifeExpGrouped', scale=alt.Scale(scheme='viridis')))\n    .properties(title={'text': 'How life expectancy varies',\n                       'subtitle': '% of population by life expectancy band, 2007'}\n                )\n)\n\noverlay = overlay = pd.DataFrame({'continentPop': [0]})\nhline = alt.Chart(overlay).mark_rule(\n    color='#333333', strokeWidth=2).encode(y='continentPop:Q')\n\n\nstacked_bar_altair + hline"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-4",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-4",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot\n#Prepare data\ngrouped_bar_df <- gapminder %>%\n  filter(year == 1967 | year == 2007) %>%\n  select(country, year, lifeExp) %>%\n  spread(year, lifeExp) %>%\n  mutate(gap = `2007` - `1967`) %>%\n  arrange(desc(gap)) %>%\n  head(5) %>%\n  gather(key = year, \n         value = lifeExp,\n         -country,\n         -gap) \n  \n#Make plot\ngrouped_bars <- ggplot(grouped_bar_df, \n                       aes(x = country, \n                           y = lifeExp, \n                           fill = as.factor(year))) +\n  geom_bar(stat=\"identity\", position=\"dodge\") +\n  geom_hline(yintercept = 0, size = 1, colour=\"#333333\") +\n  bbc_style() +\n  scale_fill_manual(values = c(\"#1380A1\", \"#FAAB18\")) +\n  labs(title=\"We're living longer\",\n       subtitle = \"Biggest life expectancy rise, 1967-2007\")"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-4",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-4",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\n\n# Make plot\ngrouped_bars_ggplot = (ggplot(grouped_bar_df,\n                       aes(x='country',\n                           y='lifeExp',\n                           fill='year')) +\n                geom_bar(stat=\"identity\", position=\"dodge\") +\n                geom_hline(yintercept=0, size=1, colour=\"#333333\") +\n#                 bbc_style() +\n                scale_fill_manual(values=(\"#1380A1\", \"#FAAB18\")) +\n                labs(title=\"We're living longer\",\n                     subtitle=\"Biggest life expectancy rise, 1967-2007\"))\n\ngrouped_bars_ggplot\n\n\n\n\n<ggplot: (-9223371941310352504)>"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-4",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-4",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair\n\ngrouped_bars_altair = (\n    alt.Chart(grouped_bar_df)\n    .mark_bar()\n    .encode(x='year:N',\n            y='lifeExp',\ncolor=alt.Color('year:N', scale=alt.Scale(range=[\"#1380A1\", \"#FAAB18\"])),\n           column='country:N')\n    .properties(title={'text': \"We're living longe\",\n                       'subtitle': 'Biggest life expectancy rise, 1967-2007'}\n                )\n)\n\ngrouped_bars_altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-5",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-5",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\n\n# Remove the Legend\nmultiline + guides(colour=False)\n\n\n\n\n<ggplot: (-9223371941310211164)>\n\n\n\nmultiline + theme(legend_position = \"none\")\n\n\n\n\n<ggplot: (-9223371941308840576)>\n\n\n\nfrom plotnine import unit\n\nImportError: cannot import name 'unit' from 'plotnine' (C:\\Users\\CHANNO.OOCLDM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\plotnine\\__init__.py)\n\n\n\n# Change position of the legend\nx=multiline + theme(\n  axis_ticks_major_x = element_line(color = \"#333333\"), \n  axis_ticks_length =  0.26)\n\nx\n\n\n\n\n<ggplot: (-9223371941310025320)>"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-5",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-5",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-5",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-5",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-6",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-6",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-6",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-6",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-6",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-6",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-7",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-7",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-7",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-7",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-7",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-7",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-8",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-8",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-8",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-8",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-8",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-8",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-9",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-9",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-9",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-9",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-9",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-9",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot\n#Prepare data\ndumbbell_df <- gapminder %>%\n  filter(year == 1967 | year == 2007) %>%\n  select(country, year, lifeExp) %>%\n  spread(year, lifeExp) %>%\n  mutate(gap = `2007` - `1967`) %>%\n  arrange(desc(gap)) %>%\n  head(10)\n\nggplot(hist_df, aes(lifeExp)) +\n  geom_histogram(binwidth = 5, colour = \"white\", fill = \"#1380A1\") +\n  geom_hline(yintercept = 0, size = 1, colour=\"#333333\") +\n  bbc_style() +\n  scale_x_continuous(limits = c(35, 95),\n                     breaks = seq(40, 90, by = 10),\n                     labels = c(\"40\", \"50\", \"60\", \"70\", \"80\", \"90 years\")) +\n  labs(title = \"How life expectancy varies\",\n       subtitle = \"Distribution of life expectancy in 2007\")"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-10",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-10",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\nNot available with plotnine."
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-10",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-10",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair\n\ndumbbell_chart_altair = (\nalt.Chart(dumbbell_chart_df).\n    mark_rule()\n)\n\ndumbbell_chart_altair\n\n()"
  },
  {
    "objectID": "blog/posts/2022-11-15-kedro_duckdb/kedro-duckdb.html",
    "href": "blog/posts/2022-11-15-kedro_duckdb/kedro-duckdb.html",
    "title": "Testing Kedro with DuckDB",
    "section": "",
    "text": "Exploring DuckDB and how can we use it with kedro"
  },
  {
    "objectID": "blog/posts/2022-11-15-kedro_duckdb/kedro-duckdb.html#practical-sql-for-data-analysis",
    "href": "blog/posts/2022-11-15-kedro_duckdb/kedro-duckdb.html#practical-sql-for-data-analysis",
    "title": "Testing Kedro with DuckDB",
    "section": "Practical SQL for Data Analysis",
    "text": "Practical SQL for Data Analysis\n\nWhat you can do together with Pandas\n\n!pip install --quiet duckdb\n\n\nimport pandas as pd\nimport numpy as np\n\nimport sqlite3\nimport duckdb\nimport time\n\nPreparation\nDownload the data and set up the Pandas data frames. We read the data into a Pandas DataFrame using DuckDB’s built-in Parquet reader.\n\n!wget -q https://github.com/cwida/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet\n\n  HTTP/1.1 301 Moved Permanently\n  Server: GitHub.com\n  Date: Tue, 15 Nov 2022 22:34:17 GMT\n  Content-Type: text/html; charset=utf-8\n  Vary: X-PJAX, X-PJAX-Container, Turbo-Visit, Turbo-Frame, Accept-Encoding, Accept, X-Requested-With\n  Location: https://github.com/duckdb/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet\n  Cache-Control: no-cache\n  Strict-Transport-Security: max-age=31536000; includeSubdomains; preload\n  X-Frame-Options: deny\n  X-Content-Type-Options: nosniff\n  X-XSS-Protection: 0\n  Referrer-Policy: origin-when-cross-origin, strict-origin-when-cross-origin\n  Content-Security-Policy: default-src 'none'; base-uri 'self'; block-all-mixed-content; child-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/; connect-src 'self' uploads.github.com objects-origin.githubusercontent.com www.githubstatus.com collector.github.com raw.githubusercontent.com api.github.com github-cloud.s3.amazonaws.com github-production-repository-file-5c1aeb.s3.amazonaws.com github-production-upload-manifest-file-7fdce7.s3.amazonaws.com github-production-user-asset-6210df.s3.amazonaws.com cdn.optimizely.com logx.optimizely.com/v1/events *.actions.githubusercontent.com wss://*.actions.githubusercontent.com online.visualstudio.com/api/v1/locations github-production-repository-image-32fea6.s3.amazonaws.com github-production-release-asset-2e65be.s3.amazonaws.com insights.github.com wss://alive.github.com; font-src github.githubassets.com; form-action 'self' github.com gist.github.com objects-origin.githubusercontent.com; frame-ancestors 'none'; frame-src viewscreen.githubusercontent.com notebooks.githubusercontent.com; img-src 'self' data: github.githubassets.com media.githubusercontent.com camo.githubusercontent.com identicons.github.com avatars.githubusercontent.com github-cloud.s3.amazonaws.com objects.githubusercontent.com objects-origin.githubusercontent.com secured-user-images.githubusercontent.com/ opengraph.githubassets.com github-production-user-asset-6210df.s3.amazonaws.com customer-stories-feed.github.com spotlights-feed.github.com *.githubusercontent.com; manifest-src 'self'; media-src github.com user-images.githubusercontent.com/ secured-user-images.githubusercontent.com/; script-src github.githubassets.com; style-src 'unsafe-inline' github.githubassets.com; worker-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/\n  Content-Length: 0\n  X-GitHub-Request-Id: D47F:F2DE:2F3E29B:303AC52:637413E9\n  HTTP/1.1 302 Found\n  Server: GitHub.com\n  Date: Tue, 15 Nov 2022 22:34:17 GMT\n  Content-Type: text/html; charset=utf-8\n  Vary: X-PJAX, X-PJAX-Container, Turbo-Visit, Turbo-Frame, Accept-Encoding, Accept, X-Requested-With\n  Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/263853960/33e88e80-95cb-11ea-8bb7-2dfa0654592c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221115%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221115T223417Z&X-Amz-Expires=300&X-Amz-Signature=07d1673053f9e8676510f46b62993e3b9b2428a17f00a613162f67690318e82f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=263853960&response-content-disposition=attachment%3B%20filename%3Dlineitemsf1.snappy.parquet&response-content-type=application%2Foctet-stream\n  Cache-Control: no-cache\n  Strict-Transport-Security: max-age=31536000; includeSubdomains; preload\n  X-Frame-Options: deny\n  X-Content-Type-Options: nosniff\n  X-XSS-Protection: 0\n  Referrer-Policy: no-referrer-when-downgrade\n  Content-Security-Policy: default-src 'none'; base-uri 'self'; block-all-mixed-content; child-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/; connect-src 'self' uploads.github.com objects-origin.githubusercontent.com www.githubstatus.com collector.github.com raw.githubusercontent.com api.github.com github-cloud.s3.amazonaws.com github-production-repository-file-5c1aeb.s3.amazonaws.com github-production-upload-manifest-file-7fdce7.s3.amazonaws.com github-production-user-asset-6210df.s3.amazonaws.com cdn.optimizely.com logx.optimizely.com/v1/events *.actions.githubusercontent.com wss://*.actions.githubusercontent.com online.visualstudio.com/api/v1/locations github-production-repository-image-32fea6.s3.amazonaws.com github-production-release-asset-2e65be.s3.amazonaws.com insights.github.com wss://alive.github.com; font-src github.githubassets.com; form-action 'self' github.com gist.github.com objects-origin.githubusercontent.com; frame-ancestors 'none'; frame-src viewscreen.githubusercontent.com notebooks.githubusercontent.com; img-src 'self' data: github.githubassets.com media.githubusercontent.com camo.githubusercontent.com identicons.github.com avatars.githubusercontent.com github-cloud.s3.amazonaws.com objects.githubusercontent.com objects-origin.githubusercontent.com secured-user-images.githubusercontent.com/ opengraph.githubassets.com github-production-user-asset-6210df.s3.amazonaws.com customer-stories-feed.github.com spotlights-feed.github.com *.githubusercontent.com; manifest-src 'self'; media-src github.com user-images.githubusercontent.com/ secured-user-images.githubusercontent.com/; script-src github.githubassets.com; style-src 'unsafe-inline' github.githubassets.com; worker-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/\n  Content-Length: 0\n  X-GitHub-Request-Id: D47F:F2DE:2F3E323:303ACEF:637413E9\n  HTTP/1.1 200 OK\n  Connection: keep-alive\n  Content-Length: 206368635\n  Content-Type: application/octet-stream\n  Last-Modified: Tue, 07 Dec 2021 13:35:44 GMT\n  ETag: \"0x8D9B986787C89B4\"\n  Server: Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0\n  x-ms-request-id: b588900b-a01e-0060-6d42-f95efa000000\n  x-ms-version: 2020-04-08\n  x-ms-creation-time: Tue, 17 Aug 2021 11:28:44 GMT\n  x-ms-lease-status: unlocked\n  x-ms-lease-state: available\n  x-ms-blob-type: BlockBlob\n  Content-Disposition: attachment; filename=lineitemsf1.snappy.parquet\n  x-ms-server-encrypted: true\n  Fastly-Restarts: 1\n  Accept-Ranges: bytes\n  Age: 0\n  Date: Tue, 15 Nov 2022 22:34:18 GMT\n  Via: 1.1 varnish\n  X-Served-By: cache-lhr7337-LHR\n  X-Cache: MISS\n  X-Cache-Hits: 0\n  X-Timer: S1668551658.108887,VS0,VE259\n\n\n\n!wget -q https://github.com/cwida/duckdb-data/releases/download/v1.0/orders.parquet\n\n\n%%time\nlineitem = duckdb.query(\"SELECT * FROM 'lineitemsf1.snappy.parquet'\").to_df()\norders = duckdb.query(\"SELECT * FROM 'orders.parquet'\").to_df()\n\nCPU times: user 7.62 s, sys: 5.43 s, total: 13 s\nWall time: 14.1 s\n\n\n\n%%time\n_ = pd.read_parquet(\"lineitemsf1.snappy.parquet\")\n_ = pd.read_parquet(\"orders.parquet\")\n\nCPU times: user 6.29 s, sys: 1.5 s, total: 7.78 s\nWall time: 5.81 s\n\n\n\ncon = duckdb.connect()\ncon.execute('PRAGMA threads=2')\n\ndef timeit(fun, name):\n    import time\n    start_time = time.monotonic()\n    fun()\n    return [name, time.monotonic() - start_time]\n\ndef plot_results(results, title):\n  df = pd.DataFrame.from_dict({\n      'name': [x[0] for x in results],\n      'time': [x[1] for x in results]\n  })\n  print(title)\n  print(df)\n\nUngrouped Aggregates\nThis performs a simple set of ungrouped aggregates (sum, min, max, avg) over a column without any filters or other complex operations.\n\nungrouped_aggregate = '''\n    SELECT SUM(l_extendedprice), MIN(l_extendedprice), MAX(l_extendedprice), AVG(l_extendedprice) FROM lineitem\n'''\n\ndef duckdb_ungrouped_aggregate(d_con):\n    print(d_con.query(ungrouped_aggregate).to_df())\n\ndef duckdb_ungrouped_aggregate_1t():\n    duckdb_ungrouped_aggregate(duckdb)\n\ndef duckdb_ungrouped_aggregate_2t():\n    duckdb_ungrouped_aggregate(con)\n\ndef pandas_ungrouped_aggregate():\n  result = lineitem.groupby(\n    ['l_returnflag', 'l_linestatus']\n  ).agg(\n    Sum=('l_extendedprice', 'sum'),\n    Min=('l_extendedprice', 'min'),\n    Max=('l_extendedprice', 'max'),\n    Avg=('l_extendedprice', 'mean')\n  )\n  print(result)\n    # print(lineitem.agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean')))\n\nua_results = []\nua_results.append(timeit(duckdb_ungrouped_aggregate_1t, 'DuckDB (1T)'))\nua_results.append(timeit(duckdb_ungrouped_aggregate_2t, 'DuckDB (2T)'))\nua_results.append(timeit(pandas_ungrouped_aggregate, 'Pandas'))\nplot_results(ua_results, 'Ungrouped Aggregate')\n\n   sum(l_extendedprice)  min(l_extendedprice)  max(l_extendedprice)  \\\n0          2.295773e+11                 901.0              104949.5   \n\n   avg(l_extendedprice)  \n0          38255.138485  \n   sum(l_extendedprice)  min(l_extendedprice)  max(l_extendedprice)  \\\n0          2.295773e+11                 901.0              104949.5   \n\n   avg(l_extendedprice)  \n0          38255.138485  \n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nA            F             5.658655e+10  904.0  104949.5  38273.129735\nN            F             1.487505e+09  920.0  104049.5  38284.467761\n             O             1.149352e+11  901.0  104749.5  38248.015609\nR            F             5.656804e+10  904.0  104899.5  38250.854626\nUngrouped Aggregate\n          name      time\n0  DuckDB (1T)  0.052544\n1  DuckDB (2T)  0.066239\n2       Pandas  0.801278\n\n\nGrouped Aggregates\nThis performs the same set of aggregates, but this time grouped by two other columns (l_returnflag and l_linestatus).\n\ngrouped_aggregate = '''\nSELECT l_returnflag,\n       l_linestatus,\n       SUM(l_extendedprice),\n       MIN(l_extendedprice),\n       MAX(l_extendedprice),\n       AVG(l_extendedprice)\nFROM lineitem\nGROUP BY l_returnflag,\n         l_linestatus\n'''\n\ndef duckdb_grouped_aggregate(d_con):\n    print(d_con.query(grouped_aggregate).to_df())\n\ndef duckdb_grouped_aggregate_1t():\n    duckdb_grouped_aggregate(duckdb)\n\ndef duckdb_grouped_aggregate_2t():\n    duckdb_grouped_aggregate(con)\n\ndef pandas_grouped_aggregate():\n    print(lineitem.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean')))\n\nresults = []\nresults.append(timeit(duckdb_grouped_aggregate_1t, 'DuckDB (1T)'))\nresults.append(timeit(duckdb_grouped_aggregate_2t, 'DuckDB (2T)'))\nresults.append(timeit(pandas_grouped_aggregate, 'Pandas'))\nplot_results(results, 'Grouped Aggregate')\n\n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.149352e+11                 901.0   \n1            R            F          5.656804e+10                 904.0   \n2            A            F          5.658655e+10                 904.0   \n3            N            F          1.487505e+09                 920.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38248.015609  \n1              104899.5          38250.854626  \n2              104949.5          38273.129735  \n3              104049.5          38284.467761  \n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.149352e+11                 901.0   \n1            R            F          5.656804e+10                 904.0   \n2            A            F          5.658655e+10                 904.0   \n3            N            F          1.487505e+09                 920.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38248.015609  \n1              104899.5          38250.854626  \n2              104949.5          38273.129735  \n3              104049.5          38284.467761  \n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nA            F             5.658655e+10  904.0  104949.5  38273.129735\nN            F             1.487505e+09  920.0  104049.5  38284.467761\n             O             1.149352e+11  901.0  104749.5  38248.015609\nR            F             5.656804e+10  904.0  104899.5  38250.854626\nGrouped Aggregate\n          name      time\n0  DuckDB (1T)  0.115463\n1  DuckDB (2T)  0.222520\n2       Pandas  0.708696\n\n\nGrouped Aggregate with a Filter\nThis benchmark performs a grouped aggregate with a filter over the shipdate column.\nAs Pandas does not perform any projection pushdown, we include a version where we manually perform the projection pushdown by filtering only the columns we actually need before running the filter and aggregate.\nThis optimization is performed automatically in DuckDB by the query optimizer.\n\ndef duckdb_grouped_aggregate_filter(d_con):\n    print(d_con.query('''\nSELECT l_returnflag,\n       l_linestatus,\n       SUM(l_extendedprice),\n       MIN(l_extendedprice),\n       MAX(l_extendedprice),\n       AVG(l_extendedprice)\nFROM lineitem\nWHERE\n    l_shipdate <= DATE '1998-09-02'\nGROUP BY l_returnflag,\n         l_linestatus\n''').to_df())\n\ndef duckdb_grouped_aggregate_filter_1t():\n    duckdb_grouped_aggregate_filter(duckdb)\n\ndef duckdb_grouped_aggregate_filter_2t():\n    duckdb_grouped_aggregate_filter(con)\n\ndef pandas_grouped_aggregate_filter():\n  filtered_df = lineitem[lineitem['l_shipdate'] < \"1998-09-02\"]\n  print(filtered_df.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean')))\n\ndef pandas_grouped_aggregate_filter_projection_pushdown():\n  pushed_down_df = lineitem[['l_shipdate', 'l_returnflag', 'l_linestatus', 'l_extendedprice']]\n  filtered_df = pushed_down_df[pushed_down_df['l_shipdate'] < \"1998-09-02\"]\n  print(filtered_df.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean')))\n\nresults = []\nresults.append(timeit(duckdb_grouped_aggregate_filter_1t, 'DuckDB (1T)'))\nresults.append(timeit(duckdb_grouped_aggregate_filter_2t, 'DuckDB (2T)'))\nresults.append(timeit(pandas_grouped_aggregate_filter, 'Pandas'))\nresults.append(timeit(pandas_grouped_aggregate_filter_projection_pushdown, 'Pandas (manual pushdown)'))\nplot_results(results, 'Grouped Aggregate + Filter')\n\n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.117017e+11                 901.0   \n1            A            F          5.658655e+10                 904.0   \n2            R            F          5.656804e+10                 904.0   \n3            N            F          1.487505e+09                 920.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38249.117989  \n1              104949.5          38273.129735  \n2              104899.5          38250.854626  \n3              104049.5          38284.467761  \n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.117017e+11                 901.0   \n1            A            F          5.658655e+10                 904.0   \n2            R            F          5.656804e+10                 904.0   \n3            N            F          1.487505e+09                 920.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38249.117989  \n1              104949.5          38273.129735  \n2              104899.5          38250.854626  \n3              104049.5          38284.467761  \n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nA            F             5.658655e+10  904.0  104949.5  38273.129735\nN            F             1.487505e+09  920.0  104049.5  38284.467761\n             O             1.116318e+11  901.0  104749.5  38249.322811\nR            F             5.656804e+10  904.0  104899.5  38250.854626\n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nA            F             5.658655e+10  904.0  104949.5  38273.129735\nN            F             1.487505e+09  920.0  104049.5  38284.467761\n             O             1.116318e+11  901.0  104749.5  38249.322811\nR            F             5.656804e+10  904.0  104899.5  38250.854626\nGrouped Aggregate + Filter\n                       name      time\n0               DuckDB (1T)  0.281653\n1               DuckDB (2T)  0.356302\n2                    Pandas  2.889015\n3  Pandas (manual pushdown)  1.625353\n\n\nGrouped Aggregate with Join and Filter\nIn this benchmark we expand on the previous benchmark by including a join and a filter on the joined-on table.\nNote that the naive version in Pandas is extremely slow, as it performs a full join of the entire table including all the columns that are not used and all the rows that will be filtered out. For that reason we have included a separate benchmark in which we have manually optimized the Pandas code by pushing down the projections and the filters.\nThese optimizations are performed automatically in DuckDB by the query optimizer.\n\n\n# projection & filter on lineitem table\nlineitem_projected = lineitem[\n  ['l_shipdate',\n   'l_orderkey',\n   'l_linestatus',\n   'l_returnflag',\n   'l_extendedprice']\n]\nlineitem_filtered = lineitem_projected[\n  lineitem_projected['l_shipdate'] < \"1998-09-02\"]\n# projection and filter on order table\norders_projected = orders[\n  ['o_orderkey',\n   'o_orderstatus']\n]\norders_filtered = orders_projected[\n  orders_projected['o_orderstatus'] == 'O']\n# perform the join\nmerged = lineitem_filtered.merge(\n  orders_filtered,\n  left_on='l_orderkey',\n  right_on='o_orderkey')\n# perform the aggregate\nresult = merged.groupby(\n  ['l_returnflag', 'l_linestatus']\n).agg(\n  Sum=('l_extendedprice', 'sum'),\n  Min=('l_extendedprice', 'min'),\n  Max=('l_extendedprice', 'max'),\n  Avg=('l_extendedprice', 'mean')\n)\n\n\nresult\n\n\n\n\n\n  \n    \n      \n      \n      Sum\n      Min\n      Max\n      Avg\n    \n    \n      l_returnflag\n      l_linestatus\n      \n      \n      \n      \n    \n  \n  \n    \n      N\n      O\n      1.080448e+11\n      901.0\n      104749.5\n      38250.662806\n    \n  \n\n\n\n\n\ndef duckdb_grouped_aggregate_filter_join(d_con):\n    print(d_con.query('''\nSELECT l_returnflag,\n       l_linestatus,\n       sum(l_extendedprice),\n       min(l_extendedprice),\n       max(l_extendedprice),\n       avg(l_extendedprice)\nFROM lineitem lineitem\nJOIN orders orders ON (l_orderkey=o_orderkey)\nWHERE l_shipdate <= DATE '1998-09-02'\n  AND o_orderstatus='O'\nGROUP BY l_returnflag,\n         l_linestatus\n''').to_df())\n\ndef duckdb_grouped_aggregate_filter_join_1t():\n    duckdb_grouped_aggregate_filter_join(duckdb)\n\ndef duckdb_grouped_aggregate_filter_join_2t():\n    duckdb_grouped_aggregate_filter_join(con)\n\ndef pandas_grouped_aggregate_filter_join():\n    merged = lineitem.merge(orders, left_on='l_orderkey', right_on='o_orderkey')\n    filtered_a = merged[merged['l_shipdate'] < \"1998-09-02\"]\n    filtered_b = filtered_a[filtered_a['o_orderstatus'] == 'O']\n    result = filtered_b.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean'))\n    print(result)\n\ndef pandas_grouped_aggregate_filter_join_manual_pushdown():\n    lineitem_projected = lineitem[['l_shipdate', 'l_orderkey', 'l_linestatus', 'l_returnflag', 'l_extendedprice']]\n    lineitem_filtered = lineitem_projected[lineitem_projected['l_shipdate'] < \"1998-09-02\"]\n    orders_projected = orders[['o_orderkey', 'o_orderstatus']]\n    orders_filtered = orders_projected[orders_projected['o_orderstatus'] == 'O']\n    merged = lineitem_filtered.merge(orders_filtered, left_on='l_orderkey', right_on='o_orderkey')\n    result = merged.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean'))\n    print(result)\n\nresults = []\nresults.append(timeit(duckdb_grouped_aggregate_filter_join_1t, 'DuckDB (1T)'))\nresults.append(timeit(duckdb_grouped_aggregate_filter_join_2t, 'DuckDB (2T)'))\nresults.append(timeit(pandas_grouped_aggregate_filter_join, 'Pandas'))\nresults.append(timeit(pandas_grouped_aggregate_filter_join_manual_pushdown, 'Pandas (manual pushdown)'))\nplot_results(results, 'Grouped Aggregate Join')\n\n\n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.081147e+11                 901.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38250.450307  \n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.081147e+11                 901.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38250.450307  \n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nN            O             1.080448e+11  901.0  104749.5  38250.662806\n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nN            O             1.080448e+11  901.0  104749.5  38250.662806\nGrouped Aggregate Join\n                       name       time\n0               DuckDB (1T)   0.218088\n1               DuckDB (2T)   0.376592\n2                    Pandas  11.403579\n3  Pandas (manual pushdown)   2.765103"
  },
  {
    "objectID": "blog/posts/2020-04-10-presentation-ready-chart.html",
    "href": "blog/posts/2020-04-10-presentation-ready-chart.html",
    "title": "Making Powerpoint Ready Chart with matplotlib",
    "section": "",
    "text": "Large Font Size Title\nIn reality, you probably don’t need a title as big as this one. But using library defautls often is not the best choice."
  },
  {
    "objectID": "blog/posts/2020-04-10-presentation-ready-chart.html#avoid-low-resolution-chart",
    "href": "blog/posts/2020-04-10-presentation-ready-chart.html#avoid-low-resolution-chart",
    "title": "Making Powerpoint Ready Chart with matplotlib",
    "section": "Avoid Low Resolution Chart",
    "text": "Avoid Low Resolution Chart\n\n\n\n\n\n\nNote\n\n\n\nBelieve it or not, a low resolution chart looks much less conviencing. Taking screenshot with larger charts helps you to preserve the resolution.\n\n\n\nResolution of the chart is much better\nMore obvious Title & Label (Try take a few step back from your monitor, see if you can read it)"
  },
  {
    "objectID": "blog/posts/2021-06-26-jupyter-sql-notebook.html",
    "href": "blog/posts/2021-06-26-jupyter-sql-notebook.html",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "",
    "text": "If you have ever written SQL queries to extract data from a database, chances are you are familiar with an IDE like the screenshot below. The IDE offers features like auto-completion, visualize the query output, display the table schema and the ER diagram. Whenever you need to write a query, this is your go-to tool. However, you may want to add Jupyter Notebook into your toolkit. It improves my productivity by complementing some missing features in IDE."
  },
  {
    "objectID": "blog/posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-self-contained-report",
    "href": "blog/posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-self-contained-report",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "Notebook as a self-contained report",
    "text": "Notebook as a self-contained report\nAs a data scientist/data analyst, you write SQL queries for ad-hoc analyses all the time. After getting the right data, you make nice-looking charts and put them in a PowerPoint and you are ready to present your findings. Unlike a well-defined ETL job, you are exploring the data and testing your hypotheses all the time. You make assumptions, which is often wrong but you only realized it after a few weeks. But all you got is a CSV that you cannot recall how it was generated in the first place.\nData is not stationary, why should your analysis be? I have seen many screenshots, fragmented scripts flying around in organizations. As a data scientist, I learned that you need to be cautious about what you heard. Don’t trust peoples’ words easily, verify the result! To achieve that, we need to know exactly how the data was extracted, what kind of assumptions have been made? Unfortunately, this information usually is not available. As a result, people are redoing the same analysis over and over. You will be surprised that this is very common in organizations. In fact, numbers often do not align because every department has its own definition for a given metric. It is not shared among the organization, and verbal communication is inaccurate and error-prone. It would be really nice if anyone in the organization can reproduce the same result with just a single click. Jupyter Notebook can achieve that reproducibility and keep your entire analysis (documentation, data, and code) in the same place."
  },
  {
    "objectID": "blog/posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-an-extension-of-ide",
    "href": "blog/posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-an-extension-of-ide",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "Notebook as an extension of IDE",
    "text": "Notebook as an extension of IDE\nWriting SQL queries in a notebook gives you extra flexibility of a full programming language alongside SQL. For example:\n\nWrite complex processing logic that is not easy in pure SQL\nCreate visualizations directly from SQL results without exporting to an intermediate CSV\n\nFor instance, you can pipe your SQL query with pandas and then make a plot. It allows you to generate analysis with richer content. If you find bugs in your code, you can modify the code and re-run the analysis. This reduces the hustles to reproduce an analysis greatly. In contrast, if your analysis is reading data from an anonymous exported CSV, it is almost guaranteed that the definition of the data will be lost. No one will be able to reproduce the dataset.\nYou can make use of the ipython_sql library to make queries in a notebook. To do this, you need to use the magic function with the inline magic % or cell magic %%.\n\nsales = %sql SELECT * from sales LIMIT 3\nsales\n\n\n\n    \n        ProductId\n        Unit\n        IsDeleted\n    \n    \n        1\n        10\n        1\n    \n    \n        1\n        10\n        1\n    \n    \n        2\n        10\n        0\n    \n\n\n\nTo make it fancier, you can even parameterize your query with variables. Tools like papermill allows you to parameterize your notebook. If you execute the notebook regularly with a scheduler, you can get a updated dashboard. To reference the python variable, the $ sign is used.\n\ntable = \"sales\"\nquery = f\"SELECT * from {table} LIMIT 3\"\nsales = %sql $query\nsales\n\n\n\n    \n        ProductId\n        Unit\n        IsDeleted\n    \n    \n        1\n        10\n        1\n    \n    \n        1\n        10\n        1\n    \n    \n        2\n        10\n        0\n    \n\n\n\nWith a little bit of python code, you can make a nice plot to summarize your finding. You can even make an interactive plot if you want. This is a very powerful way to extend your analysis.\n\nimport seaborn as sns\nsales = %sql SELECT * FROM SALES\nsales_df = sales.DataFrame()\nsales_df = sales_df.groupby('ProductId', as_index=False).sum()\nax = sns.barplot(x='ProductId', y='Unit', data=sales_df)\nax.set_title('Sales by ProductId');"
  },
  {
    "objectID": "blog/posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-collaboration-tool",
    "href": "blog/posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-collaboration-tool",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "Notebook as a collaboration tool",
    "text": "Notebook as a collaboration tool\nJupyter Notebook is flexible and it fits extremely well with exploratory data analysis. To share to a non-coder, you can share the notebook or export it as an HTML file. They can read the report or any cached executed result. If they need to verify the data or add some extra plots, they can do it easily themselves.\nIt is true that Jupyter Notebook has an infamous reputation. It is not friendly to version control, it’s hard to collaborate with notebooks. Luckily, there are efforts that make collaboration in notebook a lot easier now.\nHere what I did not show you is that the table has an isDeleted column. Some of the records are invalid and we should exclude them. In reality, this happens frequently when you are dealing with hundreds of tables that you are not familiar with. These tables are made for applications, transactions, and they do not have analytic in mind. Data Analytic is usually an afterthought. Therefore, you need to consult the SME or the maintainer of that tables. It takes many iterations to get the correct data that can be used to produce useful insight.\nWith ReviewNB, you can publish your result and invite some domain expert to review your analysis. This is where notebook shine, this kind of workflow is not possible with just the SQL script or a screenshot of your finding. The notebook itself is a useful documentation and collaboration tool.\n\nStep 1 - Review PR online\n\n\n\n\n\nStep1\n\n\nYou can view your notebook and add comments on a particular cell on ReviewNB. This lowers the technical barrier as your analysts do not have to understand Git. He can review changes and make comments on the web without the need to pull code at all. As soon as your analyst makes a suggestion, you can make changes.\n\n\nStep 2 - Review Changes\n\n\n\n\n\nStep2\n\n\nOnce you have made changes to the notebook, you can review it side by side. This is very trivial to do it in your local machine. Without ReviewNB, you have to pull both notebooks separately. As Git tracks line-level changes, you can’t really read the changes as it consists of a lot of confusing noise. It would also be impossible to view changes about the chart with git.\n\n\nStep 3 - Resolve Discussion\n\n\n\n\n\nStep3\n\n\nOnce the changes are reviewed, you can resolve the discussion and share your insight with the team. You can publish the notebook to internal sharing platform like knowledge-repo to organize the analysis.\nI hope this convince you that Notebook is a good choice for adhoc analytics. It is possible to collaborate with notebook with proper software in place. Regarless if you use notebook or not, you should try your best to document the process. Let’s make more reproducible analyses!"
  },
  {
    "objectID": "blog/posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
    "href": "blog/posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
    "title": "Hydra - Config Composition for Machine Learning Project",
    "section": "",
    "text": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/hydra-example\nMachine learning project involves large number of hyperparmeters. In many case you could have multiple config, e.g. differnet dataset, database connection, train/test mode. hydra provide a simple Command Line Interface that is useful for composing different experiment configs. In essence, it compose different files to a large config setting. It offers you the common Object Oriented Programming with YAML file. Allow you to have clear structure of configurations.\nAssume you have a config.yaml like this, where run_mode and hyperparmeter are separate folder to hold different choice of parameters. You can set defaults for them with the following structure."
  },
  {
    "objectID": "blog/posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#folder-structure",
    "href": "blog/posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#folder-structure",
    "title": "Hydra - Config Composition for Machine Learning Project",
    "section": "Folder Structure",
    "text": "Folder Structure\nconfig.yaml\ndemo.py\nrun_mode\n  - train.yaml\n  - test.yaml\nhyperparmeter\n  - base.yaml"
  },
  {
    "objectID": "blog/posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#config.yaml",
    "href": "blog/posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#config.yaml",
    "title": "Hydra - Config Composition for Machine Learning Project",
    "section": "config.yaml",
    "text": "config.yaml\ndefaults:\n - run_mode: train\n - hyperparameter: base\nThe benefit of using such approach is that it makes comparsion of experiments much easier. Instead of going through the parameters list, you only focus on the argument(the difference). It helps organize machine learning results and ease a lot of pain in tracking the model performance.\nimport hydra\nfrom omegaconf import DictConfig\n@hydra.main(config_path=\"config.yaml\")\ndef my_app(cfg : DictConfig) -> None:\n    print(cfg.pretty())\nif __name__ == \"__main__\":\n    my_app()\npython demo.py \ngamma: 0.01\nlearning_rate: 0.01\nrun_mode: train\nweek: 8\nFor example, with a simple example with 4 parameters only, you can simply run the experiment with default"
  },
  {
    "objectID": "blog/posts/kedro-debug-runner.html",
    "href": "blog/posts/kedro-debug-runner.html",
    "title": "Quick implementation of Kedro DebugRunner",
    "section": "",
    "text": "::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n%load_ext autoreload\n%autoreload 2\n\nfrom nbdev.showdoc import *\n:::\n\n\nfrom collections import Counter\nfrom itertools import chain\nfrom typing import Any, Dict, Iterable, List, Set\n\nfrom kedro.framework.hooks.manager import _NullPluginManager\nfrom kedro.io import AbstractDataSet, DataCatalog, MemoryDataSet\nfrom kedro.pipeline import Pipeline\nfrom kedro.pipeline.node import Node\nfrom kedro.runner import SequentialRunner\nfrom kedro.runner.runner import AbstractRunner, run_node\nfrom pluggy import PluginManager\n\n\nclass DebugRunner(SequentialRunner):\n    def run(\n        self,\n        pipeline: Pipeline,\n        catalog: DataCatalog,\n        dataset_names: List[str] = None,\n        hook_manager: PluginManager = None,\n        session_id: str = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Run the ``Pipeline`` using the datasets provided by ``catalog``\n        and save results back to the same objects.\n\n        Args:\n            pipeline: The ``Pipeline`` to run.\n            catalog: The ``DataCatalog`` from which to fetch data.\n            hook_manager: The ``PluginManager`` to activate hooks.\n            session_id: The id of the session.\n\n        Raises:\n            ValueError: Raised when ``Pipeline`` inputs cannot be satisfied.\n\n        Returns:\n            Any node outputs that cannot be processed by the ``DataCatalog``.\n            These are returned in a dictionary, where the keys are defined\n            by the node outputs.\n\n        \"\"\"\n        if not dataset_names:\n            dataset_names = []\n        hook_manager = hook_manager or _NullPluginManager()\n        catalog = catalog.shallow_copy()\n\n        unsatisfied = pipeline.inputs() - set(catalog.list())\n        if unsatisfied:\n            raise ValueError(\n                f\"Pipeline input(s) {unsatisfied} not found in the DataCatalog\"\n            )\n\n        free_outputs = (\n            pipeline.outputs()\n        )  # Return everything regardless if it it's in catalog\n        unregistered_ds = pipeline.data_sets() - set(catalog.list())\n        for ds_name in unregistered_ds:\n            catalog.add(ds_name, self.create_default_data_set(ds_name))\n\n        if self._is_async:\n            self._logger.info(\n                \"Asynchronous mode is enabled for loading and saving data\"\n            )\n        self._run(pipeline, catalog, dataset_names, hook_manager, session_id)\n\n        self._logger.info(\"Pipeline execution completed successfully.\")\n        \n        free_outputs = free_outputs | set(dataset_names)  # Union\n\n        return {ds_name: catalog.load(ds_name) for ds_name in free_outputs}\n\n    def _run(\n        self,\n        pipeline: Pipeline,\n        catalog: DataCatalog,\n        dataset_names: List[str],\n        hook_manager: PluginManager,\n        session_id: str = None,\n    ) -> None:\n        \"\"\"The method implementing sequential pipeline running.\n\n        Args:\n            pipeline: The ``Pipeline`` to run.\n            catalog: The ``DataCatalog`` from which to fetch data.\n            hook_manager: The ``PluginManager`` to activate hooks.\n            session_id: The id of the session.\n\n        Raises:\n            Exception: in case of any downstream node failure.\n        \"\"\"\n        nodes = pipeline.nodes\n        done_nodes = set()\n\n        load_counts = Counter(chain.from_iterable(n.inputs for n in nodes))\n\n        for exec_index, node in enumerate(nodes):\n            try:\n                run_node(node, catalog, hook_manager, self._is_async, session_id)\n                done_nodes.add(node)\n            except Exception:\n                self._suggest_resume_scenario(pipeline, done_nodes, catalog)\n                raise\n\n            # decrement load counts and release any data sets we've finished with\n            for data_set in node.inputs:\n                load_counts[data_set] -= 1\n                if load_counts[data_set] < 1 and data_set not in pipeline.inputs():\n                    if data_set not in dataset_names:\n                        catalog.release(data_set)\n            for data_set in node.outputs:\n                if load_counts[data_set] < 1 and data_set not in pipeline.outputs():\n                    if data_set not in dataset_names:\n                        catalog.release(data_set)\n\n            self._logger.info(\n                \"Completed %d out of %d tasks\", exec_index + 1, len(nodes)\n            )\n\n\n# `DebugRunner` has to be used in a different way since `session.run` don't support additional argument, so we are going to use a lower level approach and construct `Runner` and `Pipeline` and `DataCatalog` ourselves.\n\n# Testing Kedro Project: https://github.com/noklam/kedro_gallery/tree/master/kedro-debug-runner-demo\n%load_ext kedro.ipython\n%reload_kedro ~/dev/kedro_gallery/kedro-debug-runner-demo\n\nThe kedro.ipython extension is already loaded. To reload it, use:\n  %reload_ext kedro.ipython\n[10/06/22 14:45:20] INFO     Updated path to Kedro project:       __init__.py:54\n                             /Users/Nok_Lam_Chan/dev/kedro_galler               \n                             y/kedro-debug-runner-demo                          \n[10/06/22 14:45:22] INFO     Kedro project                        __init__.py:77\n                             kedro_debug_runner_demo                            \n                    INFO     Defined global variable 'context',   __init__.py:78\n                             'session', 'catalog' and 'pipelines'               \n\n\n\n%reload_kedro ~/dev/kedro_gallery/kedro-debug-runner-demo\nrunner = DebugRunner()\ndefault_pipeline = pipelines[\"__default__\"]\nrun_1 = runner.run(default_pipeline, catalog)\n\n                    INFO     Updated path to Kedro project:       __init__.py:54\n                             /Users/Nok_Lam_Chan/dev/kedro_galler               \n                             y/kedro-debug-runner-demo                          \n[10/06/22 14:45:24] INFO     Kedro project                        __init__.py:77\n                             kedro_debug_runner_demo                            \n                    INFO     Defined global variable 'context',   __init__.py:78\n                             'session', 'catalog' and 'pipelines'               \n                    INFO     Loading data from               data_catalog.py:343\n                             'example_iris_data'                                \n                             (CSVDataSet)...                                    \n                    INFO     Loading data from 'parameters'  data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: split:                    node.py:327\n                             split_data([example_iris_data,parameter            \n                             s]) -> [X_train,X_test,y_train,y_test]             \n                    INFO     Saving data to 'X_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'X_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: make_predictions:         node.py:327\n                             make_predictions([X_train,X_test,y_trai            \n                             n]) -> [y_pred]                                    \n                    INFO     Saving data to 'y_pred'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_pred'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: report_accuracy:          node.py:327\n                             report_accuracy([y_pred,y_test]) ->                \n                             None                                               \n                    INFO     Model has accuracy of 0.933 on test     nodes.py:74\n                             data.                                              \n\n\n\nrunner = DebugRunner()\ndefault_pipeline = pipelines[\"__default__\"]\nrun_2 = runner.run(default_pipeline, catalog, dataset_names=[\"example_iris_data\"])\n\n[10/06/22 14:45:27] INFO     Loading data from               data_catalog.py:343\n                             'example_iris_data'                                \n                             (CSVDataSet)...                                    \n                    INFO     Loading data from 'parameters'  data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: split:                    node.py:327\n                             split_data([example_iris_data,parameter            \n                             s]) -> [X_train,X_test,y_train,y_test]             \n                    INFO     Saving data to 'X_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'X_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: make_predictions:         node.py:327\n                             make_predictions([X_train,X_test,y_trai            \n                             n]) -> [y_pred]                                    \n                    INFO     Saving data to 'y_pred'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_pred'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: report_accuracy:          node.py:327\n                             report_accuracy([y_pred,y_test]) ->                \n                             None                                               \n                    INFO     Model has accuracy of 0.933 on test     nodes.py:74\n                             data.                                              \n                    INFO     Loading data from               data_catalog.py:343\n                             'example_iris_data'                                \n                             (CSVDataSet)...                                    \n\n\n\nrunner = DebugRunner()\ndefault_pipeline = pipelines[\"__default__\"]\nrun_3 = runner.run(default_pipeline, catalog, dataset_names=[\"X_train\"]) # Input datasets\n\n[10/06/22 14:46:01] INFO     Loading data from               data_catalog.py:343\n                             'example_iris_data'                                \n                             (CSVDataSet)...                                    \n                    INFO     Loading data from 'parameters'  data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: split:                    node.py:327\n                             split_data([example_iris_data,parameter            \n                             s]) -> [X_train,X_test,y_train,y_test]             \n                    INFO     Saving data to 'X_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'X_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: make_predictions:         node.py:327\n                             make_predictions([X_train,X_test,y_trai            \n                             n]) -> [y_pred]                                    \n                    INFO     Saving data to 'y_pred'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_pred'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: report_accuracy:          node.py:327\n                             report_accuracy([y_pred,y_test]) ->                \n                             None                                               \n                    INFO     Model has accuracy of 0.933 on test     nodes.py:74\n                             data.                                              \n                    INFO     Loading data from 'X_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n\n\n\nrun_1\n\n{}\n\n\n\nrun_2\n\n{'example_iris_data':      sepal_length  sepal_width  petal_length  petal_width    species\n 0             5.1          3.5           1.4          0.2     setosa\n 1             4.9          3.0           1.4          0.2     setosa\n 2             4.7          3.2           1.3          0.2     setosa\n 3             4.6          3.1           1.5          0.2     setosa\n 4             5.0          3.6           1.4          0.2     setosa\n ..            ...          ...           ...          ...        ...\n 145           6.7          3.0           5.2          2.3  virginica\n 146           6.3          2.5           5.0          1.9  virginica\n 147           6.5          3.0           5.2          2.0  virginica\n 148           6.2          3.4           5.4          2.3  virginica\n 149           5.9          3.0           5.1          1.8  virginica\n \n [150 rows x 5 columns]}\n\n\n\nrun_3\n\n{'X_train':      sepal_length  sepal_width  petal_length  petal_width\n 47            4.6          3.2           1.4          0.2\n 3             4.6          3.1           1.5          0.2\n 31            5.4          3.4           1.5          0.4\n 25            5.0          3.0           1.6          0.2\n 15            5.7          4.4           1.5          0.4\n ..            ...          ...           ...          ...\n 28            5.2          3.4           1.4          0.2\n 78            6.0          2.9           4.5          1.5\n 146           6.3          2.5           5.0          1.9\n 49            5.0          3.3           1.4          0.2\n 94            5.6          2.7           4.2          1.3\n \n [120 rows x 4 columns]}\n\n\n\nclass GreedySequentialRunner(SequentialRunner):\n    def run(\n        self,\n        pipeline: Pipeline,\n        catalog: DataCatalog,\n        hook_manager: PluginManager = None,\n        session_id: str = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Run the ``Pipeline`` using the datasets provided by ``catalog``\n        and save results back to the same objects.\n\n        Args:\n            pipeline: The ``Pipeline`` to run.\n            catalog: The ``DataCatalog`` from which to fetch data.\n            hook_manager: The ``PluginManager`` to activate hooks.\n            session_id: The id of the session.\n\n        Raises:\n            ValueError: Raised when ``Pipeline`` inputs cannot be satisfied.\n\n        Returns:\n            Any node outputs that cannot be processed by the ``DataCatalog``.\n            These are returned in a dictionary, where the keys are defined\n            by the node outputs.\n\n        \"\"\"\n\n        hook_manager = hook_manager or _NullPluginManager()\n        catalog = catalog.shallow_copy()\n\n        unsatisfied = pipeline.inputs() - set(catalog.list())\n        if unsatisfied:\n            raise ValueError(\n                f\"Pipeline input(s) {unsatisfied} not found in the DataCatalog\"\n            )\n\n        free_outputs = pipeline.outputs() # Return everything regardless if it it's in catalog\n        unregistered_ds = pipeline.data_sets() - set(catalog.list())\n        for ds_name in unregistered_ds:\n            catalog.add(ds_name, self.create_default_data_set(ds_name))\n\n        if self._is_async:\n            self._logger.info(\n                \"Asynchronous mode is enabled for loading and saving data\"\n            )\n        self._run(pipeline, catalog, hook_manager, session_id)\n\n        self._logger.info(\"Pipeline execution completed successfully.\")\n\n        return {ds_name: catalog.load(ds_name) for ds_name in free_outputs}"
  },
  {
    "objectID": "blog/posts/2020-11-14-hong-kong-elevation-map-with-rayshader.html",
    "href": "blog/posts/2020-11-14-hong-kong-elevation-map-with-rayshader.html",
    "title": "Hong Kong Elevation map with rayshader (with R)",
    "section": "",
    "text": "This blog is mainly reproducing the blog with different data https://www.tylermw.com/a-step-by-step-guide-to-making-3d-maps-with-satellite-imagery-in-r/. My impression is that R is doing so much better for graph compare to Python. (ggplot and now rayshader for 3D plots!)"
  },
  {
    "objectID": "blog/posts/2020-11-14-hong-kong-elevation-map-with-rayshader.html#data",
    "href": "blog/posts/2020-11-14-hong-kong-elevation-map-with-rayshader.html#data",
    "title": "Hong Kong Elevation map with rayshader (with R)",
    "section": "Data",
    "text": "Data\nTwo datasets was used for this images. Landset for RGB * LC08_L1TP_122044_20200218_20200225_01_T1.TIF\nSRTM 30M resolution elevation map * n21e113.hgt * n21e114.hgt * n22e113.hgt * n22e114.hgt The USGS explorer is a very nice tool to search data.\nI actually couldn’t find a Landsat image cover entire hong kong (some western part is missing). Further enhancement is needed for stitching together different images."
  },
  {
    "objectID": "blog/posts/2020-11-14-hong-kong-elevation-map-with-rayshader.html#setup",
    "href": "blog/posts/2020-11-14-hong-kong-elevation-map-with-rayshader.html#setup",
    "title": "Hong Kong Elevation map with rayshader (with R)",
    "section": "Setup",
    "text": "Setup\n\nconda with R Kernel\nJupyter Notebook\nfastpages\nrayshader\n\n\nUse conda install even for R Packages, I spend hours to get the environment going back and forth in Windows and Linux\n\n\n## Library\nlibrary(rayshader)\nlibrary(sp)\nlibrary(raster)\nlibrary(scales)\nlibrary(dplyr)\n\n\nelevation1 = raster::raster(\"../data/rayshader/HongKong/N21E113.hgt\")\nelevation2 = raster::raster(\"../data/rayshader/HongKong/N21E114.hgt\")\nelevation3 = raster::raster(\"../data/rayshader/HongKong/N22E113.hgt\")\nelevation4 = raster::raster(\"../data/rayshader/HongKong/N22E114.hgt\")\n\nLet’s plot the elevation map. The whole image is green-ish because most of the area is ocean, so they are at sea-level. The orange color indicate a higher elevation.\n\nhk_elevation = raster::merge(elevation1,elevation2, elevation3, elevation4)\n\nheight_shade(raster_to_matrix(hk_elevation)) %>%\nplot_map();\n\n\n\n\nelevation\n\n\nNext, we are going to process the RGB image from Landsat-8 ,The raw jpeg look like this.\n\n\n\nraw_jpeg\n\n\nSatellite raw images requries some preprocessing, before they look like what we expected.\n\nhk_r = raster::raster(\"../data/rayshader/HongKong/LC08_L1TP_122044_20200218_20200225_01_T1_B4.TIF\")\nhk_g = raster::raster(\"../data/rayshader/HongKong/LC08_L1TP_122044_20200218_20200225_01_T1_B3.TIF\")\nhk_b = raster::raster(\"../data/rayshader/HongKong/LC08_L1TP_122044_20200218_20200225_01_T1_B2.TIF\")\n\n\nhk_rbg_corrected = sqrt(raster::stack(hk_r, hk_g, hk_b))\nraster::plotRGB(hk_rbg_corrected);\n\n\n\n\nraw_corrected\n\n\nThe image is quite hazzy, which doesn’t look like the jpeg we saw earlier. We need to improve the contrast.\n\n# Since the RGB image and elevation map does not use the same coordinate system, we need to do some projections.\nhk_elevation_utm = raster::projectRaster(hk_elevation, crs = crs(hk_r), method = \"bilinear\")\ncrs(hk_elevation_utm)\n\nbottom_left = c(y=113.888, x=22.1365)\ntop_right   = c(y=114.330, x=22.5493)\n\nextent_latlong = sp::SpatialPoints(rbind(bottom_left, top_right), proj4string=sp::CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84\"))\nextent_utm = sp::spTransform(extent_latlong, raster::crs(hk_elevation_utm))\n\ne = raster::extent(extent_utm)\ne\n\nCRS arguments:\n +proj=utm +zone=49 +datum=WGS84 +units=m +no_defs \n\n\nclass      : Extent \nxmin       : 797906.6 \nxmax       : 842523 \nymin       : 2450766 \nymax       : 2497449 \n\n\n\nhk_rgb_cropped = raster::crop(hk_rbg_corrected, e)\nelevation_cropped = raster::crop(hk_elevation_utm, e)\n\nnames(hk_rgb_cropped) = c(\"r\",\"g\",\"b\")\n\nhk_r_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$r)\nhk_g_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$g)\nhk_b_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$b)\n\nhkel_matrix = rayshader::raster_to_matrix(elevation_cropped)\n\nhk_rgb_array = array(0,dim=c(nrow(hk_r_cropped),ncol(hk_r_cropped),3))\n\nhk_rgb_array[,,1] = hk_r_cropped/255 #Red layer\nhk_rgb_array[,,2] = hk_g_cropped/255 #Blue layer\nhk_rgb_array[,,3] = hk_b_cropped/255 #Green layer\n\nhk_rgb_array = aperm(hk_rgb_array, c(2,1,3))\n\nplot_map(hk_rgb_array)\n\n\n\n\nhazzy\n\n\nThe whole image is bright because we have some dark pixels in the corner. It’s similiar to taking images in a dark room, any light source will become a bright spot.\nWe can improve this by stretching the intensity. It’s really no different than how you fine tune your images on Instagram.\n\nhk_rgb_cropped = raster::crop(hk_rbg_corrected, e)\nelevation_cropped = raster::crop(hk_elevation_utm, e)\n# Stretch the images\nhk_rgb_cropped <-\nraster::stretch(hk_rgb_cropped,\nminq = .01,\nmaxq = .999,\n)\n\nnames(hk_rgb_cropped) = c(\"r\",\"g\",\"b\")\n\nhk_r_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$r)\nhk_g_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$g)\nhk_b_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$b)\n\nhkel_matrix = rayshader::raster_to_matrix(elevation_cropped)\n\nhk_rgb_array = array(0,dim=c(nrow(hk_r_cropped),ncol(hk_r_cropped),3))\n\nhk_rgb_array[,,1] = hk_r_cropped/255 #Red layer\nhk_rgb_array[,,2] = hk_g_cropped/255 #Blue layer\nhk_rgb_array[,,3] = hk_b_cropped/255 #Green layer\n\nhk_rgb_array = aperm(hk_rgb_array, c(2,1,3))\nhk_rgb_contrast = scales::rescale(hk_rgb_array,to=c(0,1))\nplot_map(hk_rgb_contrast)\n\n\n\n\nbright\n\n\nNow we get a much better image\n\nplot_3d(hk_rgb_contrast, hkel_matrix, windowsize = c(1100,900), zscale = 15, shadowdepth = -50,\n        zoom=0.5, phi=45,theta=-15,fov=70, background = \"#F2E1D0\", shadowcolor = \"#523E2B\")\n\n\nrender_scalebar(limits=c(0, 5, 10),label_unit = \"km\",position = \"W\", y=50,\n                scale_length = c(0.33,1))\nrender_compass(position = \"N\")\nrender_snapshot(title_text = \"Hong Kong | Imagery: Landsat 8 | DEM: 30m SRTM\",\n                title_bar_color = \"#000000\", title_color = \"white\", title_bar_alpha = 1,\n               clear=TRUE, )\n\n\n\n\n3d"
  },
  {
    "objectID": "blog/posts/2021-07-02-kedro-datacatalog.html",
    "href": "blog/posts/2021-07-02-kedro-datacatalog.html",
    "title": "Advance Kedro Series - Digging into Dataset Memory Management and CacheDataSet",
    "section": "",
    "text": "Today I am gonna explain some kedro internals to understnad how kedor manage your dataset. If you always write imperative python code, you may find that writing nodes and pipeline is a little bti awkward. They may seems less intuitive, however, it also enable some interesting featrue.\nThis article assumes you have basic understanding of kedro, I will focus on CacheDataSet and the auto-release dataset feature of kedro pipeline. It is useful to reduce your memory footprint without encountering the infamous Out of Memory (OOM) issue.\nTo start with, we have the default iris dataset. Normally we would do it in a YAML file, but to make things easier in Notebook, I’ll keep everything compact in a notebook.\n\nimport kedro\nkedro.__version__\n\n'0.17.4'\n\n\n\nfrom kedro.io import DataCatalog, MemoryDataSet, CachedDataSet\nfrom kedro.extras.datasets.pandas import CSVDataSet\nfrom kedro.pipeline import node, Pipeline\nfrom kedro.runner import SequentialRunner\n\n# Prepare a data catalog\ndata_catalog = DataCatalog({\"iris\": CSVDataSet('data/01_raw/iris.csv')})\n\nNext, we have a pipeline follows this execution order: A -> B -> C\n\nfrom kedro.pipeline import Pipeline, node\nimport pandas as pd\n\n\ndef A(df):\n    print('Loading the Iris Dataset')\n    return 'Step1'\n\n\ndef B(dummy):\n    return 'Step2'\n\n\ndef C(dummy):\n    return 'Step3'\n\n\npipeline = Pipeline([node(A, \"iris\", \"A\"),\n                     node(B, \"A\", \"B\"),\n                     node(C, \"B\", \"C\"),\n                    ])\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\nTo zoom in to the pipeline, we can use Hook to print out the catalog after every node’s run.\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.framework.hooks import get_hook_manager\nfrom pprint import pprint\n\ndef apply_dict(d):\n    new_dict = {}\n    for k, v in d.items():\n        if isinstance(v, CachedDataSet):\n            if v._cache.exists():\n                print(v._cache._data)\n                new_dict[k] = 'In Memory'\n            else:\n                new_dict[k] ='Cache Deleted'\n        elif v.exists():\n            new_dict[k] = 'In Memory'\n    return new_dict\n\n\nclass DebugHook:\n    \"\"\"A hook class for creating a post mortem debugging with the PDB debugger\n    whenever an error is triggered within a pipeline. The local scope from when the\n    exception occured is available within this debugging session.\n    \"\"\"\n    @hook_impl\n    def after_node_run(self, node, catalog):\n        # adding extra behaviour to a single node\n        print(f\"Finish node {node.name}\")\n        pprint(f\"Print Catalog {apply_dict(catalog._data_sets)}\")\n#         pprint(f\"Print Catalog {apply_dict2(lambda x:x.exists(), catalog._data_sets)}\")\n        print(\"*****************************\")\n        \nhook_manager = get_hook_manager()\ndebug_hook = hook_manager.register(DebugHook());\n\nThis hook will print out dataset that exist in data catalog. It is a bit tricky because kedro did not delete the dataset, it marked the underlying data as _EMPTY object instead.\n\n# Create a runner to run the pipeline\nrunner = SequentialRunner()\n\n# Run the pipeline\nrunner.run(pipeline, data_catalog);\n\nLoading the Iris Dataset\nFinish node A([iris]) -> [A]\n\"Print Catalog {'iris': 'In Memory'}\"\n*****************************\nFinish node B([A]) -> [B]\n\"Print Catalog {'iris': 'In Memory', 'A': 'In Memory'}\"\n*****************************\nFinish node C([B]) -> [C]\n\"Print Catalog {'iris': 'In Memory', 'B': 'In Memory'}\"\n*****************************\n\n\nLet’s have a look at what happened when a SequentialRunner runs a pipeline.\nIt is interesting to note that kedro takes a similar approach to Python, it uses reference counting to control the dataset life cycle. If you are interested, I have another post to dive into Python Memory Management.\n            # decrement load counts and release any data sets we've finished with\n            for data_set in node.inputs:\n                load_counts[data_set] -= 1\n                if load_counts[data_set] < 1 and data_set not in pipeline.inputs():\n                    catalog.release(data_set)\n            for data_set in node.outputs:\n                if load_counts[data_set] < 1 and data_set not in pipeline.outputs():\n                    catalog.release(data_set)\n\nCacheDataSet\nWhat does release do? It will remove the underlying data if this data is stored in memory.\n# In CSVDataSet\nhttps://github.com/quantumblacklabs/kedro/blob/master/kedro/extras/datasets/pandas/csv_dataset.py#L176-L178\n```python\ndef _release(self) -> None:\n    super()._release()\n    self._invalidate_cache()\n# In CacheDataSet\ndef _release(self) -> None:\n    self._cache.release()\n    self._dataset.release()\n# In MemoryDataSet\ndef _release(self) -> None:\n    self._data = _EMPTY\nFirst, we can test if it works as expected.\n\nd = CachedDataSet(CSVDataSet('data/01_raw/iris.csv'))\nd.load()\nd._cache._data.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n  \n\n\n\n\n\nd.exists()\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\nTrue\n\n\n\nd.release()\n\n\nd._cache.exists()\n\nFalse\n\n\nThis is the expected behavior, where the cache should be released. However, it seems not to be the case when I run the pipeline.\n\ndata_catalog = DataCatalog({\"iris\": CachedDataSet(CSVDataSet('data/01_raw/iris.csv'))})\nrunner.run(pipeline, data_catalog)\n\nLoading the Iris Dataset\nFinish node A([iris]) -> [A]\n     sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\"Print Catalog {'iris': 'In Memory'}\"\n*****************************\nFinish node B([A]) -> [B]\n     sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\"Print Catalog {'iris': 'In Memory', 'A': 'In Memory'}\"\n*****************************\nFinish node C([B]) -> [C]\n     sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\"Print Catalog {'iris': 'In Memory', 'B': 'In Memory'}\"\n*****************************\n\n\n{'C': 'Step3'}\n\n\nThe dataset is persisted throughout the entire pipeline, why? We can monkey patch the SequentialRunner to check why is this happening.\n\n\nA potential bug or undesired beahvior?\n\nfrom collections import Counter\nfrom itertools import chain\nfrom kedro.runner.runner import AbstractRunner, run_node\n\ndef _run(\n    self, pipeline, catalog, run_id = None\n) -> None:\n    \"\"\"The method implementing sequential pipeline running.\n\n    Args:\n        pipeline: The ``Pipeline`` to run.\n        catalog: The ``DataCatalog`` from which to fetch data.\n        run_id: The id of the run.\n\n    Raises:\n        Exception: in case of any downstream node failure.\n    \"\"\"\n    nodes = pipeline.nodes\n    done_nodes = set()\n\n    load_counts = Counter(chain.from_iterable(n.inputs for n in nodes))\n\n    for exec_index, node in enumerate(nodes):\n        try:\n            run_node(node, catalog, self._is_async, run_id)\n            done_nodes.add(node)\n        except Exception:\n            self._suggest_resume_scenario(pipeline, done_nodes)\n            raise\n            \n        # print load counts for every node run\n        pprint(f\"{load_counts}\")\n        print(\"pipeline input: \", pipeline.inputs())\n        print(\"pipeline output: \", pipeline.outputs())\n\n        # decrement load counts and release any data sets we've finished with\n        for data_set in node.inputs:\n            load_counts[data_set] -= 1\n            if load_counts[data_set] < 1 and data_set not in pipeline.inputs():\n                catalog.release(data_set)\n        for data_set in node.outputs:\n            if load_counts[data_set] < 1 and data_set not in pipeline.outputs():\n                catalog.release(data_set)\n\n        self._logger.info(\n            \"Completed %d out of %d tasks\", exec_index + 1, len(nodes)\n        )\n        \nSequentialRunner._run = _run\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\nNow we re-run the pipeline. Let’s reset the hook to only print related information.\n\nclass PrintHook:\n    @hook_impl\n    def after_node_run(self, node, catalog):\n        # adding extra behaviour to a single node\n        print(f\"Finish node {node.name}\")\n        print(\"*****************************\")\n        \n\nhook_manager.set_blocked(debug_hook); # I tried hook_manger.unregister(), but it is not working.\nprint_hook = hook_manager.register(PrintHook())\n\n\n# Create a runner to run the pipeline\nrunner = SequentialRunner()\n\n# Run the pipeline\nrunner.run(pipeline, data_catalog);\n\nLoading the Iris Dataset\nFinish node A([iris]) -> [A]\n*****************************\n\"Counter({'iris': 1, 'A': 1, 'B': 1})\"\npipeline input:  {'iris'}\npipeline output:  {'C'}\nFinish node B([A]) -> [B]\n*****************************\n\"Counter({'A': 1, 'B': 1, 'iris': 0})\"\npipeline input:  {'iris'}\npipeline output:  {'C'}\nFinish node C([B]) -> [C]\n*****************************\n\"Counter({'B': 1, 'iris': 0, 'A': 0})\"\npipeline input:  {'iris'}\npipeline output:  {'C'}\n\n\n\n\nConclusion\nSo the reason why the iris data is kept becasue it is always in pipeline.inputs(), which I think is not what we wanted."
  },
  {
    "objectID": "blog/posts/2020-12-04-kedro-pipeline.html",
    "href": "blog/posts/2020-12-04-kedro-pipeline.html",
    "title": "Introduction to Kedro - pipeline for data science",
    "section": "",
    "text": "Data Scientist often starts their development with a Jupyter Notebook. As the notebook grows larger, it’s inevitable to convert it to a python script. It starts with one file, then another one, and it accumulates quickly. Converting a notebook could be more than just pasting the code in a script. It involves careful thinking and refactoring.\nA pipeline library can be helpful in a few ways: - modular pipeline, it can be executed partially. - easily run in parallel - check for loop dependecies\n\n\nKedro is a development workflow tool that allows you to create portable data pipelines. It applies software engineering best practices to make your data science code reproducible, modular and well-documented. For example, you can easily create a template for new projects, build a documentation site, lint your code and always have an expected structure to find your config and data.\n\nKedro is a lightweight pipeline library without need to setup infracstructure.\n\nIn comparison to Airflow or Luigi, Kedro is much more lightweighted. It helps you to write production-ready code, and let data engineer and data scientist work together with the same code base. It also has good Jupyter support, so data scientist can still use the tool that they are familiar with.\nFunctions and Pipeline\n\n\ndef split_data(data: pd.DataFrame, example_test_data_ratio: float):\n    ...\n    return dict(\n        train_x=train_data_x,\n        train_y=train_data_y,\n        test_x=test_data_x,\n        test_y=test_data_y,\n    )\nNode is the core component of kedro Pipeline. For example, we have a python function that split data into train/test set. A node take 4 arguments. func, inputs, outputs, name. To use this function as a node, we would write something like this.\n\nnode(split_data, inputs=[\"example_iris_data\", \"params:example_test_data_ratio\"],\n                outputs= dict(\n                train_x=\"example_train_x\",\n                train_y=\"example_train_y\",\n                test_x=\"example_test_x\",\n                test_y=\"example_test_y\",\n                ),\n         name=\"split_data\")\nIt’s fairly simple, and it resemble the original function. The only significant difference is, split_data takes a df and float, but in our nodes, it becomes a list of strings. I will explain it in Section 3.2.\n\n\n\nPipeline is nothing more than a list of Node, it helps you to reuse nodes for different pipelines\nPipeline([ṅode(),\n         [node(),\n            ...]])\nHere is an simple Pipeline which does splitting data, train a model, make predictions, and report metrics.\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                split_data,\n                [\"example_iris_data\", \"params:example_test_data_ratio\"],\n                dict(\n                    train_x=\"example_train_x\",\n                    train_y=\"example_train_y\",\n                    test_x=\"example_test_x\",\n                    test_y=\"example_test_y\",\n                ),\n            ),\n            node(\n                train_model,\n                [\"example_train_x\", \"example_train_y\", \"parameters\"],\n                \"example_model\",\n            ),\n            node(\n                predict,\n                dict(model=\"example_model\", test_x=\"example_test_x\"),\n                \"example_predictions\",\n            ),\n            node(report_accuracy, [\"example_predictions\", \"example_test_y\"], None, name='report1'),\n            node(report_accuracy, [\"example_predictions\", \"example_test_y\"], None, name='report2'),\n        ]\n    )\nYou can also use node tags or writing different defined pipeline to reuse your node easily.\n\n\n\nInternally, Kedro always form a graph for your entire pipelines, which can be visaulized with this command.\nkedro viz\nThis starts a web server that visualizes the dependencies of your function, parameters and data,you can also filter some nodes of function with the UI.\n\n\n\nviz\n\n\n\n\n\n\n\n\n\n\nimage.png\n\n\nYou can execute your pipeline partially with this command. This with execute your pipeline from A to C except the last Node D.\nkedro run --from-nodes=\"A, B, C\"\nIf you pay attention to this graph, Node B and Node C has no dependency, they only depend on Node A. With kedro, you can parallelize this execution for free by using this command.\nkedro run --parallel\n\n\n\nNow, you have basic understand of what is Node and Pipeline, you also learnt that you can use kedro run command to execute your pipeline with different options. Before I jump into other kedro features, let me explain a bit more about functional programming. This concept is at the heart of data processing library like spark.\nFunctional programming, means using functions to program literally. It may sounds silly, but bear with me.\nPure Function has these characteristics: 1. No side effect, it won’t change state outside of the function scope. 2. If you repeating running the same function with same input(argument), it should give you the same output. 3. Easy to parallel if there is no data dependency\nConsider this simple function that add 1 to your input:\n\ndef func1(x):\n    x=x+1\n\ndef func2(x):\n    return x+1\nvar1 = 1\nvar2 = 1\n\n\nfunc1(var1) # var1=2\nfunc2(var2) # var2=2\nThey both add 1 to your input, so which version is a better function?\nfunc1(var1) # var1=3\nfunc2(var2) # var2=2    \nNow consider if we run this function twice. func1 changes the result to 3, while func2 still give you 2. I argue func2 is better in this case.\nWhy does this matter? Or how is it going to be useful at all? Well, it makes debugging much easier. It is because you only need to debug code inside a function, not 200 lines of code before it. This greatly reduce the complexity that you have to worried about your data. This fundamental principle is what powering the pipeline, and the reason why you can just use kedro run --parallel to parallelize some computation.\nIt will also be easier to write test for function. func1 is harder to test, because you need to consider all possible code path. You may end up need to write verbose test cases like this.\ndef test_case1():\n    func_A()\n    func_B()\n    \ndef test_case2():\n    func_A()\n    func_A()\n    func_B()\nHow does using Kedro helps to achieve this? Think about func1, if it is written as a Node, it will look like this.\nNode(func1, inputs=var1, output=None, name=\"func1\")\nSince it is a Node without any output, it will have no impact to the downstreams. In order to use that variable, you will naturally writing code looks more like func2 instead.\nLet’s look at one more example.\n\nk = 10\ndef func3(x):\n    return x+k\n\n\nfunc3(10)\n\n20\n\n\nNow consider func3, it is a valid Python function. You can run it in a notebook or in a script, but it wouldn’t be possible for a Node, sinec a Node only have access to its input. It will just throw an error to you immediately.\nnode(func3, inputs='x', outputs='some_result', name='func3')\nBy writing nodes, you limit your function to only access variable within its scope. It helps to prevent a lot of bug.\n\n\n\nI hope the examples demonstrate how writing nodes help transform your code towards functional style. In reality, decoupling your functions from a programming is not straight forward.\nConsider this example. \nLook at how data np.nan is changed. This wouldn’t be a problem if we have one program, since we will just passing all variable in memroy, without the step that writing and reading from a file.\nError like these are subtle and dangerous, it may not throw error, but ruining our features quality. We have better chance to catch these error in a small program, but it would be much harder to isolate the issue if we have 1000 lines of code. The sooner you integrate it into your pipeline, the easier the integration is. In fact, we can do better. We could introduce test case for validating data, I would explain more in Section 3.5.\n\n\n\n\n Data Catalog is an API for Dataset. It includes a Data Model from from raw data, feature, to reporting layer and a standard Data I/O API. It integrates with pandas, spark, SQLAlchemy and Cloud Storage.\nTo use Data Catalog, you would first need to define your dataset in the catalog.yml. You will have give it a name and type, denoting whether it is a SQL query or a CSV. Optionally, you can pass in any arguments that are supported from the underlying API as well.\nexample_iris_data:\n  type: pandas.CSVDataSet\n  filepath: data/01_raw/iris.csv\n\n\nLet’s reuse our split_data function. When you create a node that using the split_data function, you would pass in the string of the dataset instead of an actual dataframe, the Reading/Writing operation is handled by Kedro, so you don’t have to write to_csv() or read_csv() yourself.\nparameters.yml\nexample_test_data_ratio: 0.2\nA node using the split_data function.\n\nnode(split_data, inputs=[\"example_iris_data\", \"params:example_test_data_ratio\"],\n                outputs= dict(\n                train_x=\"example_train_x\",\n                train_y=\"example_train_y\",\n                test_x=\"example_test_x\",\n                test_y=\"example_test_y\",\n                ),\n         name=\"split_data\")\nHere the inputs “example_iris_data” is refering to a dataset defined by catalog.yml, kedro will load the csv for you. Same applies for params:example_test_data_ratio.\nBy using catalog and parmaeters, it already makes your program cleaner. You now have a single file to manager all data source, and a single file contains all parameters, which is configurable. Your functions now is parameterized, you can simply change configuration in a single file without going into every possible script to change a number.\nData Catalog abstract away the Data I/O logic from the data processing function.\nIt process data and write a file.\ndef process_data(df):\n    ... # do some processing\n    df.to_csv('xxx.csv')\nIt only process data\ndef process_data(df):\n    ... #do some processing\n    return df\nThis applies the single-responsibility principle (SRP), meaning that your function is only doing one thing at a time. There are many benefits from it, for example, it makes data versioning easier. I will explain this in Section 3.3.\n\n\n\nRemember our we pass in a string to our node, and it will look for the corresponding dataset? What if we do not define it? It could be a lot of work if we need to define everything. Besides, some variable are not needed to be written out as a file, it could just stay as in memory.\nIn fact, kedro use MemroyDataset by default. Which means you could simply pass in a string that is not defined, the string will be use as the name of the variable. There are more useful dataset like CacheDataset, you can find more details in this link.\nhttps://kedro.readthedocs.io/en/stable/kedro.extras.datasets.html\np.s. When using kedro pipeline, you only define the node’s inputs and outputs, but you never defined the order of execution. From my experience, there are pros and cons. The benefits is, your code is less coupled, and due to this, kedro is able to execute your pipeline in parallel whenever possible to speed up your program. However, it means the order of execution is not guaranteed, this may cause unexpected effect. For example, if you are training a machine learning model, it is common to set a random seed at the beginning. Due to the randomness of execution, you may not get identical result, as the order of execution is different everytime, thus the sequence of the random number used is random too. In general this is not a big problem, but if you have a strong need to make sure you have identical output (e.g. regression test), it may cause some trouble and you need to use dummy input and output to force kedro run your pipeline in a specific order."
  },
  {
    "objectID": "blog/posts/2022-11-16-python_single_dispatch/python-dispatch-typehint.html",
    "href": "blog/posts/2022-11-16-python_single_dispatch/python-dispatch-typehint.html",
    "title": "Function overloading - singledispatch in Python with type hint",
    "section": "",
    "text": "With Python>=3.7, the @singledispatch method can now understand the type hints. It behaves like function overloading but it’s more dynamic than the static langauge.\nHere is a quick example to demonstrate it.\n\nfrom functools import singledispatch\n\n@singledispatch\ndef foo(x):\n    print(\"foo\")\n\n\n@foo.register\ndef _(x: float):\n    print(\"It's a float\")\n\n\n@foo.register\ndef _(x: str):\n    print(\"It's a string now!\")\n\nLet’s see how it works.\n\nfoo(1)\n\nfoo\n\n\n\nfoo(1.0)\n\nIt's a float\n\n\n\nfoo(\"1\")\n\nIt's a string now!\n\n\nThe function foo now understand the type of the argument and dispatch the corresponding functions. This is nicer than a big chunk of if/else statement since it’s less couple. It’s also easy to extend this. Imagine the foo function is import from a package, it’s easy to extend it.\n\n# Imagine `foo` was imported from a package\n# Now that you have a special type and you want to extend it from your own library, you don't need to touch the source code at all.\n\n# from bar import foo\nclass Nok:\n    ...\n\n\n@foo.register\ndef _(x: Nok):\n    print(\"Nok\")\n\n\nnok = Nok()\nfoo(nok)\n\nNok\n\n\nThis is only possible because Python is a dynamic language. In contrast, to achieve the same functionalities with monkey patching, you would need to copy the source code of the function and extend the if/else block.\nLet’s dive a bit deeper to the decorator.\n\nprint([attr for attr in dir(foo) if not attr.startswith(\"_\")])\n\n['dispatch', 'register', 'registry']\n\n\n\nfoo.dispatch\n\n<function functools.singledispatch.<locals>.dispatch(cls)>\n\n\n\nfoo.register\n\n<function functools.singledispatch.<locals>.register(cls, func=None)>\n\n\n\nfoo.registry\n\nmappingproxy({object: <function __main__.foo(x)>,\n              float: <function __main__._(x: float)>,\n              str: <function __main__._(x: str)>,\n              __main__.Nok: <function __main__._(x: __main__.Nok)>,\n              __main__.Nok: <function __main__._(x: __main__.Nok)>})\n\n\n\nfrom collections import abc\nisinstance(foo.registry, abc.Mapping)\n\nThe foo.registry is the most interesting part. Basically, it’s a dictionary-like object which store the types. It behaves like\nif type(x) == \"int\":\n    do_something()\nelif type(x) == \"float\":\n    do_somthing_else()\nelse:\n    do_this_instead()"
  },
  {
    "objectID": "blog/posts/2022-11-13-pandas_expert/kedro-meta-analysis.html",
    "href": "blog/posts/2022-11-13-pandas_expert/kedro-meta-analysis.html",
    "title": "Understanding the Kedro codebase - A quick dirty meta-analysis - (Part I)",
    "section": "",
    "text": "Inspired by this talk\n\nHow many lines of code in Kedro?\n\nfrom pathlib import Path\nimport pandas as pd\nfrom collections import Counter\n\n\nREPO_PATH = Path(\"/Users/Nok_Lam_Chan/GitHub/kedro\")\nlist(REPO_PATH.iterdir())\n\n[PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test_requirements.txt'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/CODE_OF_CONDUCT.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/LICENSE.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tools'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro_technical_charter.pdf'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.DS_Store'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.pytest_cache'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/derby.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro.egg-info'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.pre-commit-config.yaml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.coverage'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/Makefile'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/CITATION.cff'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/CODEOWNERS'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/pyproject.toml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/trufflehog-ignore.txt'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/dependency'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/MANIFEST.in'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/docs'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.readthedocs.yml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/dep_tree.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/README.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/RELEASE.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/setup.py'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/demo-project'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/logs'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.mypy_cache'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.gitignore'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/static'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/CONTRIBUTING.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/behave.ini'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.github'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.gitpod.yml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/info.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/coverage.xml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/errors.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.git'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/htmlcov'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.vscode'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/data'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/conf'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.circleci'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/import.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/notebooks'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.run'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.idea'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/src')]\n\n\n\ndef count_effective_line(counter, fn):\n    with open (fn) as f:\n        for line in f:\n            counter[fn] += 1\n\n\nlines_count = Counter()\nfor fn in REPO_PATH.rglob(\"*/*.py\"):\n#     print(fn)\n    count_effective_line(lines_count, fn)\nprint(lines_count)\n            \n\nCounter({PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_spark_dataset.py'): 984, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_pipeline.py'): 940, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/pipeline.py'): 926, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/test_session.py'): 891, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/micropkg.py'): 854, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/micropkg/test_micropkg_pull.py'): 846, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/core.py'): 748, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_cli.py'): 730, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_data_catalog.py'): 685, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_starters.py'): 639, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/cli_steps.py'): 623, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py'): 612, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/docs/conf.py'): 598, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/data_catalog.py'): 594, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/docs/build/conf.py'): 587, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/micropkg/test_micropkg_package.py'): 581, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/test_session_extension_hooks.py'): 576, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_partitioned_dataset.py'): 565, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/starters.py'): 552, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/partitioned_dataset.py'): 551, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/pipeline/test_pipeline.py'): 522, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_incremental_dataset.py'): 503, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/context/test_context.py'): 485, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/config/test_templated_config.py'): 482, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_project.py'): 479, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_jupyter.py'): 470, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/utils.py'): 469, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py'): 456, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/tensorflow/test_tensorflow_model_dataset.py'): 441, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/sql_dataset.py'): 438, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/matplotlib/test_matplotlib_writer.py'): 436, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_node.py'): 434, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/session.py'): 423, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/spark_dataset.py'): 422, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_modular_pipeline.py'): 418, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/runner/test_parallel_runner.py'): 401, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/project.py'): 392, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_pipeline_with_transcoding.py'): 391, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_generic_dataset.py'): 383, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/conftest.py'): 381, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_sql_dataset.py'): 374, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/project/__init__.py'): 369, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/config/test_config.py'): 354, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/parallel_runner.py'): 353, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/context/context.py'): 345, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_parquet_dataset.py'): 344, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/pipeline.py'): 336, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_gbq_dataset.py'): 315, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_spark_hive_dataset.py'): 314, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/gbq_dataset.py'): 314, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_catalog.py'): 305, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/ipython/test_ipython.py'): 304, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_csv_dataset.py'): 300, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/hooks/specs.py'): 296, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/modular_pipeline.py'): 290, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/jupyter.py'): 282, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_excel_dataset.py'): 281, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/templated_config.py'): 281, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/dagascii.py'): 275, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/runner/test_sequential_runner.py'): 273, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pickle/test_pickle_dataset.py'): 269, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/micropkg/test_micropkg_requirements.py'): 266, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/project/test_pipeline_discovery.py'): 260, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/excel_dataset.py'): 254, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/test_startup.py'): 250, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/common.py'): 248, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/generic_dataset.py'): 246, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_hdf_dataset.py'): 245, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/matplotlib/matplotlib_writer.py'): 243, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pickle/pickle_dataset.py'): 243, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_xml_dataset.py'): 241, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_json_dataset.py'): 241, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/geojson/test_geojson_dataset.py'): 232, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pillow/test_image_dataset.py'): 231, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/parquet_dataset.py'): 230, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_pipeline_from_missing.py'): 227, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_memory_dataset.py'): 226, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/networkx/test_json_dataset.py'): 226, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/email/test_message_dataset.py'): 226, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/docs/source/conf.py'): 225, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/docs/source/conf.py'): 225, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/docs/source/conf.py'): 225, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/docs/source/conf.py'): 224, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/docs/source/conf.py'): 224, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/docs/source/conf.py'): 222, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/holoviews/test_holoviews_writer.py'): 220, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_feather_dataset.py'): 220, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/spark_hive_dataset.py'): 220, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/runner/test_thread_runner.py'): 213, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/cli.py'): 211, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/yaml/test_yaml_dataset.py'): 210, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/hdf_dataset.py'): 204, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/json/test_json_dataset.py'): 200, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_lambda_dataset.py'): 194, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/tracking/test_metrics_dataset.py'): 194, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/csv_dataset.py'): 194, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/feather_dataset.py'): 191, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/redis/redis_dataset.py'): 189, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/networkx/test_gml_dataset.py'): 188, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/networkx/test_graphml_dataset.py'): 188, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tensorflow/tensorflow_model_dataset.py'): 188, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/email/message_dataset.py'): 188, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/text/test_text_dataset.py'): 187, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/json_dataset.py'): 187, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/tracking/test_json_dataset.py'): 185, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/catalog.py'): 176, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/spark_jdbc_dataset.py'): 175, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/xml_dataset.py'): 171, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/api/test_api_dataset.py'): 170, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/runner/conftest.py'): 168, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/redis/test_redis_dataset.py'): 165, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/ipython/__init__.py'): 164, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/dask/test_parquet_dataset.py'): 162, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/json/json_dataset.py'): 160, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/conftest.py'): 159, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/geopandas/geojson_dataset.py'): 157, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/startup.py'): 156, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/thread_runner.py'): 156, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/yaml/yaml_dataset.py'): 155, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/tools/test_cli.py'): 154, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/plotly/json_dataset.py'): 154, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/networkx/json_dataset.py'): 150, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/networkx/gml_dataset.py'): 145, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/networkx/graphml_dataset.py'): 143, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_cached_dataset.py'): 142, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pillow/image_dataset.py'): 142, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/api/api_dataset.py'): 142, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_node_run.py'): 141, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/biosequence/biosequence_dataset.py'): 137, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/holoviews/holoviews_writer.py'): 137, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/dask/parquet_dataset.py'): 136, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/config.py'): 134, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/memory_dataset.py'): 132, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/text/text_dataset.py'): 131, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/environment.py'): 128, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_cli_hooks.py'): 128, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/test_session_hook_manager.py'): 126, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_spark_jdbc_dataset.py'): 121, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/plotly/plotly_dataset.py'): 117, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/cached_dataset.py'): 113, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/lambda_dataset.py'): 113, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/plotly/test_plotly_dataset.py'): 108, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/deltatable_dataset.py'): 108, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/bioinformatics/test_biosequence_dataset.py'): 107, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/hooks/manager.py'): 106, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/sh_run.py'): 105, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/project/test_settings.py'): 102, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/plotly/test_json_dataset.py'): 101, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_core.py'): 96, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/logging/color_logger.py'): 95, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/conftest.py'): 89, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/test_store.py'): 89, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_deltatable_dataset.py'): 89, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_registry.py'): 88, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/sequential_runner.py'): 87, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/util.py'): 84, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_pipeline_integration.py'): 84, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/pipeline/conftest.py'): 84, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_science/nodes.py'): 80, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/project/test_pipeline_registry.py'): 79, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/micropkg/conftest.py'): 79, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/hooks/test_manager.py'): 75, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/nodes.py'): 74, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/nodes.py'): 74, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/nodes.py'): 74, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/nodes.py'): 74, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tracking/metrics_dataset.py'): 68, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_memory_dataset.py'): 67, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tools/cli.py'): 62, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/settings.py'): 62, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/project/test_logging.py'): 58, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/settings.py'): 56, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_engineering/nodes.py'): 51, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/registry.py'): 50, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/hooks/manager.py'): 49, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/test/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tracking/json_dataset.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/hooks/specs.py'): 46, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/shelvestore.py'): 43, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/tests/test_run.py'): 41, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/conftest.py'): 41, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/tests/test_run.py'): 41, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/tests/test_run.py'): 40, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/setup.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/tests/test_run.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/setup.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/tests/test_run.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/setup.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/settings.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/tests/test_run.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/pipeline.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/abstract_config.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/store.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/setup.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/settings.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/__init__.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/pipeline.py'): 38, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/test/settings.py'): 37, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/setup.py'): 37, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/settings.py'): 37, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/setup.py'): 37, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/conftest.py'): 35, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/pipeline.py'): 33, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/pipeline.py'): 33, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/__init__.py'): 33, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/test_utils.py'): 30, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/utils.py'): 28, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_plugin/plugin.py'): 27, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_science/pipeline.py'): 27, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tools/circleci/github_scripts/kedro_version.py'): 26, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_engineering/pipeline.py'): 26, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/settings.py'): 24, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/hooks/test_manager.py'): 22, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/extensions/ipython.py'): 22, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipeline_registry.py'): 19, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/__init__.py'): 19, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/pipeline_registry.py'): 18, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/pipeline_registry.py'): 18, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/pipeline_registry.py'): 18, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/pipeline_registry.py'): 18, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/logging/test_color_logger.py'): 16, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/__init__.py'): 16, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipeline_registry.py'): 16, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/logging/__init__.py'): 15, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/networkx/__init__.py'): 15, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/__init__.py'): 14, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/test/pipeline_registry.py'): 13, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_plugin/setup.py'): 12, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/__init__.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/hooks/markers.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/plotly/__init__.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/api/__init__.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tracking/__init__.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/__main__.py'): 10, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/hooks/markers.py'): 10, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/pipeline/{{ cookiecutter.pipeline_name }}/__init__.py'): 10, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/pipeline/{{ cookiecutter.pipeline_name }}/pipeline.py'): 10, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/tests/test_pipeline.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/tests/test_pipeline.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/tests/test_pipeline.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/__init__.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/pipeline/{{ cookiecutter.pipeline_name }}/tests/test_pipeline.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/dask/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/redis/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/geopandas/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pillow/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/json/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/biosequence/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tensorflow/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/matplotlib/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/yaml/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pickle/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/text/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/holoviews/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/email/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_science/__init__.py'): 7, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_engineering/__init__.py'): 7, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/context/__init__.py'): 7, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/__init__.py'): 7, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/__init__.py'): 6, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/hooks/__init__.py'): 5, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/hooks/__init__.py'): 5, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/test/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/pipeline/{{ cookiecutter.pipeline_name }}/nodes.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/settings.py'): 3, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/__init__.py'): 3, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/extensions/__init__.py'): 3, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/__init__.py'): 2, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/__init__.py'): 1})\n\n\n\n\nClean up the dictionary a little bit\n\nline_counts_df = pd.DataFrame(lines_count.items(), columns=[\"fullpath\",\"line_of_code\"])\nline_counts_df[\"fullpath\"] = line_counts_df[\"fullpath\"].apply(str)\nline_counts_df[\"fullpath\"] =  line_counts_df[\"fullpath\"].str.replace(\"/Users/Nok_Lam_Chan/GitHub/kedro/\", \"\")\nline_counts_df.head(2)\n\n\n\n\n\n  \n    \n      \n      fullpath\n      line_of_code\n    \n  \n  \n    \n      0\n      tools/cli.py\n      62\n    \n    \n      1\n      features/environment.py\n      128\n    \n  \n\n\n\n\n\nline_counts_df[[\"toplevel\",\"module\",\"submodule\",\"filename\"]] = line_counts_df[\"fullpath\"].str.split(\"/\",expand=True, n=3)\n\n\nline_counts_df\n\n\n\n\n\n  \n    \n      \n      fullpath\n      line_of_code\n      toplevel\n      module\n      submodule\n      filename\n    \n  \n  \n    \n      0\n      tools/cli.py\n      62\n      tools\n      cli.py\n      None\n      None\n    \n    \n      1\n      features/environment.py\n      128\n      features\n      environment.py\n      None\n      None\n    \n    \n      2\n      tests/test_utils.py\n      30\n      tests\n      test_utils.py\n      None\n      None\n    \n    \n      3\n      tests/conftest.py\n      89\n      tests\n      conftest.py\n      None\n      None\n    \n    \n      4\n      docs/conf.py\n      598\n      docs\n      conf.py\n      None\n      None\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      276\n      kedro/extras/datasets/pandas/feather_dataset.py\n      191\n      kedro\n      extras\n      datasets\n      pandas/feather_dataset.py\n    \n    \n      277\n      kedro/extras/datasets/pandas/hdf_dataset.py\n      204\n      kedro\n      extras\n      datasets\n      pandas/hdf_dataset.py\n    \n    \n      278\n      kedro/extras/datasets/pandas/csv_dataset.py\n      194\n      kedro\n      extras\n      datasets\n      pandas/csv_dataset.py\n    \n    \n      279\n      kedro/extras/datasets/pandas/excel_dataset.py\n      254\n      kedro\n      extras\n      datasets\n      pandas/excel_dataset.py\n    \n    \n      280\n      kedro/extras/datasets/pandas/gbq_dataset.py\n      314\n      kedro\n      extras\n      datasets\n      pandas/gbq_dataset.py\n    \n  \n\n281 rows × 6 columns\n\n\n\n\n## Sort by Top level module\nline_counts_df.groupby([\"toplevel\"]).sum().sort_values(ascending=False, by =\"line_of_code\")\n\n\n\n\n\n  \n    \n      \n      line_of_code\n    \n    \n      toplevel\n      \n    \n  \n  \n    \n      tests\n      25341\n    \n    \n      kedro\n      18683\n    \n    \n      features\n      1587\n    \n    \n      docs\n      1185\n    \n    \n      resume-kedro\n      1007\n    \n    \n      iris-demo\n      550\n    \n    \n      iris\n      547\n    \n    \n      test\n      405\n    \n    \n      tools\n      88\n    \n  \n\n\n\n\nInterstingly we have roughly a 1:1 ratio between tests and kedro\n\nline_counts_df.groupby([\"module\",\"submodule\"]).sum().sort_values(ascending=False, by =\"line_of_code\")\n\n\n\n\n\n  \n    \n      \n      \n      line_of_code\n    \n    \n      module\n      submodule\n      \n    \n  \n  \n    \n      extras\n      datasets\n      15775\n    \n    \n      framework\n      cli\n      8837\n    \n    \n      session\n      2574\n    \n    \n      pipeline\n      test_pipeline.py\n      940\n    \n    \n      pipeline.py\n      926\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      config\n      __init__.py\n      19\n    \n    \n      runner\n      __init__.py\n      16\n    \n    \n      pipeline\n      __init__.py\n      9\n    \n    \n      extras\n      __init__.py\n      2\n    \n    \n      framework\n      __init__.py\n      1\n    \n  \n\n74 rows × 1 columns\n\n\n\n\n## Sort by Sub-module\nkedro_line_counts_df = line_counts_df[line_counts_df[\"toplevel\"] == \"kedro\"]\ntmp = kedro_line_counts_df.groupby(\"module\").sum().rename(mapper={\"line_of_code\": \"module_line_of_code\"},axis=1 )\nkedro_line_counts_df_group = kedro_line_counts_df.groupby([\"module\",\"submodule\"]).sum().reset_index().merge(tmp, left_on=\"module\", right_on=\"module\")\n\n\n# .sort_values(ascending=False, by =\"line_of_code\")\n\n\nkedro_line_counts_df.groupby([\"module\"]).sum().sort_values(ascending=False, by =\"line_of_code\")\n\n\n\n\n\n  \n    \n      \n      line_of_code\n    \n    \n      module\n      \n    \n  \n  \n    \n      extras\n      6871\n    \n    \n      framework\n      5246\n    \n    \n      io\n      2284\n    \n    \n      pipeline\n      1837\n    \n    \n      runner\n      1068\n    \n    \n      config\n      721\n    \n    \n      templates\n      443\n    \n    \n      ipython\n      164\n    \n    \n      utils.py\n      28\n    \n    \n      __init__.py\n      11\n    \n    \n      __main__.py\n      10\n    \n  \n\n\n\n\n\n# Sort by file \nkedro_line_counts_df_group.sort_values(ascending=False, by =[\"module_line_of_code\",\"line_of_code\"])\n\n\n\n\n\n  \n    \n      \n      module\n      submodule\n      line_of_code\n      module_line_of_code\n    \n  \n  \n    \n      6\n      extras\n      datasets\n      6734\n      6871\n    \n    \n      8\n      extras\n      logging\n      110\n      6871\n    \n    \n      7\n      extras\n      extensions\n      25\n      6871\n    \n    \n      5\n      extras\n      __init__.py\n      2\n      6871\n    \n    \n      10\n      framework\n      cli\n      3439\n      5246\n    \n    \n      14\n      framework\n      session\n      511\n      5246\n    \n    \n      12\n      framework\n      hooks\n      418\n      5246\n    \n    \n      13\n      framework\n      project\n      369\n      5246\n    \n    \n      11\n      framework\n      context\n      352\n      5246\n    \n    \n      15\n      framework\n      startup.py\n      156\n      5246\n    \n    \n      9\n      framework\n      __init__.py\n      1\n      5246\n    \n    \n      18\n      io\n      core.py\n      748\n      2284\n    \n    \n      19\n      io\n      data_catalog.py\n      594\n      2284\n    \n    \n      22\n      io\n      partitioned_dataset.py\n      551\n      2284\n    \n    \n      21\n      io\n      memory_dataset.py\n      132\n      2284\n    \n    \n      17\n      io\n      cached_dataset.py\n      113\n      2284\n    \n    \n      20\n      io\n      lambda_dataset.py\n      113\n      2284\n    \n    \n      16\n      io\n      __init__.py\n      33\n      2284\n    \n    \n      27\n      pipeline\n      pipeline.py\n      926\n      1837\n    \n    \n      26\n      pipeline\n      node.py\n      612\n      1837\n    \n    \n      25\n      pipeline\n      modular_pipeline.py\n      290\n      1837\n    \n    \n      24\n      pipeline\n      __init__.py\n      9\n      1837\n    \n    \n      30\n      runner\n      runner.py\n      456\n      1068\n    \n    \n      29\n      runner\n      parallel_runner.py\n      353\n      1068\n    \n    \n      32\n      runner\n      thread_runner.py\n      156\n      1068\n    \n    \n      31\n      runner\n      sequential_runner.py\n      87\n      1068\n    \n    \n      28\n      runner\n      __init__.py\n      16\n      1068\n    \n    \n      4\n      config\n      templated_config.py\n      281\n      721\n    \n    \n      2\n      config\n      common.py\n      248\n      721\n    \n    \n      3\n      config\n      config.py\n      134\n      721\n    \n    \n      1\n      config\n      abstract_config.py\n      39\n      721\n    \n    \n      0\n      config\n      __init__.py\n      19\n      721\n    \n    \n      34\n      templates\n      project\n      410\n      443\n    \n    \n      33\n      templates\n      pipeline\n      33\n      443\n    \n    \n      23\n      ipython\n      __init__.py\n      164\n      164\n    \n  \n\n\n\n\n\n# Total number of LOC\nkedro_line_counts_df[\"line_of_code\"].sum()\n\n18683\n\n\n\n\nConclusion\nThe kedro codebase is not huge, roughly 20000 line of code, compare to pandas which has > 250000 of code, 10x smaller. The datasets and framework code is the largest module which isn’t surprise to me. The more surprising is how small config actually is, but it creates huge complexity in terms of a kedro project. The cli is also relatively huge as it takes ~3000 lines of code which I didn’t expected."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "What is a Jupyter Kernel? By ChatGPT\n\n\n\n\n\n\n\njupyter\n\n\n\n\nWhat is a Jupyter Kernel?\n\n\n\n\n\n\nApr 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction overloading - singledispatch in Python with type hint\n\n\n\n\n\n\n\npython\n\n\n\n\nUsing singledispatch with type hint\n\n\n\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Kedro with DuckDB\n\n\n\n\n\n\n\npython\n\n\n\n\nKedro Meet the Duck\n\n\n\n\n\n\nNov 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Kedro codebase - A quick dirty meta-analysis - (Part I)\n\n\n\n\n\n\n\nkedro\n\n\n\n\nMeta-analysis of the kedro codebase\n\n\n\n\n\n\nNov 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeing Python Expert\n\n\n\n\n\n\n\npython\n\n\n\n\nNotes for the talk - James Powell: So you want to be a Python expert? | PyData Seattle 2017\n\n\n\n\n\n\nNov 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick implementation of Kedro DebugRunner\n\n\n\n\n\n\n\neuropython\n\n\n\n\nQuick implementation of a debug runner\n\n\n\n\n\n\nNov 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvance Kedro Series - Digging into Dataset Memory Management and CacheDataSet\n\n\n\n\n\n\n\npython\n\n\nkedro\n\n\n\n\nKedro pipeline offers some nice feature like automatically release data in memory that is no longer need. How is this possible? Let’s dive deep into the code.\n\n\n\n\n\n\nJul 2, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nJupyter Superpower - Extend SQL analysis with Python\n\n\n\n\n\n\n\npython\n\n\nreviewnb\n\n\nsql\n\n\n\n\nMaking collboration with Notebook possible and share perfect SQL analysis with Notebook.\n\n\n\n\n\n\nJun 26, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\ndeepcopy, LGBM and pickle\n\n\n\n\n\n\n\npython\n\n\npickle\n\n\ndeepcopy\n\n\n\n\nAt first sight, these 3 things may not sounds related at all. I am writing this article to share a bug with lightgbm that I encountered and it eventually leads to deeper understanding of what pickle really are.\n\n\n\n\n\n\nMar 19, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up pyodbc for Impala connection, works on both Linux and Window\n\n\n\n\n\n\n\npyodbc\n\n\nimpala\n\n\n\n\nEasiest way to connect with Impala in Windows\n\n\n\n\n\n\nMar 5, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Kedro - pipeline for data science\n\n\n\n\n\n\n\npython\n\n\nkedro\n\n\n\n\nKedro\n\n\n\n\n\n\nDec 4, 2020\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nHong Kong Elevation map with rayshader (with R)\n\n\n\n\n\n\n\nR\n\n\n\n\nInspired by https://www.reddit.com/r/dataisbeautiful/comments/bjp8bg/the_united_states_of_elevation_oc/. This is my little weekend project, Hong Kong elevation tile with rayshader, powered by fastpages with Jupyter notebook! I haven’t used R in years, so I spent a lot more time than expected to finish this.\n\n\n\n\n\n\nNov 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating the BBC style graphic in Python - plotnine and altair\n\n\n\n\n\n\n\npython\n\n\n\n\nDo you know that you can have ggplot in BBC R graphics cookbook? This is an attempt to reproduce https://bbc.github.io/rcookbook/ in python\n\n\n\n\n\n\nApr 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\nMaking Powerpoint Ready Chart with matplotlib\n\n\n\n\n\n\n\npython\n\n\n\n\nAre you tired with fiddling with every single python plot and you always forgot how to configure matplotlib? Start creating your own matplotlib style or just use this. [Updated on 17-07-2022]\n\n\n\n\n\n\nApr 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\ndata augmentation - Understand MixUp and Beta Distribution\n\n\n\n\n\n\n\nML\n\n\n\n\nMixup and Beta Distribution\n\n\n\n\n\n\nFeb 9, 2020\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHydra - Config Composition for Machine Learning Project\n\n\n\n\n\n\n\npython\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2020\n\n\nnoklam\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "noklam",
    "section": "",
    "text": "::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nfrom noklam.core import *\n:::\n\nBlog\nHey! I am Nok, click here to see my blog! I am still learning how to blog with notebook so the site looks a bit messy now! I am current a software engineer at Kedro, I am interested in softwate engineering & machine learning"
  }
]